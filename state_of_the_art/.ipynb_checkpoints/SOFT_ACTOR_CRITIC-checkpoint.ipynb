{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.distributions import Normal\n",
    "from scipy.ndimage.filters import uniform_filter1d\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "import random as rand\n",
    "from itertools import count\n",
    "import time\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self,capacity):   \n",
    "        self.capacity = capacity\n",
    "        self.memory = {}\n",
    "        self.push_count = 0\n",
    "        \n",
    "    def push(self, timestep, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory[timestep] = experience\n",
    "        else:\n",
    "            del_key = None\n",
    "            for k in self.memory.keys():\n",
    "                del_key = k\n",
    "                break\n",
    "            self.memory.pop(del_key, None)\n",
    "            self.memory[timestep] = experience\n",
    "        self.push_count+=1\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch_tuple = random.sample(self.memory.items(), batch_size)#key(timestep), experiences\n",
    "        experience_array = []\n",
    "        for timestep, experiences in batch_tuple:\n",
    "            experience_array.append(experiences)\n",
    "        return experience_array\n",
    "    \n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory)>=batch_size\n",
    "    \n",
    "    def update_td_error(self, sampled_experiences, timesteps):\n",
    "        for i in range(len(timesteps)):\n",
    "            self.memory[timesteps[i]] = sampled_experiences[i]\n",
    "        \n",
    "    def get_memory_values(self):\n",
    "        return self.memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env, ReplayMemory, online_policy, online_q_a, online_q_b,\\\n",
    "                online_policy_optimizer,online_q_network_optimizer_a, online_q_network_optimizer_b, target_q_a,\\\n",
    "                target_q_b, gamma=0.99, alpha_pr=0.4, beta_pr=0.3,\\\n",
    "                tau=0.01, n_ep=120, max_steps=100000, memory_size=1000000, batch_size=50,\\\n",
    "                policy_update_step=2, target_update=50, min_sample_size=500, warm_up=2):\n",
    "        \n",
    "        \n",
    "        self.env = env\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.alpha_pr = alpha_pr\n",
    "        self.beta_pr = beta_pr\n",
    "        self.agent_max_memory=memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.n_ep = n_ep\n",
    "        self.max_steps = max_steps\n",
    "        self.min_sample_size=min_sample_size\n",
    "        self.target_update=target_update\n",
    "        self.policy_update_step=policy_update_step\n",
    "        self.online_policy_network = online_policy\n",
    "        self.online_policy_optimizer = online_policy_optimizer\n",
    "        self.online_q_network_a = online_q_a\n",
    "        self.online_q_network_optimizer_a = online_q_network_optimizer_a\n",
    "        self.online_q_network_b = online_q_b\n",
    "        self.online_q_network_optimizer_b = online_q_network_optimizer_b\n",
    "        self.target_q_network_a = target_q_a\n",
    "        self.target_q_network_b = target_q_b\n",
    "        self.current_timestep = 0\n",
    "        self.warm_up = warm_up\n",
    "        self.agent_memory = ReplayMemory(self.agent_max_memory)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available()\\\n",
    "                        else \"cpu\")\n",
    "        self.Xp = namedtuple('Experience',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done', 'abs_td_error','timestep'))\n",
    "        \n",
    "    def extract_tensors(self, experiences):\n",
    "        batch = self.Xp(*zip(*experiences))\n",
    "        state = np.stack(batch.state) #stack\n",
    "        action = np.stack(batch.action)\n",
    "        next_state = np.stack(batch.next_state)\n",
    "        reward = np.stack(batch.reward)\n",
    "        done = np.stack(batch.done)\n",
    "        abs_td_error = np.stack(batch.abs_td_error)\n",
    "        timestep = np.stack(batch.timestep)\n",
    "        return state,action,next_state,reward,done,abs_td_error,timestep\n",
    "    \n",
    "    def rebuild_experiences(self, state, action, next_state, reward, done, abs_error, timestep):\n",
    "        exp_list = []\n",
    "        for idx_ in range(len(state)):\n",
    "            exp_list.append(\\\n",
    "                        self.Xp(state[idx_], action[idx_], next_state[idx_], reward[idx_],\\\n",
    "                           done[idx_], abs_error[idx_], timestep[idx_]))\n",
    "        return exp_list \n",
    "    \n",
    "    def prioritize_samples(self, experience_samples, alpha, beta):\n",
    "        state,action,next_state,reward,done,abs_td_error,timesteps \\\n",
    "                            = self.extract_tensors(experience_samples)\n",
    "        abs_td_error, indices_ = (list(t) for t in zip(*sorted(\\\n",
    "                            zip(abs_td_error.tolist(), timesteps))))\n",
    "        abs_td_error.reverse()\n",
    "        indices_.reverse()#reverse to march sort func\n",
    "        abs_td_error = np.array(abs_td_error)\n",
    "        abs_td_error  = torch.tensor(abs_td_error)\n",
    "        ranks = np.arange(1, len(abs_td_error)+1)\n",
    "        priorities = 1.0/ranks\n",
    "        priorities = priorities**alpha\n",
    "        priorities = np.expand_dims(priorities, axis=1)\n",
    "        probabilities = priorities/np.sum(priorities, axis=0)\n",
    "        assert np.isclose(probabilities.sum(), 1.0)\n",
    "        number_of_samples  = len(probabilities)\n",
    "        weight_importance_ = number_of_samples*probabilities\n",
    "        weight_importance_ = weight_importance_**-beta\n",
    "        weight_importance_max = np.max(weight_importance_)\n",
    "        weight_importance_scaled = weight_importance_/weight_importance_max\n",
    "        return weight_importance_scaled, indices_ \n",
    "    \n",
    "    def update_model(self, experience_samples,\\\n",
    "                weighted_importance, timestep_indices):\n",
    "        \n",
    "        states, actions, next_states, rewards, done, _ , timesteps =\\\n",
    "                            self.extract_tensors(experience_samples)\n",
    "         \n",
    "        arrange_weighted_values = [timestep_indices.index(i) for i in timesteps]\n",
    "    \n",
    "        states = torch.tensor(np.squeeze(states)).float().to(self.device)\n",
    "        next_states = torch.tensor(np.squeeze(next_states)).float().to(self.device)\n",
    "        actions = torch.tensor(actions).float().to(self.device)\n",
    "        rewards = torch.tensor(rewards).unsqueeze(1).float().to(self.device)\n",
    "        done = torch.tensor(done).unsqueeze(1).float().to(self.device)\n",
    "        weighted_importance = torch.tensor(weighted_importance).float().to(self.device)\n",
    "    \n",
    "        #optimize alpha\n",
    "        current_actions,_, log_pi_s = self.online_policy_network.full_pass(states)\n",
    "        target_alpha = (self.online_policy_network.target_entropy +\\\n",
    "                   log_pi_s).detach()\n",
    "        alpha_loss = -(self.online_policy_network.log_alpha*\\\n",
    "                 target_alpha).mean()\n",
    "        self.online_policy_network.target_entropy_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.online_policy_network.target_entropy_optimizer.step()\n",
    "    \n",
    "        #set alpha\n",
    "        alpha = self.online_policy_network.log_alpha.exp()\n",
    "    \n",
    "        #optimize online using target nets\n",
    "        predicted_action_policy,_, log_pi_ns =\\\n",
    "                                    self.online_policy_network.full_pass(next_states)\n",
    "\n",
    "        qsa_target_a = self.target_q_network_a(next_states, predicted_action_policy)\n",
    "        qsa_target_b = self.target_q_network_b(next_states, predicted_action_policy)\n",
    "        target_qsa = torch.min(qsa_target_a, qsa_target_b)\n",
    "        target_qsa = target_qsa - alpha*log_pi_ns\n",
    "        target_qsa = rewards + self.gamma*target_qsa*(1 - done)\n",
    "        target_qsa = target_qsa*weighted_importance\n",
    "        target_qsa = target_qsa.detach()\n",
    "        qsa_online_a = self.online_q_network_a(states, actions)\n",
    "        qsa_online_b = self.online_q_network_b(states, actions)\n",
    "        qsa_online_a = qsa_online_a*weighted_importance.detach()\n",
    "        qsa_online_b = qsa_online_b*weighted_importance.detach()\n",
    "    \n",
    "        #update online networks\n",
    "        loss_func = torch.nn.SmoothL1Loss()\n",
    "        qa_loss = loss_func(qsa_online_a, target_qsa.detach())\n",
    "        qb_loss = loss_func(qsa_online_b, target_qsa.detach())\n",
    "    \n",
    "        self.online_q_network_optimizer_a.zero_grad()\n",
    "        qa_loss.backward()\n",
    "        self.online_q_network_optimizer_a.step()\n",
    "    \n",
    "        self.online_q_network_optimizer_b.zero_grad()\n",
    "        qb_loss.backward()\n",
    "        self.online_q_network_optimizer_b.step()\n",
    "    \n",
    "        abs_a = (target_qsa.squeeze() - qsa_online_a.squeeze())\n",
    "        abs_b = (target_qsa.squeeze() - qsa_online_b.squeeze())\n",
    "        ovr_update = (abs_a + abs_b)/2\n",
    "        ovr_update = abs(ovr_update.detach().cpu().numpy())\n",
    "        if self.current_timestep % self.policy_update_step == 0:\n",
    "            current_actions,_ , log_pi = self.online_policy_network.full_pass(states)\n",
    "            qsa_online_a = self.online_q_network_a(states, current_actions)\n",
    "            qsa_online_b = self.online_q_network_b(states, current_actions)\n",
    "            qsa_min = torch.min(qsa_online_a, qsa_online_b)\n",
    "            policy_loss = (alpha*log_pi\\\n",
    "                      -qsa_min).mean()\n",
    "            self.online_policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.online_policy_optimizer.step()\n",
    "        states, actions, next_states, rewards, done, td_err , timesteps =\\\n",
    "                self.extract_tensors(experience_samples)    \n",
    "        experiences_rebuilded = self.rebuild_experiences\\\n",
    "                    (states, actions, next_states, rewards, done, ovr_update, timesteps)\n",
    "        return experiences_rebuilded, timesteps\n",
    "    \n",
    "    def query_error(self, state, action, next_state, reward):\n",
    "\n",
    "        state = torch.tensor(state).unsqueeze(0).float().to(self.device)\n",
    "        next_state = torch.tensor(next_state).unsqueeze(0).float().to(self.device)\n",
    "        alpha = self.online_policy_network.log_alpha.exp()\n",
    "        ns_actions,_, log_pi_ns = self.online_policy_network.full_pass(next_state)\n",
    "        q_target_next_states_action_a = self.target_q_network_a(next_state,\\\n",
    "                                                    ns_actions.detach())\n",
    "        q_target_next_states_action_b = self.target_q_network_b(next_state,\\\n",
    "                                                    ns_actions.detach())\n",
    "        q_min = torch.min(q_target_next_states_action_a, q_target_next_states_action_b)\n",
    "        q_target = q_min - alpha * log_pi_ns\n",
    "        q_target = reward + (self.gamma*q_target.detach())\n",
    "        action = np.expand_dims(action, axis=0)\n",
    "        q_online_state_action_val_a = self.online_q_network_a(state, action)\n",
    "        q_online_state_action_val_b = self.online_q_network_b(state, action)\n",
    "        abs_a = (q_target - q_online_state_action_val_a)\n",
    "        abs_b = (q_target - q_online_state_action_val_b)\n",
    "        abs_stack = (abs_a + abs_b)/2\n",
    "        ovr_abs_update = abs_stack\n",
    "        ovr_abs_update = ovr_abs_update.squeeze()\n",
    "        return np.absolute(ovr_abs_update.detach().cpu().numpy())\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state).float().to(self.device)\n",
    "        state = state.unsqueeze(0)\n",
    "        warm_up_action = self.batch_size * self.warm_up\n",
    "        if self.agent_memory.can_provide_sample(warm_up_action) == False:\n",
    "            action = np.random.uniform(low=self.env.action_space.low, high=self.env.action_space.high)\n",
    "            action = action.reshape(self.env.action_space.high.shape)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                mean,log_std = self.online_policy_network.forward(state)\n",
    "                action = torch.tanh(Normal(mean, log_std.exp()).sample())\n",
    "                action = self.online_policy_network.rescale_actions(action)\n",
    "                action = action.detach().cpu().numpy().reshape(self.env.action_space.high.shape)\n",
    "        return action\n",
    "    \n",
    "   \n",
    "    \n",
    "    def update_targets(self, online_q_network_, target_q_network_, tau):\n",
    "        for target_weights, online_weights in zip(target_q_network_.parameters(), online_q_network_.parameters()):\n",
    "            target_weight_update = (1.0 - tau)*target_weights.data\n",
    "            online_weight_update = tau*online_weights.data\n",
    "            sum_up = target_weight_update + online_weight_update\n",
    "            target_weights.data.copy_(sum_up)\n",
    "        return target_q_network_\n",
    "    \n",
    "    def train(self):\n",
    "        reward_per_ep = []\n",
    "        for e in tqdm(range(self.n_ep)):\n",
    "            state = self.env.reset()\n",
    "            reward_accumulated = 0\n",
    "            while True:\n",
    "                self.env.render()\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                td_error = self.query_error(state, action, next_state, reward)\n",
    "                reward_accumulated+=reward\n",
    "                is_truncated = 'TimeLimit.truncated' in info and\\\n",
    "                                info['TimeLimit.truncated']\n",
    "                is_failure = done and not is_truncated\n",
    "                self.agent_memory.push(self.current_timestep,\\\n",
    "                                       self.Xp(state, action, next_state, reward, is_failure, td_error, self.current_timestep))\n",
    "                state = next_state\n",
    "                if self.agent_memory.can_provide_sample(self.min_sample_size):\n",
    "                    experience_samples = self.agent_memory.sample(self.batch_size)\n",
    "                    weighted_importance, indices =\\\n",
    "                                self.prioritize_samples(experience_samples, self.alpha_pr, self.beta_pr)\n",
    "                    experiences_rebuilded, timesteps = \\\n",
    "                                self.update_model(experience_samples, weighted_importance, indices)\n",
    "                    self.agent_memory.update_td_error(experiences_rebuilded, timesteps)\n",
    "                    if self.current_timestep % self.target_update == 0:\n",
    "                        self.target_q_network_a = self.update_targets(self.online_q_network_a,self.target_q_network_a, self.tau)\n",
    "                        self.target_q_network_b = self.update_targets(self.online_q_network_b,self.target_q_network_b, self.tau)\n",
    "                    if done == True:\n",
    "                        reward_per_ep.append(reward_accumulated)\n",
    "                        break\n",
    "                    if self.current_timestep > self.max_steps:\n",
    "                        self.env.close()\n",
    "                        break\n",
    "                        return reward_per_ep\n",
    "                self.current_timestep+=1\n",
    "        self.env.close()\n",
    "        return reward_per_ep         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCGP(nn.Module):\n",
    "    def __init__(self,env, observation_space, action_space, hidden_dims=(32,32),\\\n",
    "                log_alpha_lr=0.001, min_log= -20, max_log=2):\n",
    "        super(FCGP, self).__init__()\n",
    "        self.input_size = observation_space\n",
    "        self.env = env\n",
    "        self.distribution_out = action_space\n",
    "        self.mean_out = action_space\n",
    "        self.log_alpha_lr = log_alpha_lr\n",
    "        self.min_log = min_log\n",
    "        self.max_log = max_log\n",
    "        self.input_layer = nn.Linear(self.input_size, hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i],\\\n",
    "                                    hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        \n",
    "        self.mean_layer = nn.Linear(hidden_dims[-1], action_space)#predict mean\n",
    "        self.distribution_layer = nn.Linear(hidden_dims[-1], action_space)#predict distribution\n",
    "        \n",
    "        self.target_entropy =  -1 * torch.tensor(np.prod(env.action_space.high.shape)).float()#recommended target entropy\n",
    "        self.log_alpha = torch.zeros(1, requires_grad=True)\n",
    "        self.target_entropy_optimizer = torch.optim.Adam([self.log_alpha],\\\n",
    "                                                         lr=log_alpha_lr)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available()\\\n",
    "                                  else \"cpu\")\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state).float().to(self.device)\n",
    "        x = F.relu(self.input_layer(state))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x))\n",
    "        mean_output = self.mean_layer(x)\n",
    "        log_distribution_output = self.distribution_layer(x)\n",
    "        log_distribution_output = log_distribution_output.clamp(self.min_log, self.max_log)#clamp log values so that they wont explode\n",
    "        return mean_output, log_distribution_output\n",
    "    \n",
    "    def rescale_actions(self, x):\n",
    "        tan_min = torch.tanh(torch.Tensor([float('-inf')])).to(self.device)\n",
    "        tan_max = torch.tanh(torch.Tensor([float('inf')])).to(self.device)\n",
    "        env_high = torch.tensor(self.env.action_space.high).float().to(self.device)\n",
    "        env_low = torch.tensor(self.env.action_space.low).float().to(self.device)\n",
    "        rescale_fn = lambda x: (x - tan_min) * (env_high - env_low)/\\\n",
    "                                     (tan_max - tan_min) + env_low\n",
    "        x = rescale_fn(x)\n",
    "        return x.to(self.device)\n",
    "        \n",
    "    \n",
    "    def full_pass(self, state, epsilon=1e-6):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state).float().to(self.device)\n",
    "        mean, log_distribution = self.forward(state)\n",
    "        pi_s = Normal(mean, log_distribution.exp())\n",
    "        sampled_distributions = pi_s.rsample()\n",
    "        tan_h_actions = torch.tanh(sampled_distributions)\n",
    "        \n",
    "        rescaled_actions = self.rescale_actions(tan_h_actions)\n",
    "        log_probs = pi_s.log_prob(sampled_distributions) - torch.log((\\\n",
    "                                                            1 - tan_h_actions.pow(2)).clamp(0,1) + epsilon)\n",
    "        log_probs = log_probs.sum(dim=1, keepdim=True)\n",
    "        return rescaled_actions, mean, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCQV(nn.Module):\n",
    "    def __init__(self, observation_space, action_space, hidden_dims=(32,32)):\n",
    "        super(FCQV, self).__init__()\n",
    "        self.input_size = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.input_layer = nn.Linear(self.input_size + self.action_space,\\\n",
    "                                    hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], 1)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available()\\\n",
    "                                  else \"cpu\")\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state).float().to(self.device)\n",
    "        if not isinstance(action, torch.Tensor):\n",
    "            action = torch.tensor(action).float().to(self.device)\n",
    "        x = torch.cat((state, action), dim=1)\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x))\n",
    "        x = self.output_layer(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grad_flow(named_parameters):\n",
    "    ave_grads = []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean().detach().cpu().numpy())\n",
    "    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(xmin=0, xmax=len(ave_grads))\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_network(online_network, target_network):\n",
    "    for online_weights, target_weights in zip(online_network.parameters(),\\\n",
    "                                              target_network.parameters()):\n",
    "        target_weights.data.copy_(online_weights.data)\n",
    "    return target_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SAC_PER(env,\n",
    "         gamma=0.99,\n",
    "         alpha_pr=0.4,\n",
    "         beta_pr=0.3,\n",
    "         memory_size = 1000000,\n",
    "         tau = 0.01,\n",
    "         n_ep=120,\n",
    "         max_steps = 100000,\n",
    "         max_steps_per_ep = 300,\n",
    "         warm_up=2,\n",
    "         min_sample_size=600,\n",
    "         batch_size = 50,\n",
    "         offline_update = 50,\n",
    "         policy_update=2\n",
    "         ):\n",
    "    \n",
    "    \n",
    "    observation_space = len(env.reset())\n",
    "    action_space_high, action_space_low = env.action_space.high, env.action_space.low\n",
    "    n_actions = len(action_space_high)\n",
    "    online_policy_network = FCGP(env,observation_space,n_actions,\\\n",
    "                                     hidden_dims=(128,64,64))\n",
    "    \n",
    "    online_q_network_a = FCQV(observation_space,\\\n",
    "                                     n_actions,hidden_dims=(128,64,64))\n",
    "    online_q_network_b = FCQV(observation_space,\\\n",
    "                                     n_actions,hidden_dims=(128,64,64))\n",
    "    \n",
    "    target_q_network_a = FCQV(observation_space,\\\n",
    "                                     n_actions,hidden_dims=(128,64,64))\n",
    "    target_q_network_b = FCQV(observation_space,\\\n",
    "                                     n_actions,hidden_dims=(128,64,64))\n",
    "    \n",
    "    #copy parameters from online to target\n",
    "    target_q_network_a = copy_network(online_q_network_a, target_q_network_a)\n",
    "    target_q_network_b = copy_network(online_q_network_b, target_q_network_b)\n",
    "    \n",
    "    target_q_network_a.eval()\n",
    "    target_q_network_b.eval()\n",
    "    \n",
    "    online_policy_optimizer = torch.optim.Adam(online_policy_network.parameters(),lr=0.0008)\n",
    "    online_qa_network_optimizer = torch.optim.Adam(online_q_network_a.parameters(),lr=0.0008)\n",
    "    online_qb_network_optimizer = torch.optim.Adam(online_q_network_b.parameters(),lr=0.0008)\n",
    "    \n",
    "    agent = Agent(env, ReplayMemory, online_policy_network, online_q_network_a, online_q_network_b,\\\n",
    "                online_policy_optimizer,online_qa_network_optimizer, online_qb_network_optimizer, target_q_network_a,\\\n",
    "                target_q_network_b, gamma=0.99, alpha_pr=0.4, beta_pr=0.3,\\\n",
    "                tau=0.01, n_ep=120, max_steps=100000, memory_size=1000000, batch_size=50,\\\n",
    "                policy_update_step=2, target_update=5, min_sample_size=500, warm_up=2)\n",
    "    rewards = agent.train() \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\")#baseline_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███████████████████████████████▋                                                 | 47/120 [02:19<04:10,  3.43s/it]"
     ]
    }
   ],
   "source": [
    "rewards = SAC_PER(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_grad_flow(a.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = uniform_filter1d(rewards, size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for i,e in enumerate(arr):\n",
    "    y.append(i)\n",
    "    x.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x14081da31c8>]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y, x)#u=500, T=0.001 pu:50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
