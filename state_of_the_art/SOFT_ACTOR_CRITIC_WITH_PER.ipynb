{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.distributions import Normal\n",
    "from scipy.ndimage.filters import uniform_filter1d\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "import random as rand\n",
    "from itertools import count\n",
    "import time\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self,capacity):   \n",
    "        self.capacity = capacity\n",
    "        self.memory = {}\n",
    "        self.push_count = 0\n",
    "        \n",
    "    def push(self, timestep, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory[timestep] = experience\n",
    "        else:\n",
    "            del_key = None\n",
    "            for k in self.memory.keys():\n",
    "                del_key = k\n",
    "                break\n",
    "            self.memory.pop(del_key, None)\n",
    "            self.memory[timestep] = experience\n",
    "        self.push_count+=1\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch_tuple = random.sample(self.memory.items(), batch_size)#key(timestep), experiences\n",
    "        experience_array = []\n",
    "        for timestep, experiences in batch_tuple:\n",
    "            experience_array.append(experiences)\n",
    "        return experience_array\n",
    "    \n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory)>=batch_size\n",
    "    \n",
    "    def update_td_error(self, sampled_experiences, timesteps):\n",
    "        for i in range(len(timesteps)):\n",
    "            self.memory[timesteps[i]] = sampled_experiences[i]\n",
    "        \n",
    "    def get_memory_values(self):\n",
    "        return self.memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env, ReplayMemory, online_policy, online_q_a, online_q_b,\\\n",
    "                online_policy_optimizer,online_q_network_optimizer_a, online_q_network_optimizer_b, target_q_a,\\\n",
    "                target_q_b, gamma=0.99, alpha_pr=0.4, beta_pr=0.3,\\\n",
    "                tau=0.01, n_ep=120, max_steps=100000, memory_size=1000000, batch_size=50,\\\n",
    "                policy_update_step=2, target_update=50, min_sample_size=500, warm_up=2):\n",
    "        \n",
    "        \n",
    "        self.env = env\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.alpha_pr = alpha_pr\n",
    "        self.beta_pr = beta_pr\n",
    "        self.agent_max_memory=memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.n_ep = n_ep\n",
    "        self.max_steps = max_steps\n",
    "        self.min_sample_size=min_sample_size\n",
    "        self.target_update=target_update\n",
    "        self.policy_update_step=policy_update_step\n",
    "        self.online_policy_network = online_policy\n",
    "        self.online_policy_optimizer = online_policy_optimizer\n",
    "        self.online_q_network_a = online_q_a\n",
    "        self.online_q_network_optimizer_a = online_q_network_optimizer_a\n",
    "        self.online_q_network_b = online_q_b\n",
    "        self.online_q_network_optimizer_b = online_q_network_optimizer_b\n",
    "        self.target_q_network_a = target_q_a\n",
    "        self.target_q_network_b = target_q_b\n",
    "        self.current_timestep = 0\n",
    "        self.warm_up = warm_up\n",
    "        self.agent_memory = ReplayMemory(self.agent_max_memory)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available()\\\n",
    "                        else \"cpu\")\n",
    "        self.Xp = namedtuple('Experience',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done', 'abs_td_error','timestep'))\n",
    "        \n",
    "    def extract_tensors(self, experiences):\n",
    "        batch = self.Xp(*zip(*experiences))\n",
    "        state = np.stack(batch.state) #stack\n",
    "        action = np.stack(batch.action)\n",
    "        next_state = np.stack(batch.next_state)\n",
    "        reward = np.stack(batch.reward)\n",
    "        done = np.stack(batch.done)\n",
    "        abs_td_error = np.stack(batch.abs_td_error)\n",
    "        timestep = np.stack(batch.timestep)\n",
    "        return state,action,next_state,reward,done,abs_td_error,timestep\n",
    "    \n",
    "    def rebuild_experiences(self, state, action, next_state, reward, done, abs_error, timestep):\n",
    "        exp_list = []\n",
    "        for idx_ in range(len(state)):\n",
    "            exp_list.append(\\\n",
    "                        self.Xp(state[idx_], action[idx_], next_state[idx_], reward[idx_],\\\n",
    "                           done[idx_], abs_error[idx_], timestep[idx_]))\n",
    "        return exp_list \n",
    "    \n",
    "    def prioritize_samples(self, experience_samples, alpha, beta):\n",
    "        state,action,next_state,reward,done,abs_td_error,timesteps \\\n",
    "                            = self.extract_tensors(experience_samples)\n",
    "        abs_td_error, indices_ = (list(t) for t in zip(*sorted(\\\n",
    "                            zip(abs_td_error.tolist(), timesteps))))\n",
    "        abs_td_error.reverse()\n",
    "        indices_.reverse()#reverse to march sort func\n",
    "        abs_td_error = np.array(abs_td_error)\n",
    "        abs_td_error  = torch.tensor(abs_td_error)\n",
    "        ranks = np.arange(1, len(abs_td_error)+1)\n",
    "        priorities = 1.0/ranks\n",
    "        priorities = priorities**alpha\n",
    "        priorities = np.expand_dims(priorities, axis=1)\n",
    "        probabilities = priorities/np.sum(priorities, axis=0)\n",
    "        assert np.isclose(probabilities.sum(), 1.0)\n",
    "        number_of_samples  = len(probabilities)\n",
    "        weight_importance_ = number_of_samples*probabilities\n",
    "        weight_importance_ = weight_importance_**-beta\n",
    "        weight_importance_max = np.max(weight_importance_)\n",
    "        weight_importance_scaled = weight_importance_/weight_importance_max\n",
    "        return weight_importance_scaled, indices_ \n",
    "    \n",
    "    def update_model(self, experience_samples,\\\n",
    "                weighted_importance, timestep_indices):\n",
    "        \n",
    "        states, actions, next_states, rewards, done, _ , timesteps =\\\n",
    "                    self.extract_tensors(experience_samples)\n",
    "         \n",
    "        arrange_weighted_values = [timestep_indices.index(i) for i in timesteps]\n",
    "    \n",
    "        states = torch.tensor(np.squeeze(states)).float().to(self.device)\n",
    "        next_states = torch.tensor(np.squeeze(next_states)).float().to(self.device)\n",
    "        actions = torch.tensor(actions).float().to(self.device)\n",
    "        rewards = torch.tensor(rewards).unsqueeze(1).float().to(self.device)\n",
    "        done = torch.tensor(done).unsqueeze(1).float().to(self.device)\n",
    "        weighted_importance = torch.tensor(weighted_importance).float().to(self.device)\n",
    "    \n",
    "        #optimize alpha\n",
    "        current_actions,_, log_pi_s = self.online_policy_network.full_pass(states)\n",
    "        target_alpha = (self.online_policy_network.target_entropy +\\\n",
    "                   log_pi_s).detach()\n",
    "        alpha_loss = -(self.online_policy_network.log_alpha*\\\n",
    "                 target_alpha).mean()\n",
    "        self.online_policy_network.target_entropy_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.online_policy_network.target_entropy_optimizer.step()\n",
    "    \n",
    "        #set alpha\n",
    "        alpha = self.online_policy_network.log_alpha.exp()\n",
    "    \n",
    "        #optimize online using target nets\n",
    "        predicted_action_policy,_, log_pi_ns =\\\n",
    "                        self.online_policy_network.full_pass(next_states)\n",
    "\n",
    "        qsa_target_a = self.target_q_network_a(next_states,\\\n",
    "                                        predicted_action_policy)\n",
    "        qsa_target_b = self.target_q_network_b(next_states,\\\n",
    "                                        predicted_action_policy)\n",
    "        target_qsa = torch.min(qsa_target_a, qsa_target_b)\n",
    "        target_qsa = target_qsa - alpha*log_pi_ns\n",
    "        target_qsa = rewards + self.gamma*target_qsa*(1 - done)\n",
    "        target_qsa = target_qsa*weighted_importance\n",
    "        target_qsa = target_qsa.detach()\n",
    "        qsa_online_a = self.online_q_network_a(states, actions)\n",
    "        qsa_online_b = self.online_q_network_b(states, actions)\n",
    "        qsa_online_a = qsa_online_a*weighted_importance.detach()\n",
    "        qsa_online_b = qsa_online_b*weighted_importance.detach()\n",
    "    \n",
    "        #update online networks\n",
    "        loss_func = torch.nn.SmoothL1Loss()\n",
    "        qa_loss = loss_func(qsa_online_a,\\\n",
    "                            target_qsa.detach())\n",
    "        qb_loss = loss_func(qsa_online_b,\\\n",
    "                            target_qsa.detach())\n",
    "\n",
    "        self.online_q_network_optimizer_a.zero_grad()\n",
    "        qa_loss.backward()\n",
    "        self.online_q_network_optimizer_a.step()\n",
    "    \n",
    "        self.online_q_network_optimizer_b.zero_grad()\n",
    "        qb_loss.backward()\n",
    "        self.online_q_network_optimizer_b.step()\n",
    "    \n",
    "        abs_a = (target_qsa.squeeze() - qsa_online_a.squeeze())\n",
    "        abs_b = (target_qsa.squeeze() - qsa_online_b.squeeze())\n",
    "        ovr_update = (abs_a + abs_b)/2\n",
    "        ovr_update = abs(ovr_update.detach().cpu().numpy())\n",
    "        if self.current_timestep % self.policy_update_step == 0:\n",
    "            current_actions,_ , log_pi = self.online_policy_network.full_pass(states)\n",
    "            qsa_online_a = self.online_q_network_a(states, current_actions)\n",
    "            qsa_online_b = self.online_q_network_b(states, current_actions)\n",
    "            qsa_min = torch.min(qsa_online_a, qsa_online_b)\n",
    "            policy_loss = (alpha*log_pi\\\n",
    "                      -qsa_min).mean()\n",
    "            self.online_policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.online_policy_optimizer.step()\n",
    "        states, actions, next_states, rewards, done, td_err , timesteps =\\\n",
    "                self.extract_tensors(experience_samples)    \n",
    "        experiences_rebuilded = self.rebuild_experiences\\\n",
    "                (states, actions, next_states, rewards, done, ovr_update, timesteps)\n",
    "        return experiences_rebuilded, timesteps\n",
    "    \n",
    "    def query_error(self, state, action, next_state, reward):\n",
    "\n",
    "        state = torch.tensor(state).unsqueeze(0).float().to(self.device)\n",
    "        next_state = torch.tensor(next_state).unsqueeze(0).float().to(self.device)\n",
    "        alpha = self.online_policy_network.log_alpha.exp()\n",
    "        ns_actions,_, log_pi_ns = self.online_policy_network.full_pass(next_state)\n",
    "        q_target_next_states_action_a = self.target_q_network_a(next_state,\\\n",
    "                                                    ns_actions.detach())\n",
    "        q_target_next_states_action_b = self.target_q_network_b(next_state,\\\n",
    "                                                    ns_actions.detach())\n",
    "        q_min = torch.min(q_target_next_states_action_a, q_target_next_states_action_b)\n",
    "        q_target = q_min - alpha * log_pi_ns\n",
    "        q_target = reward + (self.gamma*q_target.detach())\n",
    "        action = np.expand_dims(action, axis=0)\n",
    "        q_online_state_action_val_a = self.online_q_network_a(state, action)\n",
    "        q_online_state_action_val_b = self.online_q_network_b(state, action)\n",
    "        abs_a = (q_target - q_online_state_action_val_a)\n",
    "        abs_b = (q_target - q_online_state_action_val_b)\n",
    "        abs_stack = (abs_a + abs_b)/2\n",
    "        ovr_abs_update = abs_stack\n",
    "        ovr_abs_update = ovr_abs_update.squeeze()\n",
    "        return np.absolute(ovr_abs_update.detach().cpu().numpy())\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state).float().to(self.device)\n",
    "        state = state.unsqueeze(0)\n",
    "        warm_up_action = self.batch_size * self.warm_up\n",
    "        if self.agent_memory.can_provide_sample(warm_up_action) == False:\n",
    "            action = np.random.uniform(low=self.env.action_space.low,\\\n",
    "                                       high=self.env.action_space.high)\n",
    "            action = action.reshape(self.env.action_space.high.shape)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                mean,log_std = self.online_policy_network.forward(state)\n",
    "                action = torch.tanh(Normal(mean, log_std.exp()).sample())\n",
    "                action = self.online_policy_network.rescale_actions(action)\n",
    "                action = action.detach().cpu().numpy().reshape(self.env.action_space.high.shape)\n",
    "        return action\n",
    "    \n",
    "    def update_targets(self, online_q_network_, target_q_network_, tau):\n",
    "        for target_weights, online_weights in zip(target_q_network_.parameters(),\\\n",
    "                                                  online_q_network_.parameters()):\n",
    "            target_weight_update = (1.0 - tau)*target_weights.data\n",
    "            online_weight_update = tau*online_weights.data\n",
    "            sum_up = target_weight_update + online_weight_update\n",
    "            target_weights.data.copy_(sum_up)\n",
    "        return target_q_network_\n",
    "    \n",
    "    def train(self):\n",
    "        reward_per_ep = []\n",
    "        for e in tqdm(range(self.n_ep)):\n",
    "            state = self.env.reset()\n",
    "            reward_accumulated = 0\n",
    "            while True:\n",
    "                self.env.render()\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                td_error = self.query_error(state, action, next_state, reward)\n",
    "                reward_accumulated+=reward\n",
    "                is_truncated = 'TimeLimit.truncated' in info and\\\n",
    "                                info['TimeLimit.truncated']\n",
    "                is_failure = done and not is_truncated\n",
    "                self.agent_memory.push(self.current_timestep,\\\n",
    "                            self.Xp(state, action, next_state,\\\n",
    "                            reward, is_failure, td_error, self.current_timestep))\n",
    "                state = next_state\n",
    "                if self.agent_memory.can_provide_sample(self.min_sample_size):\n",
    "                    experience_samples = self.agent_memory.sample(self.batch_size)\n",
    "                    weighted_importance, indices =\\\n",
    "                            self.prioritize_samples(experience_samples, self.alpha_pr, self.beta_pr)\n",
    "                    experiences_rebuilded, timesteps = \\\n",
    "                            self.update_model(experience_samples, weighted_importance, indices)\n",
    "                    self.agent_memory.update_td_error(experiences_rebuilded, timesteps)\n",
    "                    if self.current_timestep % self.target_update == 0:\n",
    "                        self.target_q_network_a =\\\n",
    "                            self.update_targets(self.online_q_network_a,self.target_q_network_a, self.tau)\n",
    "                        self.target_q_network_b =\\\n",
    "                            self.update_targets(self.online_q_network_b,self.target_q_network_b, self.tau)\n",
    "                    if done == True:\n",
    "                        reward_per_ep.append(reward_accumulated)\n",
    "                        break\n",
    "                    if self.current_timestep > self.max_steps:\n",
    "                        self.env.close()\n",
    "                        break\n",
    "                        return reward_per_ep\n",
    "                self.current_timestep+=1\n",
    "        self.env.close()\n",
    "        return reward_per_ep         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCGP(nn.Module):\n",
    "    def __init__(self,env, observation_space, action_space, hidden_dims=(32,32),\\\n",
    "                log_alpha_lr=0.001, min_log= -20, max_log=2):\n",
    "        super(FCGP, self).__init__()\n",
    "        self.input_size = observation_space\n",
    "        self.env = env\n",
    "        self.distribution_out = action_space\n",
    "        self.mean_out = action_space\n",
    "        self.log_alpha_lr = log_alpha_lr\n",
    "        self.min_log = min_log\n",
    "        self.max_log = max_log\n",
    "        self.input_layer = nn.Linear(self.input_size, hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i],\\\n",
    "                                    hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        \n",
    "        self.mean_layer = nn.Linear(hidden_dims[-1], action_space)#predict mean\n",
    "        self.distribution_layer = nn.Linear(hidden_dims[-1], action_space)#predict distribution\n",
    "        \n",
    "        self.target_entropy =  -1 * torch.tensor(np.prod(env.action_space.high.shape)).float()#recommended target entropy\n",
    "        self.log_alpha = torch.zeros(1, requires_grad=True)\n",
    "        self.target_entropy_optimizer = torch.optim.Adam([self.log_alpha],\\\n",
    "                                                         lr=log_alpha_lr)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available()\\\n",
    "                                  else \"cpu\")\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state).float().to(self.device)\n",
    "        x = F.relu(self.input_layer(state))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x))\n",
    "        mean_output = self.mean_layer(x)\n",
    "        log_distribution_output = self.distribution_layer(x)\n",
    "        log_distribution_output = log_distribution_output.clamp(self.min_log, self.max_log)#clamp log values so that they wont explode\n",
    "        return mean_output, log_distribution_output\n",
    "    \n",
    "    def rescale_actions(self, x):\n",
    "        tan_min = torch.tanh(torch.Tensor([float('-inf')])).to(self.device)\n",
    "        tan_max = torch.tanh(torch.Tensor([float('inf')])).to(self.device)\n",
    "        env_high = torch.tensor(self.env.action_space.high).float().to(self.device)\n",
    "        env_low = torch.tensor(self.env.action_space.low).float().to(self.device)\n",
    "        rescale_fn = lambda x: (x - tan_min) * (env_high - env_low)/\\\n",
    "                                     (tan_max - tan_min) + env_low\n",
    "        x = rescale_fn(x)\n",
    "        return x.to(self.device)\n",
    "        \n",
    "    \n",
    "    def full_pass(self, state, epsilon=1e-6):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state).float().to(self.device)\n",
    "        mean, log_distribution = self.forward(state)\n",
    "        pi_s = Normal(mean, log_distribution.exp())\n",
    "        sampled_distributions = pi_s.rsample()\n",
    "        tan_h_actions = torch.tanh(sampled_distributions)\n",
    "        \n",
    "        rescaled_actions = self.rescale_actions(tan_h_actions)\n",
    "        log_probs = pi_s.log_prob(sampled_distributions) - torch.log((\\\n",
    "                                                            1 - tan_h_actions.pow(2)).clamp(0,1) + epsilon)\n",
    "        log_probs = log_probs.sum(dim=1, keepdim=True)\n",
    "        return rescaled_actions, mean, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCQV(nn.Module):\n",
    "    def __init__(self, observation_space, action_space, hidden_dims=(32,32)):\n",
    "        super(FCQV, self).__init__()\n",
    "        self.input_size = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.input_layer = nn.Linear(self.input_size + self.action_space,\\\n",
    "                                    hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], 1)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available()\\\n",
    "                                  else \"cpu\")\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state).float().to(self.device)\n",
    "        if not isinstance(action, torch.Tensor):\n",
    "            action = torch.tensor(action).float().to(self.device)\n",
    "        x = torch.cat((state, action), dim=1)\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x))\n",
    "        x = self.output_layer(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grad_flow(named_parameters):\n",
    "    ave_grads = []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean().detach().cpu().numpy())\n",
    "    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(xmin=0, xmax=len(ave_grads))\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_network(online_network, target_network):\n",
    "    for online_weights, target_weights in zip(online_network.parameters(),\\\n",
    "                                              target_network.parameters()):\n",
    "        target_weights.data.copy_(online_weights.data)\n",
    "    return target_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SAC_PER(env):\n",
    "    \n",
    "    \n",
    "    observation_space = len(env.reset())\n",
    "    action_space_high, action_space_low = env.action_space.high, env.action_space.low\n",
    "    n_actions = len(action_space_high)\n",
    "    online_policy_network = FCGP(env,observation_space,n_actions,\\\n",
    "                                     hidden_dims=(128,64,64))\n",
    "    \n",
    "    online_q_network_a = FCQV(observation_space,\\\n",
    "                                     n_actions,hidden_dims=(128,64,64))\n",
    "    online_q_network_b = FCQV(observation_space,\\\n",
    "                                     n_actions,hidden_dims=(128,64,64))\n",
    "    \n",
    "    target_q_network_a = FCQV(observation_space,\\\n",
    "                                     n_actions,hidden_dims=(128,64,64))\n",
    "    target_q_network_b = FCQV(observation_space,\\\n",
    "                                     n_actions,hidden_dims=(128,64,64))\n",
    "    \n",
    "    #copy parameters from online to target\n",
    "    target_q_network_a = copy_network(online_q_network_a, target_q_network_a)\n",
    "    target_q_network_b = copy_network(online_q_network_b, target_q_network_b)\n",
    "    \n",
    "    target_q_network_a.eval()\n",
    "    target_q_network_b.eval()\n",
    "    \n",
    "    online_policy_optimizer = torch.optim.Adam(online_policy_network.parameters(),lr=0.0008)\n",
    "    online_qa_network_optimizer = torch.optim.Adam(online_q_network_a.parameters(),lr=0.0008)\n",
    "    online_qb_network_optimizer = torch.optim.Adam(online_q_network_b.parameters(),lr=0.0008)\n",
    "    \n",
    "    agent = Agent(env, ReplayMemory, online_policy_network, online_q_network_a, online_q_network_b,\\\n",
    "                online_policy_optimizer,online_qa_network_optimizer, online_qb_network_optimizer, target_q_network_a,\\\n",
    "                target_q_network_b, gamma=0.99, alpha_pr=0.4, beta_pr=0.3,\\\n",
    "                tau=0.01, n_ep=120, max_steps=100000, memory_size=1000000, batch_size=50,\\\n",
    "                policy_update_step=2, target_update=5, min_sample_size=500, warm_up=2)\n",
    "    rewards = agent.train() \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\")#baseline_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 120/120 [07:21<00:00,  3.68s/it]\n"
     ]
    }
   ],
   "source": [
    "rewards = SAC_PER(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_grad_flow(a.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = uniform_filter1d(rewards, size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for i,e in enumerate(arr):\n",
    "    y.append(i)\n",
    "    x.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1408aefbc48>]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
