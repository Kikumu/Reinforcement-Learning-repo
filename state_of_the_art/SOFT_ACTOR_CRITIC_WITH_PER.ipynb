{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.distributions import Normal\n",
    "from scipy.ndimage.filters import uniform_filter1d\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "import random as rand\n",
    "from itertools import count\n",
    "import time\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self,capacity):   \n",
    "        self.capacity = capacity\n",
    "        self.memory = {}\n",
    "        self.push_count = 0\n",
    "        \n",
    "    def push(self, timestep, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory[timestep] = experience\n",
    "        else:\n",
    "            del_key = None\n",
    "            for k in self.memory.keys():\n",
    "                del_key = k\n",
    "                break\n",
    "            self.memory.pop(del_key, None)\n",
    "            self.memory[timestep] = experience\n",
    "        self.push_count+=1\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch_tuple = random.sample(self.memory.items(), batch_size)#key(timestep), experiences\n",
    "        experience_array = []\n",
    "        for timestep, experiences in batch_tuple:\n",
    "            experience_array.append(experiences)\n",
    "        return experience_array\n",
    "    \n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory)>=batch_size\n",
    "    \n",
    "    def update_td_error(self, sampled_experiences, timesteps):\n",
    "        for i in range(len(timesteps)):\n",
    "            self.memory[timesteps[i]] = sampled_experiences[i]\n",
    "        \n",
    "    def get_memory_values(self):\n",
    "        return self.memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env, ReplayMemory, online_policy, online_q_a, online_q_b,\\\n",
    "                online_policy_optimizer,online_q_network_optimizer_a, online_q_network_optimizer_b, target_q_a,\\\n",
    "                target_q_b, gamma=0.99, alpha_pr=0.4, beta_pr=0.3,\\\n",
    "                tau=0.01, n_ep=120, max_steps=100000, memory_size=1000000, batch_size=50,\\\n",
    "                policy_update_step=2, target_update=50, min_sample_size=500, warm_up=2, rank_based_prioritization=True):\n",
    "        \n",
    "        \n",
    "        self.env = env\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.alpha_pr = alpha_pr\n",
    "        self.beta_pr = beta_pr\n",
    "        self.agent_max_memory=memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.n_ep = n_ep\n",
    "        self.max_steps = max_steps\n",
    "        self.min_sample_size=min_sample_size\n",
    "        self.target_update=target_update\n",
    "        self.policy_update_step=policy_update_step\n",
    "        self.online_policy_network = online_policy\n",
    "        self.online_policy_optimizer = online_policy_optimizer\n",
    "        self.online_q_network_a = online_q_a\n",
    "        self.online_q_network_optimizer_a = online_q_network_optimizer_a\n",
    "        self.online_q_network_b = online_q_b\n",
    "        self.online_q_network_optimizer_b = online_q_network_optimizer_b\n",
    "        self.target_q_network_a = target_q_a\n",
    "        self.target_q_network_b = target_q_b\n",
    "        self.current_timestep = 0\n",
    "        self.warm_up = warm_up\n",
    "        self.agent_memory = ReplayMemory(self.agent_max_memory)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available()\\\n",
    "                        else \"cpu\")\n",
    "        self.Xp = namedtuple('Experience',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done', 'abs_td_error','timestep'))\n",
    "        self.rank_based = rank_based_prioritization\n",
    "        \n",
    "    def extract_tensors(self, experiences):\n",
    "        batch = self.Xp(*zip(*experiences))\n",
    "        state = np.stack(batch.state) #stack\n",
    "        action = np.stack(batch.action)\n",
    "        next_state = np.stack(batch.next_state)\n",
    "        reward = np.stack(batch.reward)\n",
    "        done = np.stack(batch.done)\n",
    "        abs_td_error = np.stack(batch.abs_td_error)\n",
    "        timestep = np.stack(batch.timestep)\n",
    "        return state,action,next_state,reward,done,abs_td_error,timestep\n",
    "    \n",
    "    def rebuild_experiences(self, state, action, next_state, reward, done, abs_error, timestep):\n",
    "        exp_list = []\n",
    "        for idx_ in range(len(state)):\n",
    "            exp_list.append(\\\n",
    "                        self.Xp(state[idx_], action[idx_], next_state[idx_], reward[idx_],\\\n",
    "                           done[idx_], abs_error[idx_], timestep[idx_]))\n",
    "        return exp_list \n",
    "    \n",
    "    def prioritize_samples(self, experience_samples, alpha, beta,\\\n",
    "                           epsilon = 1e-6):\n",
    "        state,action,next_state,reward,done,abs_td_error,timesteps \\\n",
    "                            = self.extract_tensors(experience_samples)\n",
    "        \n",
    "        if self.rank_based == True:\n",
    "            abs_td_error, indices_ = (list(t) for t in zip(*sorted(\\\n",
    "                            zip(abs_td_error.tolist(), timesteps))))\n",
    "            abs_td_error.reverse()\n",
    "            indices_.reverse()#reverse to march sort func\n",
    "            abs_td_error = np.array(abs_td_error)\n",
    "            abs_td_error  = torch.tensor(abs_td_error)\n",
    "            ranks = np.arange(1, len(abs_td_error)+1)\n",
    "            priorities = 1.0/ranks\n",
    "        else:\n",
    "            priorities = abs_td_error + epsilon\n",
    "            indices_ = None\n",
    "        \n",
    "        priorities = priorities**alpha\n",
    "        priorities = np.expand_dims(priorities, axis=1)\n",
    "        probabilities = priorities/np.sum(priorities, axis=0)\n",
    "        assert np.isclose(probabilities.sum(), 1.0)\n",
    "        number_of_samples  = len(probabilities)\n",
    "        weight_importance_ = number_of_samples*probabilities\n",
    "        weight_importance_ = weight_importance_**-beta\n",
    "        weight_importance_max = np.max(weight_importance_)\n",
    "        weight_importance_scaled = weight_importance_/weight_importance_max\n",
    "        return weight_importance_scaled, indices_ \n",
    "    \n",
    "    def update_model(self, experience_samples,\\\n",
    "                weighted_importance, timestep_indices):\n",
    "        \n",
    "        states, actions, next_states, rewards, done, _ , timesteps =\\\n",
    "                    self.extract_tensors(experience_samples)\n",
    "        \n",
    "        if self.rank_based == True:\n",
    "            arrange_weighted_values = [timestep_indices.index(i) for i in timesteps]\n",
    "        \n",
    "        states = torch.tensor(np.squeeze(states)).float().to(self.device)\n",
    "        next_states = torch.tensor(np.squeeze(next_states)).float().to(self.device)\n",
    "        actions = torch.tensor(actions).float().to(self.device)\n",
    "        rewards = torch.tensor(rewards).unsqueeze(1).float().to(self.device)\n",
    "        done = torch.tensor(done).unsqueeze(1).float().to(self.device)\n",
    "        weighted_importance = torch.tensor(weighted_importance).float().to(self.device)\n",
    "    \n",
    "        #optimize alpha\n",
    "        current_actions,_, log_pi_s = self.online_policy_network.full_pass(states)\n",
    "        target_alpha = (self.online_policy_network.target_entropy +\\\n",
    "                   log_pi_s).detach()\n",
    "        alpha_loss = -(self.online_policy_network.log_alpha*\\\n",
    "                 target_alpha).mean()\n",
    "        self.online_policy_network.target_entropy_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.online_policy_network.target_entropy_optimizer.step()\n",
    "    \n",
    "        #set alpha\n",
    "        alpha = self.online_policy_network.log_alpha.exp()\n",
    "    \n",
    "        #optimize online using target nets\n",
    "        predicted_action_policy,_, log_pi_ns =\\\n",
    "                        self.online_policy_network.full_pass(next_states)\n",
    "\n",
    "        qsa_target_a = self.target_q_network_a(next_states,\\\n",
    "                                        predicted_action_policy)\n",
    "        qsa_target_b = self.target_q_network_b(next_states,\\\n",
    "                                        predicted_action_policy)\n",
    "        target_qsa = torch.min(qsa_target_a, qsa_target_b)\n",
    "        target_qsa = target_qsa - alpha*log_pi_ns\n",
    "        target_qsa = rewards + self.gamma*target_qsa*(1 - done)\n",
    "        target_qsa = target_qsa*weighted_importance\n",
    "        #print(weighted_importance)\n",
    "        target_qsa = target_qsa.detach()\n",
    "        qsa_online_a = self.online_q_network_a(states, actions)\n",
    "        qsa_online_b = self.online_q_network_b(states, actions)\n",
    "        qsa_online_a = qsa_online_a*weighted_importance.detach()\n",
    "        qsa_online_b = qsa_online_b*weighted_importance.detach()\n",
    "    \n",
    "        #update online networks\n",
    "        loss_func = torch.nn.SmoothL1Loss()\n",
    "        qa_loss = loss_func(qsa_online_a,\\\n",
    "                            target_qsa.detach())\n",
    "        qb_loss = loss_func(qsa_online_b,\\\n",
    "                            target_qsa.detach())\n",
    "\n",
    "        self.online_q_network_optimizer_a.zero_grad()\n",
    "        qa_loss.backward()\n",
    "        self.online_q_network_optimizer_a.step()\n",
    "    \n",
    "        self.online_q_network_optimizer_b.zero_grad()\n",
    "        qb_loss.backward()\n",
    "        self.online_q_network_optimizer_b.step()\n",
    "    \n",
    "        abs_a = (target_qsa.squeeze() - qsa_online_a.squeeze())\n",
    "        abs_b = (target_qsa.squeeze() - qsa_online_b.squeeze())\n",
    "        ovr_update = (abs_a + abs_b)/2\n",
    "        ovr_update = abs(ovr_update.detach().cpu().numpy())\n",
    "        if self.current_timestep % self.policy_update_step == 0:\n",
    "            current_actions,_ , log_pi = self.online_policy_network.full_pass(states)\n",
    "            qsa_online_a = self.online_q_network_a(states, current_actions)\n",
    "            qsa_online_b = self.online_q_network_b(states, current_actions)\n",
    "            qsa_min = torch.min(qsa_online_a, qsa_online_b)\n",
    "            policy_loss = (alpha*log_pi\\\n",
    "                      -qsa_min).mean()\n",
    "            self.online_policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.online_policy_optimizer.step()\n",
    "        states, actions, next_states, rewards, done, td_err , timesteps =\\\n",
    "                self.extract_tensors(experience_samples)    \n",
    "        experiences_rebuilded = self.rebuild_experiences\\\n",
    "                (states, actions, next_states, rewards, done, ovr_update, timesteps)\n",
    "        return experiences_rebuilded, timesteps\n",
    "    \n",
    "    def query_error(self, state, action, next_state, reward):\n",
    "\n",
    "        state = torch.tensor(state).unsqueeze(0).float().to(self.device)\n",
    "        next_state = torch.tensor(next_state).unsqueeze(0).float().to(self.device)\n",
    "        alpha = self.online_policy_network.log_alpha.exp()\n",
    "        ns_actions,_, log_pi_ns = self.online_policy_network.full_pass(next_state)\n",
    "        q_target_next_states_action_a = self.target_q_network_a(next_state,\\\n",
    "                                                    ns_actions.detach())\n",
    "        q_target_next_states_action_b = self.target_q_network_b(next_state,\\\n",
    "                                                    ns_actions.detach())\n",
    "        q_min = torch.min(q_target_next_states_action_a, q_target_next_states_action_b)\n",
    "        q_target = q_min - alpha * log_pi_ns\n",
    "        q_target = reward + (self.gamma*q_target.detach())\n",
    "        action = np.expand_dims(action, axis=0)\n",
    "        q_online_state_action_val_a = self.online_q_network_a(state, action)\n",
    "        q_online_state_action_val_b = self.online_q_network_b(state, action)\n",
    "        abs_a = (q_target - q_online_state_action_val_a)\n",
    "        abs_b = (q_target - q_online_state_action_val_b)\n",
    "        abs_stack = (abs_a + abs_b)/2\n",
    "        ovr_abs_update = abs_stack\n",
    "        ovr_abs_update = ovr_abs_update.squeeze()\n",
    "        return np.absolute(ovr_abs_update.detach().cpu().numpy())\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state).float().to(self.device)\n",
    "        state = state.unsqueeze(0)\n",
    "        warm_up_action = self.batch_size * self.warm_up\n",
    "        if self.agent_memory.can_provide_sample(warm_up_action) == False:\n",
    "            action = np.random.uniform(low=self.env.action_space.low,\\\n",
    "                                       high=self.env.action_space.high)\n",
    "            action = action.reshape(self.env.action_space.high.shape)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                mean,log_std = self.online_policy_network.forward(state)\n",
    "                action = torch.tanh(Normal(mean, log_std.exp()).sample())\n",
    "                action = self.online_policy_network.rescale_actions(action)\n",
    "                action = action.detach().cpu().numpy().reshape(self.env.action_space.high.shape)\n",
    "        return action\n",
    "    \n",
    "    def update_targets(self, online_q_network_, target_q_network_, tau):\n",
    "        for target_weights, online_weights in zip(target_q_network_.parameters(),\\\n",
    "                                                  online_q_network_.parameters()):\n",
    "            target_weight_update = (1.0 - tau)*target_weights.data\n",
    "            online_weight_update = tau*online_weights.data\n",
    "            sum_up = target_weight_update + online_weight_update\n",
    "            target_weights.data.copy_(sum_up)\n",
    "        return target_q_network_\n",
    "    \n",
    "    def train(self):\n",
    "        reward_per_ep = []\n",
    "        for e in tqdm(range(self.n_ep)):\n",
    "            state = self.env.reset()\n",
    "            reward_accumulated = 0\n",
    "            while True:\n",
    "                self.env.render()\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                #reward = abs(reward)\n",
    "                td_error = self.query_error(state, action, next_state, reward)\n",
    "                reward_accumulated+=reward\n",
    "                is_truncated = 'TimeLimit.truncated' in info and\\\n",
    "                                info['TimeLimit.truncated']\n",
    "                is_failure = done and not is_truncated\n",
    "                self.agent_memory.push(self.current_timestep,\\\n",
    "                            self.Xp(state, action, next_state,\\\n",
    "                            reward, is_failure, td_error, self.current_timestep))\n",
    "                state = next_state\n",
    "                if self.agent_memory.can_provide_sample(self.min_sample_size):\n",
    "                    experience_samples = self.agent_memory.sample(self.batch_size)\n",
    "                    weighted_importance, indices =\\\n",
    "                            self.prioritize_samples(experience_samples, self.alpha_pr, self.beta_pr)\n",
    "                    experiences_rebuilded, timesteps = \\\n",
    "                            self.update_model(experience_samples, weighted_importance, indices)\n",
    "                    self.agent_memory.update_td_error(experiences_rebuilded, timesteps)\n",
    "                    if self.current_timestep % self.target_update == 0:\n",
    "                        self.target_q_network_a =\\\n",
    "                            self.update_targets(self.online_q_network_a,self.target_q_network_a, self.tau)\n",
    "                        self.target_q_network_b =\\\n",
    "                            self.update_targets(self.online_q_network_b,self.target_q_network_b, self.tau)\n",
    "                    if done == True:\n",
    "                        reward_per_ep.append(reward_accumulated)\n",
    "                        break\n",
    "                    if self.current_timestep > self.max_steps:\n",
    "                        self.env.close()\n",
    "                        break\n",
    "                        return reward_per_ep\n",
    "                self.current_timestep+=1\n",
    "        self.env.close()\n",
    "        return reward_per_ep         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCGP(nn.Module):\n",
    "    def __init__(self,env, observation_space, action_space, hidden_dims=(32,32),\\\n",
    "                log_alpha_lr=0.001, min_log= -20, max_log=2):\n",
    "        super(FCGP, self).__init__()\n",
    "        self.input_size = observation_space\n",
    "        self.env = env\n",
    "        self.distribution_out = action_space\n",
    "        self.mean_out = action_space\n",
    "        self.log_alpha_lr = log_alpha_lr\n",
    "        self.min_log = min_log\n",
    "        self.max_log = max_log\n",
    "        self.input_layer = nn.Linear(self.input_size, hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i],\\\n",
    "                                    hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        \n",
    "        self.mean_layer = nn.Linear(hidden_dims[-1], action_space)#predict mean\n",
    "        self.distribution_layer = nn.Linear(hidden_dims[-1], action_space)#predict distribution\n",
    "        \n",
    "        self.target_entropy =  -1 * torch.tensor(np.prod(env.action_space.high.shape)).float()#recommended target entropy\n",
    "        self.log_alpha = torch.zeros(1, requires_grad=True)\n",
    "        self.target_entropy_optimizer = torch.optim.Adam([self.log_alpha],\\\n",
    "                                                         lr=log_alpha_lr)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available()\\\n",
    "                                  else \"cpu\")\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state).float().to(self.device)\n",
    "        x = F.relu(self.input_layer(state))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x))\n",
    "        mean_output = self.mean_layer(x)\n",
    "        log_distribution_output = self.distribution_layer(x)\n",
    "        log_distribution_output = log_distribution_output.clamp(self.min_log, self.max_log)#clamp log values so that they wont explode\n",
    "        return mean_output, log_distribution_output\n",
    "    \n",
    "    def rescale_actions(self, x):\n",
    "        tan_min = torch.tanh(torch.Tensor([float('-inf')])).to(self.device)\n",
    "        tan_max = torch.tanh(torch.Tensor([float('inf')])).to(self.device)\n",
    "        env_high = torch.tensor(self.env.action_space.high).float().to(self.device)\n",
    "        env_low = torch.tensor(self.env.action_space.low).float().to(self.device)\n",
    "        rescale_fn = lambda x: (x - tan_min) * (env_high - env_low)/\\\n",
    "                                     (tan_max - tan_min) + env_low\n",
    "        x = rescale_fn(x)\n",
    "        return x.to(self.device)\n",
    "        \n",
    "    \n",
    "    def full_pass(self, state, epsilon=1e-6):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state).float().to(self.device)\n",
    "        mean, log_distribution = self.forward(state)\n",
    "        pi_s = Normal(mean, log_distribution.exp())\n",
    "        sampled_distributions = pi_s.rsample()\n",
    "        tan_h_actions = torch.tanh(sampled_distributions)\n",
    "        \n",
    "        rescaled_actions = self.rescale_actions(tan_h_actions)\n",
    "        log_probs = pi_s.log_prob(sampled_distributions) - torch.log((\\\n",
    "                                                            1 - tan_h_actions.pow(2)).clamp(0,1) + epsilon)\n",
    "        log_probs = log_probs.sum(dim=1, keepdim=True)\n",
    "        return rescaled_actions, mean, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCQV(nn.Module):\n",
    "    def __init__(self, observation_space, action_space, hidden_dims=(32,32)):\n",
    "        super(FCQV, self).__init__()\n",
    "        self.input_size = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.input_layer = nn.Linear(self.input_size + self.action_space,\\\n",
    "                                    hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], 1)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available()\\\n",
    "                                  else \"cpu\")\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state).float().to(self.device)\n",
    "        if not isinstance(action, torch.Tensor):\n",
    "            action = torch.tensor(action).float().to(self.device)\n",
    "        x = torch.cat((state, action), dim=1)\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x))\n",
    "        x = self.output_layer(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grad_flow(named_parameters):\n",
    "    ave_grads = []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean().detach().cpu().numpy())\n",
    "    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(xmin=0, xmax=len(ave_grads))\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_network(online_network, target_network):\n",
    "    for online_weights, target_weights in zip(online_network.parameters(),\\\n",
    "                                              target_network.parameters()):\n",
    "        target_weights.data.copy_(online_weights.data)\n",
    "    return target_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SAC_PER(env):\n",
    "    \n",
    "    \n",
    "    observation_space = len(env.reset())\n",
    "    action_space_high, action_space_low = env.action_space.high, env.action_space.low\n",
    "    n_actions = len(action_space_high)\n",
    "    online_policy_network = FCGP(env,observation_space,n_actions,\\\n",
    "                                     hidden_dims=(128,64,64))\n",
    "    \n",
    "    online_q_network_a = FCQV(observation_space,\\\n",
    "                                     n_actions,hidden_dims=(128,64,64))\n",
    "    online_q_network_b = FCQV(observation_space,\\\n",
    "                                     n_actions,hidden_dims=(128,64,64))\n",
    "    \n",
    "    target_q_network_a = FCQV(observation_space,\\\n",
    "                                     n_actions,hidden_dims=(128,64,64))\n",
    "    target_q_network_b = FCQV(observation_space,\\\n",
    "                                     n_actions,hidden_dims=(128,64,64))\n",
    "    \n",
    "    #copy parameters from online to target\n",
    "    target_q_network_a = copy_network(online_q_network_a, target_q_network_a)\n",
    "    target_q_network_b = copy_network(online_q_network_b, target_q_network_b)\n",
    "    \n",
    "    target_q_network_a.eval()\n",
    "    target_q_network_b.eval()\n",
    "    \n",
    "    online_policy_optimizer = torch.optim.Adam(online_policy_network.parameters(),lr=0.0008)\n",
    "    online_qa_network_optimizer = torch.optim.Adam(online_q_network_a.parameters(),lr=0.0008)\n",
    "    online_qb_network_optimizer = torch.optim.Adam(online_q_network_b.parameters(),lr=0.0008)\n",
    "    \n",
    "    agent = Agent(env, ReplayMemory, online_policy_network, online_q_network_a, online_q_network_b,\\\n",
    "                online_policy_optimizer,online_qa_network_optimizer, online_qb_network_optimizer, target_q_network_a,\\\n",
    "                target_q_network_b, gamma=0.99, alpha_pr=0.4, beta_pr=0.2,\\\n",
    "                tau=0.01, n_ep=100, max_steps=100000, memory_size=200000, batch_size=100,\\\n",
    "                policy_update_step=2, target_update=20, min_sample_size=500, warm_up=100, rank_based_prioritization=False)\n",
    "    rewards = agent.train() \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make(\"MountainCarContinuous-v0\")#baseline_env\n",
    "env = gym.make(\"Pendulum-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [07:34<00:00,  4.54s/it]\n"
     ]
    }
   ],
   "source": [
    "rewards = SAC_PER(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_grad_flow(a.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = uniform_filter1d(rewards, size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for i,e in enumerate(arr):\n",
    "    y.append(i)\n",
    "    x.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x24389a60fc8>]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y, x)#20% bias correction, medium-high prioritization rates(40%), proportional prioritization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
