{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.distributions import Normal\n",
    "from scipy.ndimage.filters import uniform_filter1d\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "import random as rand\n",
    "from itertools import count\n",
    "import time\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self,capacity):   \n",
    "        self.capacity = capacity\n",
    "        self.memory = {}\n",
    "        self.push_count = 0\n",
    "        \n",
    "    def push(self, timestep, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory[timestep] = experience\n",
    "        else:\n",
    "            del_key = None\n",
    "            for k in self.memory.keys():\n",
    "                del_key = k\n",
    "                break\n",
    "            self.memory.pop(del_key, None)\n",
    "            self.memory[timestep] = experience\n",
    "        self.push_count+=1\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch_tuple = random.sample(self.memory.items(), batch_size)#key(timestep), experiences\n",
    "        experience_array = []\n",
    "        for timestep, experiences in batch_tuple:\n",
    "            experience_array.append(experiences)\n",
    "        return experience_array\n",
    "    \n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory)>=batch_size\n",
    "    \n",
    "    def update_td_error(self, sampled_experiences, timesteps):\n",
    "        for i in range(len(timesteps)):\n",
    "            self.memory[timesteps[i]] = sampled_experiences[i]\n",
    "        \n",
    "    def get_memory_values(self):\n",
    "        return self.memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, env, ReplayMemory, online_policy, online_q_a, online_q_b,\\\n",
    "                online_policy_optimizer,online_q_network_optimizer_a, online_q_network_optimizer_b, target_q_a,\\\n",
    "                target_q_b, gamma=0.99, alpha_pr=0.4, beta_pr=0.3,\\\n",
    "                tau=0.01, n_ep=120, max_steps=100000, memory_size=1000000, batch_size=50,\\\n",
    "                policy_update_step=2, target_update=50, min_sample_size=500, warm_up=2, rank_based_prioritization=True):\n",
    "        \n",
    "        \n",
    "        self.env = env\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.alpha_pr = alpha_pr\n",
    "        self.beta_pr = beta_pr\n",
    "        self.agent_max_memory=memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.n_ep = n_ep\n",
    "        self.max_steps = max_steps\n",
    "        self.min_sample_size=min_sample_size\n",
    "        self.target_update=target_update\n",
    "        self.policy_update_step=policy_update_step\n",
    "        self.online_policy_network = online_policy\n",
    "        self.online_policy_optimizer = online_policy_optimizer\n",
    "        self.online_q_network_a = online_q_a\n",
    "        self.online_q_network_optimizer_a = online_q_network_optimizer_a\n",
    "        self.online_q_network_b = online_q_b\n",
    "        self.online_q_network_optimizer_b = online_q_network_optimizer_b\n",
    "        self.target_q_network_a = target_q_a\n",
    "        self.target_q_network_b = target_q_b\n",
    "        self.current_timestep = 0\n",
    "        self.warm_up = warm_up\n",
    "        self.agent_memory = ReplayMemory(self.agent_max_memory)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available()\\\n",
    "                        else \"cpu\")\n",
    "        self.Xp = namedtuple('Experience',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done', 'abs_td_error','timestep'))\n",
    "        self.rank_based = rank_based_prioritization\n",
    "        \n",
    "    def extract_tensors(self, experiences):\n",
    "        batch = self.Xp(*zip(*experiences))\n",
    "        state = np.stack(batch.state) #stack\n",
    "        action = np.stack(batch.action)\n",
    "        next_state = np.stack(batch.next_state)\n",
    "        reward = np.stack(batch.reward)\n",
    "        done = np.stack(batch.done)\n",
    "        abs_td_error = np.stack(batch.abs_td_error)\n",
    "        timestep = np.stack(batch.timestep)\n",
    "        return state,action,next_state,reward,done,abs_td_error,timestep\n",
    "    \n",
    "    def rebuild_experiences(self, state, action, next_state, reward, done, abs_error, timestep):\n",
    "        exp_list = []\n",
    "        for idx_ in range(len(state)):\n",
    "            exp_list.append(\\\n",
    "                        self.Xp(state[idx_], action[idx_], next_state[idx_], reward[idx_],\\\n",
    "                           done[idx_], abs_error[idx_], timestep[idx_]))\n",
    "        return exp_list \n",
    "    \n",
    "    def prioritize_samples(self, experience_samples, alpha, beta,\\\n",
    "                           epsilon = 1e-6):\n",
    "        state,action,next_state,reward,done,abs_td_error,timesteps \\\n",
    "                            = self.extract_tensors(experience_samples)\n",
    "        \n",
    "        if self.rank_based == True:\n",
    "            abs_td_error, indices_ = (list(t) for t in zip(*sorted(\\\n",
    "                            zip(abs_td_error.tolist(), timesteps))))\n",
    "            abs_td_error.reverse()\n",
    "            indices_.reverse()#reverse to march sort func\n",
    "            abs_td_error = np.array(abs_td_error)\n",
    "            abs_td_error  = torch.tensor(abs_td_error)\n",
    "            ranks = np.arange(1, len(abs_td_error)+1)\n",
    "            priorities = 1.0/ranks\n",
    "        else:\n",
    "            priorities = abs_td_error + epsilon\n",
    "            indices_ = None\n",
    "        \n",
    "        priorities = priorities**alpha\n",
    "        priorities = np.expand_dims(priorities, axis=1)\n",
    "        probabilities = priorities/np.sum(priorities, axis=0)\n",
    "        assert np.isclose(probabilities.sum(), 1.0)\n",
    "        number_of_samples  = len(probabilities)\n",
    "        weight_importance_ = number_of_samples*probabilities\n",
    "        weight_importance_ = weight_importance_**-beta\n",
    "        weight_importance_max = np.max(weight_importance_)\n",
    "        weight_importance_scaled = weight_importance_/weight_importance_max\n",
    "        return weight_importance_scaled, indices_ \n",
    "    \n",
    "    def update_model(self, experience_samples,\\\n",
    "                weighted_importance, timestep_indices):\n",
    "        \n",
    "        states, actions, next_states, rewards, done, _ , timesteps =\\\n",
    "                    self.extract_tensors(experience_samples)\n",
    "        \n",
    "        if self.rank_based == True:\n",
    "            arrange_weighted_values = [timestep_indices.index(i) for i in timesteps]\n",
    "        \n",
    "        states = torch.tensor(np.squeeze(states)).float().to(self.device)\n",
    "        next_states = torch.tensor(np.squeeze(next_states)).float().to(self.device)\n",
    "        actions = torch.tensor(actions).float().to(self.device)\n",
    "        rewards = torch.tensor(rewards).unsqueeze(1).float().to(self.device)\n",
    "        done = torch.tensor(done).unsqueeze(1).float().to(self.device)\n",
    "        weighted_importance = torch.tensor(weighted_importance).float().to(self.device)\n",
    "    \n",
    "        #optimize alpha\n",
    "        current_actions,_, log_pi_s = self.online_policy_network.full_pass(states)\n",
    "        target_alpha = (self.online_policy_network.target_entropy +\\\n",
    "                   log_pi_s).detach()\n",
    "        alpha_loss = -(self.online_policy_network.log_alpha*\\\n",
    "                 target_alpha).mean()\n",
    "        self.online_policy_network.target_entropy_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.online_policy_network.target_entropy_optimizer.step()\n",
    "    \n",
    "        #set alpha\n",
    "        alpha = self.online_policy_network.log_alpha.exp()\n",
    "    \n",
    "        #optimize online using target nets\n",
    "        predicted_action_policy,_, log_pi_ns =\\\n",
    "                        self.online_policy_network.full_pass(next_states)\n",
    "\n",
    "        qsa_target_a = self.target_q_network_a(next_states,\\\n",
    "                                        predicted_action_policy)\n",
    "        qsa_target_b = self.target_q_network_b(next_states,\\\n",
    "                                        predicted_action_policy)\n",
    "        target_qsa = torch.min(qsa_target_a, qsa_target_b)\n",
    "        target_qsa = target_qsa - alpha*log_pi_ns\n",
    "        target_qsa = rewards + self.gamma*target_qsa*(1 - done)\n",
    "        target_qsa = target_qsa*weighted_importance\n",
    "        #print(weighted_importance)\n",
    "        target_qsa = target_qsa.detach()\n",
    "        qsa_online_a = self.online_q_network_a(states, actions)\n",
    "        qsa_online_b = self.online_q_network_b(states, actions)\n",
    "        qsa_online_a = qsa_online_a*weighted_importance.detach()\n",
    "        qsa_online_b = qsa_online_b*weighted_importance.detach()\n",
    "    \n",
    "        #update online networks\n",
    "        loss_func = torch.nn.SmoothL1Loss()\n",
    "        qa_loss = loss_func(qsa_online_a,\\\n",
    "                            target_qsa.detach())\n",
    "        qb_loss = loss_func(qsa_online_b,\\\n",
    "                            target_qsa.detach())\n",
    "\n",
    "        self.online_q_network_optimizer_a.zero_grad()\n",
    "        qa_loss.backward()\n",
    "        self.online_q_network_optimizer_a.step()\n",
    "    \n",
    "        self.online_q_network_optimizer_b.zero_grad()\n",
    "        qb_loss.backward()\n",
    "        self.online_q_network_optimizer_b.step()\n",
    "    \n",
    "        abs_a = (target_qsa.squeeze() - qsa_online_a.squeeze())\n",
    "        abs_b = (target_qsa.squeeze() - qsa_online_b.squeeze())\n",
    "        ovr_update = (abs_a + abs_b)/2\n",
    "        ovr_update = abs(ovr_update.detach().cpu().numpy())\n",
    "        if self.current_timestep % self.policy_update_step == 0:\n",
    "            current_actions,_ , log_pi = self.online_policy_network.full_pass(states)\n",
    "            qsa_online_a = self.online_q_network_a(states, current_actions)\n",
    "            qsa_online_b = self.online_q_network_b(states, current_actions)\n",
    "            qsa_min = torch.min(qsa_online_a, qsa_online_b)\n",
    "            policy_loss = (alpha*log_pi\\\n",
    "                      -qsa_min).mean()\n",
    "            self.online_policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.online_policy_optimizer.step()\n",
    "        states, actions, next_states, rewards, done, td_err , timesteps =\\\n",
    "                self.extract_tensors(experience_samples)    \n",
    "        experiences_rebuilded = self.rebuild_experiences\\\n",
    "                (states, actions, next_states, rewards, done, ovr_update, timesteps)\n",
    "        return experiences_rebuilded, timesteps\n",
    "    \n",
    "    def query_error(self, state, action, next_state, reward):\n",
    "\n",
    "        state = torch.tensor(state).unsqueeze(0).float().to(self.device)\n",
    "        next_state = torch.tensor(next_state).unsqueeze(0).float().to(self.device)\n",
    "        alpha = self.online_policy_network.log_alpha.exp()\n",
    "        ns_actions,_, log_pi_ns = self.online_policy_network.full_pass(next_state)\n",
    "        q_target_next_states_action_a = self.target_q_network_a(next_state,\\\n",
    "                                                    ns_actions.detach())\n",
    "        q_target_next_states_action_b = self.target_q_network_b(next_state,\\\n",
    "                                                    ns_actions.detach())\n",
    "        q_min = torch.min(q_target_next_states_action_a, q_target_next_states_action_b)\n",
    "        q_target = q_min - alpha * log_pi_ns\n",
    "        q_target = reward + (self.gamma*q_target.detach())\n",
    "        action = np.expand_dims(action, axis=0)\n",
    "        q_online_state_action_val_a = self.online_q_network_a(state, action)\n",
    "        q_online_state_action_val_b = self.online_q_network_b(state, action)\n",
    "        abs_a = (q_target - q_online_state_action_val_a)\n",
    "        abs_b = (q_target - q_online_state_action_val_b)\n",
    "        abs_stack = (abs_a + abs_b)/2\n",
    "        ovr_abs_update = abs_stack\n",
    "        ovr_abs_update = ovr_abs_update.squeeze()\n",
    "        return np.absolute(ovr_abs_update.detach().cpu().numpy())\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state).float().to(self.device)\n",
    "        state = state.unsqueeze(0)\n",
    "        warm_up_action = self.batch_size * self.warm_up\n",
    "        if self.agent_memory.can_provide_sample(warm_up_action) == False:\n",
    "            action = np.random.uniform(low=self.env.action_space.low,\\\n",
    "                                       high=self.env.action_space.high)\n",
    "            action = action.reshape(self.env.action_space.high.shape)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                mean,log_std = self.online_policy_network.forward(state)\n",
    "                action = torch.tanh(Normal(mean, log_std.exp()).sample())\n",
    "                action = self.online_policy_network.rescale_actions(action)\n",
    "                action = action.detach().cpu().numpy().reshape(self.env.action_space.high.shape)\n",
    "        return action\n",
    "    \n",
    "    def update_targets(self, online_q_network_, target_q_network_, tau):\n",
    "        for target_weights, online_weights in zip(target_q_network_.parameters(),\\\n",
    "                                                  online_q_network_.parameters()):\n",
    "            target_weight_update = (1.0 - tau)*target_weights.data\n",
    "            online_weight_update = tau*online_weights.data\n",
    "            sum_up = target_weight_update + online_weight_update\n",
    "            target_weights.data.copy_(sum_up)\n",
    "        return target_q_network_\n",
    "    \n",
    "    def train(self):\n",
    "        reward_per_ep = []\n",
    "        for e in tqdm(range(self.n_ep)):\n",
    "            state = self.env.reset()\n",
    "            reward_accumulated = 0\n",
    "            while True:\n",
    "                self.env.render()\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                td_error = self.query_error(state, action, next_state, reward)\n",
    "                reward_accumulated+=reward\n",
    "                is_truncated = 'TimeLimit.truncated' in info and\\\n",
    "                                info['TimeLimit.truncated']\n",
    "                is_failure = done and not is_truncated\n",
    "                self.agent_memory.push(self.current_timestep,\\\n",
    "                            self.Xp(state, action, next_state,\\\n",
    "                            reward, is_failure, td_error, self.current_timestep))\n",
    "                state = next_state\n",
    "                if self.agent_memory.can_provide_sample(self.min_sample_size):\n",
    "                    experience_samples = self.agent_memory.sample(self.batch_size)\n",
    "                    weighted_importance, indices =\\\n",
    "                            self.prioritize_samples(experience_samples, self.alpha_pr, self.beta_pr)\n",
    "                    experiences_rebuilded, timesteps = \\\n",
    "                            self.update_model(experience_samples, weighted_importance, indices)\n",
    "                    self.agent_memory.update_td_error(experiences_rebuilded, timesteps)\n",
    "                    if self.current_timestep % self.target_update == 0:\n",
    "                        self.target_q_network_a =\\\n",
    "                            self.update_targets(self.online_q_network_a,self.target_q_network_a, self.tau)\n",
    "                        self.target_q_network_b =\\\n",
    "                            self.update_targets(self.online_q_network_b,self.target_q_network_b, self.tau)\n",
    "                    if done == True:\n",
    "                        reward_per_ep.append(reward_accumulated)\n",
    "                        break\n",
    "                    if self.current_timestep > self.max_steps:\n",
    "                        self.env.close()\n",
    "                        break\n",
    "                        return reward_per_ep\n",
    "                self.current_timestep+=1\n",
    "        self.env.close()\n",
    "        return reward_per_ep         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCGP(nn.Module):\n",
    "    def __init__(self,env, observation_space, action_space, hidden_dims=(32,32),\\\n",
    "                log_alpha_lr=0.001, min_log= -20, max_log=2):\n",
    "        super(FCGP, self).__init__()\n",
    "        self.input_size = observation_space\n",
    "        self.env = env\n",
    "        self.distribution_out = action_space\n",
    "        self.mean_out = action_space\n",
    "        self.log_alpha_lr = log_alpha_lr\n",
    "        self.min_log = min_log\n",
    "        self.max_log = max_log\n",
    "        self.input_layer = nn.Linear(self.input_size, hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i],\\\n",
    "                                    hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        \n",
    "        self.mean_layer = nn.Linear(hidden_dims[-1], action_space)#predict mean\n",
    "        self.distribution_layer = nn.Linear(hidden_dims[-1], action_space)#predict distribution\n",
    "        \n",
    "        self.target_entropy =  -1 * torch.tensor(np.prod(env.action_space.high.shape)).float()#recommended target entropy\n",
    "        self.log_alpha = torch.zeros(1, requires_grad=True)\n",
    "        self.target_entropy_optimizer = torch.optim.Adam([self.log_alpha],\\\n",
    "                                                         lr=log_alpha_lr)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available()\\\n",
    "                                  else \"cpu\")\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state).float().to(self.device)\n",
    "        x = F.relu(self.input_layer(state))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x))\n",
    "        mean_output = self.mean_layer(x)\n",
    "        log_distribution_output = self.distribution_layer(x)\n",
    "        log_distribution_output = log_distribution_output.clamp(self.min_log, self.max_log)#clamp log values so that they wont explode\n",
    "        return mean_output, log_distribution_output\n",
    "    \n",
    "    def rescale_actions(self, x):\n",
    "        tan_min = torch.tanh(torch.Tensor([float('-inf')])).to(self.device)\n",
    "        tan_max = torch.tanh(torch.Tensor([float('inf')])).to(self.device)\n",
    "        env_high = torch.tensor(self.env.action_space.high).float().to(self.device)\n",
    "        env_low = torch.tensor(self.env.action_space.low).float().to(self.device)\n",
    "        rescale_fn = lambda x: (x - tan_min) * (env_high - env_low)/\\\n",
    "                                     (tan_max - tan_min) + env_low\n",
    "        x = rescale_fn(x)\n",
    "        return x.to(self.device)\n",
    "        \n",
    "    \n",
    "    def full_pass(self, state, epsilon=1e-6):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state).float().to(self.device)\n",
    "        mean, log_distribution = self.forward(state)\n",
    "        pi_s = Normal(mean, log_distribution.exp())\n",
    "        sampled_distributions = pi_s.rsample()\n",
    "        tan_h_actions = torch.tanh(sampled_distributions)\n",
    "        \n",
    "        rescaled_actions = self.rescale_actions(tan_h_actions)\n",
    "        log_probs = pi_s.log_prob(sampled_distributions) - torch.log((\\\n",
    "                                                            1 - tan_h_actions.pow(2)).clamp(0,1) + epsilon)\n",
    "        log_probs = log_probs.sum(dim=1, keepdim=True)\n",
    "        return rescaled_actions, mean, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCQV(nn.Module):\n",
    "    def __init__(self, observation_space, action_space, hidden_dims=(32,32)):\n",
    "        super(FCQV, self).__init__()\n",
    "        self.input_size = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.input_layer = nn.Linear(self.input_size + self.action_space,\\\n",
    "                                    hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], 1)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available()\\\n",
    "                                  else \"cpu\")\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state).float().to(self.device)\n",
    "        if not isinstance(action, torch.Tensor):\n",
    "            action = torch.tensor(action).float().to(self.device)\n",
    "        x = torch.cat((state, action), dim=1)\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x))\n",
    "        x = self.output_layer(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grad_flow(named_parameters):\n",
    "    ave_grads = []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean().detach().cpu().numpy())\n",
    "    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, linewidth=1, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(xmin=0, xmax=len(ave_grads))\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_network(online_network, target_network):\n",
    "    for online_weights, target_weights in zip(online_network.parameters(),\\\n",
    "                                              target_network.parameters()):\n",
    "        target_weights.data.copy_(online_weights.data)\n",
    "    return target_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SAC_PER(env):\n",
    "    \n",
    "    \n",
    "    observation_space = len(env.reset())\n",
    "    action_space_high, action_space_low = env.action_space.high, env.action_space.low\n",
    "    n_actions = len(action_space_high)\n",
    "    online_policy_network = FCGP(env,observation_space,n_actions,\\\n",
    "                                     hidden_dims=(128,64,64))\n",
    "    \n",
    "    online_q_network_a = FCQV(observation_space,\\\n",
    "                                     n_actions,hidden_dims=(128,64,64))\n",
    "    online_q_network_b = FCQV(observation_space,\\\n",
    "                                     n_actions,hidden_dims=(128,64,64))\n",
    "    \n",
    "    target_q_network_a = FCQV(observation_space,\\\n",
    "                                     n_actions,hidden_dims=(128,64,64))\n",
    "    target_q_network_b = FCQV(observation_space,\\\n",
    "                                     n_actions,hidden_dims=(128,64,64))\n",
    "    \n",
    "    #copy parameters from online to target\n",
    "    target_q_network_a = copy_network(online_q_network_a, target_q_network_a)\n",
    "    target_q_network_b = copy_network(online_q_network_b, target_q_network_b)\n",
    "    \n",
    "    target_q_network_a.eval()\n",
    "    target_q_network_b.eval()\n",
    "    \n",
    "    online_policy_optimizer = torch.optim.Adam(online_policy_network.parameters(),lr=0.0008)\n",
    "    online_qa_network_optimizer = torch.optim.Adam(online_q_network_a.parameters(),lr=0.0008)\n",
    "    online_qb_network_optimizer = torch.optim.Adam(online_q_network_b.parameters(),lr=0.0008)\n",
    "    \n",
    "    agent = Agent(env, ReplayMemory, online_policy_network, online_q_network_a, online_q_network_b,\\\n",
    "                online_policy_optimizer,online_qa_network_optimizer, online_qb_network_optimizer, target_q_network_a,\\\n",
    "                target_q_network_b, gamma=0.99, alpha_pr=0.75, beta_pr=0.85,\\\n",
    "                tau=0.01, n_ep=100, max_steps=100000, memory_size=500000, batch_size=100,\\\n",
    "                policy_update_step=2, target_update=100, min_sample_size=500, warm_up=100, rank_based_prioritization=False)\n",
    "    rewards = agent.train() \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCarContinuous-v0\")#baseline_env\n",
    "#env = gym.make(\"Pendulum-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 100/100 [1:56:40<00:00, 70.01s/it]\n"
     ]
    }
   ],
   "source": [
    "rewards = SAC_PER(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_grad_flow(a.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = uniform_filter1d(rewards, size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for i,e in enumerate(arr):\n",
    "    y.append(i)\n",
    "    x.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2730af5d988>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y, x)#80% bias correction, medium-high prioritization rates(75%), proportional prioritization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
