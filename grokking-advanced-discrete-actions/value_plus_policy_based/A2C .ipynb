{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.nn.init import kaiming_uniform_#\n",
    "from matplotlib import pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "import random as rand\n",
    "from itertools import count\n",
    "from torch.distributions import Categorical\n",
    "import torch.multiprocessing as mp \n",
    "main_seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_system'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp.get_all_sharing_strategies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipes =[mp.Pipe() for rank in range(5) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmultiprocessing.Pipe([duplex])\\n    Returns a pair (conn1, conn2) of Connection objects representing the ends of a pipe.\\n    If duplex is True (the default) then the pipe is bidirectional. \\n    If duplex is False then the pipe is unidirectional: \\n    conn1 can only be used for receiving messages and conn2 can only be used for sending messages.\\nsee: https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Pipe\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "multiprocessing.Pipe([duplex])\n",
    "    Returns a pair (conn1, conn2) of Connection objects representing the ends of a pipe.\n",
    "    If duplex is True (the default) then the pipe is bidirectional. \n",
    "    If duplex is False then the pipe is unidirectional: \n",
    "    conn1 can only be used for receiving messages and conn2 can only be used for sending messages.\n",
    "see: https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Pipe\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............\n",
      "(<multiprocessing.connection.PipeConnection object at 0x0000025BF02EE9C8>, <multiprocessing.connection.PipeConnection object at 0x0000025BFD688AC8>)\n",
      "..............\n",
      "(<multiprocessing.connection.PipeConnection object at 0x0000025BFD3AF248>, <multiprocessing.connection.PipeConnection object at 0x0000025BFD692108>)\n",
      "..............\n",
      "(<multiprocessing.connection.PipeConnection object at 0x0000025BFD692248>, <multiprocessing.connection.PipeConnection object at 0x0000025BFD692408>)\n",
      "..............\n",
      "(<multiprocessing.connection.PipeConnection object at 0x0000025BFD692348>, <multiprocessing.connection.PipeConnection object at 0x0000025BFD692488>)\n",
      "..............\n",
      "(<multiprocessing.connection.PipeConnection object at 0x0000025BFD6923C8>, <multiprocessing.connection.PipeConnection object at 0x0000025BFD692508>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(pipes)):\n",
    "    print('..............')\n",
    "    print(pipes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<CartPoleEnv<CartPole-v0>>>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "env.seed(0)\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearApproximator_A2C(nn.Module):\n",
    "    def __init__(self,state_shape,policy_outputs, state_value_output=1, hidden_dims=(32,32)):\n",
    "        super(linearApproximator_A2C, self).__init__()\n",
    "        self.input_size = state_shape\n",
    "        self.policy_outputs = policy_outputs\n",
    "        self.state_value_output = state_value_output\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available()\\\n",
    "                                   else \"cpu\")\n",
    "        \n",
    "        self.fc1  = nn.Linear(self.input_size,hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(\\\n",
    "                                hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        \n",
    "        self.policy_output_layer  = nn.Linear(hidden_dims[-1],self.policy_outputs)\n",
    "        self.state_value_output_layer = nn.Linear(hidden_dims[-1],self.state_value_output)\n",
    "        self.to(self.device)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, state_shape):\n",
    "        if not isinstance(state_shape, torch.Tensor):\n",
    "            state_shape = torch.tensor(state_shape, dtype=torch.float32)\n",
    "        state_shape = state_shape.to(self.device)\n",
    "            \n",
    "        x = self.fc1(state_shape)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x))\n",
    "        \n",
    "        logits = self.policy_output_layer(x)#logits, preferences of actions\n",
    "        state_value = self.state_value_output_layer(x)#predicted state value\n",
    "        return logits, state_value\n",
    "        \n",
    "    def full_pass(self, state):\n",
    "        if not isinstance(state_shape, torch.Tensor):\n",
    "            state_shape = torch.tensor(state_shape, dtype=torch.float32)\n",
    "        state = state.float().to(device)\n",
    "        logits, state_value = self.forward(state)\n",
    "        distribution = Categorical(logits=logits)\n",
    "        action = distribution.sample()#sample action\n",
    "        log_prob_action = distribution.log_prob(action).unsqueeze(-1)#gets prob of sampled action\n",
    "        entropy = distribution.entropy().unsqueeze(-1)\n",
    "        return action.item(), log_prob_action, entropy, logits, state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(action_model, state):\n",
    "    action, log_prob_action, entropy, logits\\\n",
    "                = action_model.full_pass(state)\n",
    "    return action, log_prob_action, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmultiprocessing.Pipe([duplex])\\n    Returns a pair (conn1, conn2) of Connection objects representing the ends of a pipe.\\n    If duplex is True (the default) then the pipe is bidirectional. \\n    If duplex is False then the pipe is unidirectional: \\n    conn1 can only be used for receiving messages and conn2 can only be used for sending messages.\\nsee: https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Pipe\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "multiprocessing.Pipe([duplex])\n",
    "    Returns a pair (conn1, conn2) of Connection objects representing the ends of a pipe.\n",
    "    If duplex is True (the default) then the pipe is bidirectional. \n",
    "    If duplex is False then the pipe is unidirectional: \n",
    "    conn1 can only be used for receiving messages and conn2 can only be used for sending messages.\n",
    "see: https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Pipe\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "multiprocessing.Pipe([duplex])\n",
    "    Returns a pair (conn1, conn2) of Connection objects representing the ends of a pipe.\n",
    "    If duplex is True (the default) then the pipe is bidirectional. \n",
    "    If duplex is False then the pipe is unidirectional: \n",
    "    conn1 can only be used for receiving messages and conn2 can only be used for sending messages.\n",
    "see: https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Pipe\n",
    "'''\n",
    "class Multiprocess_env():\n",
    "    def __init__(self, env, n_seeds):\n",
    "        #n_seeds is basically == no of workers\n",
    "        self.env = env\n",
    "        self.n_seeds = n_seeds\n",
    "        self.env_pipes = [mp.Pipe() for seeds in range(self.n_seeds)]\n",
    "        self.envs = [mp.Process\\\n",
    "                    (target=self.work,\\\n",
    "                    args=(seed, self.env_pipes[seed][1])) for seed in range(self.n_seeds)]\n",
    "        [w.start() for w in self.envs]#these envs can only send out data\n",
    "        \n",
    "    def broadcast_msg(self, msg):\n",
    "        [receiver.send(msg) for receiver, sender in self.env_pipes]\n",
    "        \n",
    "    def send_msg(self, msg, seed):\n",
    "        receiver, sender = self.env_pipes[seed]\n",
    "        receiver.send(msg)\n",
    "        \n",
    "    def close(self, **kwargs):\n",
    "        self.broadcast_msg(('close', kwargs))\n",
    "        [e.join() for e in self.envs]\n",
    "    \n",
    "    def past_limit(self, **kwargs):\n",
    "        self.broadcast_msg(('past_limit', kwargs))\n",
    "        return np.vstack([receiver.recv()\\\n",
    "                             for receiver, sender in self.env_pipes])\n",
    "    \n",
    "    def step(self, actions):\n",
    "        assert len(actions) == self.n_seeds\n",
    "        [self.send_msg\\\n",
    "            (('step', {'action':actions[seed]}), seed)\\\n",
    "                        for seed in range(self.n_seeds)]\n",
    "        results = []\n",
    "        for seed in range(self.n_seeds):\n",
    "            receiver, sender = self.env_pipes[seed]\n",
    "            obj_, reward, done, info = receiver.recv()\n",
    "            results.append((obj_,\n",
    "                           np.array(reward, dtype=np.float),\n",
    "                           np.array(done, dtype=np.float),\n",
    "                           info))\n",
    "        return[np.vstack(block) for block in np.array(results).T]\n",
    "        \n",
    "    def reset(self, seed=None, **kwargs):\n",
    "        if seed is not None:\n",
    "            receiver, sender = self.env_pipes[seed]\n",
    "            self.send_msg(('reset', {}), seed)\n",
    "            o = receiver.recv()\n",
    "            return o\n",
    "        \n",
    "        self.broadcast_msg(('reset',kwargs))\n",
    "        return np.vstack([receiver.recv()\\\n",
    "                             for receiver, sender in self.env_pipes])\n",
    "            \n",
    "    def work(self, seed, sender_pipe):\n",
    "        env = self.env\n",
    "        env.seed(seed)\n",
    "        while True:\n",
    "            cmd, kwargs = sender_pipe.recv()\n",
    "            if cmd == 'reset':\n",
    "                sender_pipe.send(env.reset(**kwargs))\n",
    "            elif cmd == 'step':\n",
    "                sender_pipe.send(env.step(**kwargs))\n",
    "            elif cmd == 'past_limit':\n",
    "                sender_pipe.send(env._elapsed_steps >= env._max_episode_steps)\n",
    "            else:\n",
    "                env.close(**kwargs)\n",
    "                del env\n",
    "                sender_pipe.close()\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C():\n",
    "    def __init__(self, a2c_network,\n",
    "                a2c_optimizer,\n",
    "                a2c_lr,\n",
    "                policy_loss_weight,\n",
    "                value_loss_weight,\n",
    "                entropy_loss_weight,\n",
    "                max_steps,\n",
    "                n_seeds,\n",
    "                tau,\n",
    "                gamma):\n",
    "        assert n_seeds > 1\n",
    "        self.a2c_network = a2c_network\n",
    "        self.a2c_optimizer = a2c_optimizer\n",
    "        self.a2c_lr = a2c_lr\n",
    "        \n",
    "        self.policy_loss_weight = policy_loss_weight\n",
    "        self.entropy_loss_weight = entropy_loss_weight\n",
    "        self.value_loss_weight = value_loss_weight\n",
    "        \n",
    "        self.max_steps = max_steps\n",
    "        self.n_seeds = n_seeds\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def optimize_model(self, log_pa, entropies, states, rewards):\n",
    "        log_pa = torch.stack(log_pa).squeeze()\n",
    "        entropies = torch.stack(entropies).squeeze()\n",
    "        states = torch.stack(states).squeeze()\n",
    "        \n",
    "        T = len(rewards)#trajectory length\n",
    "        discounts = np.logspace(0, T, num = T,\n",
    "                               base = self.gamma, endpoint=False)\n",
    "        rewards = np.array(rewards).squeeze()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " abort_after = 60\n",
    "    start = time.time()\n",
    "    terminate_env = False\n",
    "    ovr_rewards = []\n",
    "    \n",
    "    while not terminate_env:\n",
    "        delta = time.time() - start\n",
    "        state = env.reset()#set of n_envs\n",
    "        acc_rewards = 0\n",
    "        \n",
    "        n_steps = 0\n",
    "        reward_state = []\n",
    "        state_store = []\n",
    "        log_pa = []\n",
    "        entropy_store = []\n",
    "        \n",
    "        for step in count(start = 1):\n",
    "            action, log_prob, entropy = \\\n",
    "                    select_action(A2C_policy_network, state)\n",
    "            next_state, reward, done, info = \\\n",
    "                    env.step(action)\n",
    "            acc_rewards += reward\n",
    "            is_truncated = 'TimeLimit.truncated' in info and\\\n",
    "                                info['TimeLimit.truncated']\n",
    "            is_failure = done and not is_truncated\n",
    "            reward_store.append(reward)\n",
    "            state_store.append(state)\n",
    "            log_pa.append(log_prob)\n",
    "            entropy_store.append(entropy)\n",
    "            state = next_state\n",
    "            \n",
    "            if done==True or step - n_steps == max_steps:\n",
    "                if is_failure:\n",
    "                    reward_store[-1] = 0.0\n",
    "                if len(reward_store) < 2:\n",
    "                    continue\n",
    "                optimize_model(reward_state, log_pa,\\\n",
    "                    state_store, entropy_store, A2C_value_network,\\\n",
    "                    A2C_policy_network, A2C_policy_optimizer,\\\n",
    "                    A2C_value_optimizer, gamma, entropy_beta,\\\n",
    "                    gae_tau)\n",
    "                reward_store = []\n",
    "                state_store = []\n",
    "                log_pa = []\n",
    "                entropy_store = []\n",
    "                n_steps = step\n",
    "            if done == True:\n",
    "                ovr_rewards.append(acc_rewards)\n",
    "                break\n",
    "            if delta >= abort_after:\n",
    "                terminate_worker = True\n",
    "                break\n",
    "    return ovr_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A2C_(env,\n",
    "        n_envs,\n",
    "        gamma=0.99):\n",
    "    \n",
    "    obs_space = len(env.reset())\n",
    "    action_space = len(env.action_space.high)\n",
    "    A2C_network = linearApproximator_A2C(obs_space, action_space)\n",
    "    A2C_network_optimizer = torch.optim.Adam(A2C_network.parameters(),lr=0.0008, weight_decay = 0.01)\n",
    "    \n",
    "    for seed in range(n_envs):\n",
    "        #get trajectory from environment seed using model\n",
    "        \n",
    "        #optimize model\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
