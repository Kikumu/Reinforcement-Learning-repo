{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.nn.init import kaiming_uniform_#\n",
    "from matplotlib import pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "import random as rand\n",
    "from itertools import count\n",
    "from torch.distributions import Categorical\n",
    "main_seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_system'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp.get_all_sharing_strategies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<multiprocessing.connection.PipeConnection object at 0x0000027D69527F48>, <multiprocessing.connection.PipeConnection object at 0x0000027D6952D648>)\n",
      "(<multiprocessing.connection.PipeConnection object at 0x0000027D6952D648>, <multiprocessing.connection.PipeConnection object at 0x0000027D6952D4C8>)\n",
      "(<multiprocessing.connection.PipeConnection object at 0x0000027D6952D648>, <multiprocessing.connection.PipeConnection object at 0x0000027D6952D6C8>)\n",
      "(<multiprocessing.connection.PipeConnection object at 0x0000027D6952D648>, <multiprocessing.connection.PipeConnection object at 0x0000027D6952D088>)\n",
      "(<multiprocessing.connection.PipeConnection object at 0x0000027D6952D648>, <multiprocessing.connection.PipeConnection object at 0x0000027D6952D4C8>)\n"
     ]
    }
   ],
   "source": [
    "for rank in range(5):\n",
    "    print(mp.Pipe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<CartPoleEnv<CartPole-v0>>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "env.seed(0)\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearApproximator_A2C(nn.Module):\n",
    "    def __init__(self,state_shape,policy_outputs, state_value_output=1, hidden_dims=(32,32)):\n",
    "        super(linearApproximator_A2C, self).__init__()\n",
    "        self.input_size = state_shape\n",
    "        self.policy_outputs = policy_outputs\n",
    "        self.state_value_output = state_value_output\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available()\\\n",
    "                                   else \"cpu\")\n",
    "        \n",
    "        self.fc1  = nn.Linear(self.input_size,hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(\\\n",
    "                                hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        \n",
    "        self.policy_output_layer  = nn.Linear(hidden_dims[-1],self.policy_outputs)\n",
    "        self.state_value_output_layer = nn.Linear(hidden_dims[-1],self.state_value_output)\n",
    "        self.to(self.device)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, state_shape):\n",
    "        if not isinstance(state_shape, torch.Tensor):\n",
    "            state_shape = torch.tensor(state_shape, dtype=torch.float32)\n",
    "        state_shape = state_shape.to(self.device)\n",
    "            \n",
    "        x = self.fc1(state_shape)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x))\n",
    "        \n",
    "        logits = self.policy_output_layer(x)#logits, preferences of actions\n",
    "        state_value = self.state_value_output_layer(x)#predicted state value\n",
    "        return logits, state_value\n",
    "        \n",
    "    def full_pass(self, state):\n",
    "        if not isinstance(state_shape, torch.Tensor):\n",
    "            state_shape = torch.tensor(state_shape, dtype=torch.float32)\n",
    "        state = state.float().to(device)\n",
    "        logits, state_value = self.forward(state)\n",
    "        distribution = Categorical(logits=logits)\n",
    "        action = distribution.sample()#sample action\n",
    "        log_prob_action = distribution.log_prob(action).unsqueeze(-1)#gets prob of sampled action\n",
    "        entropy = distribution.entropy().unsqueeze(-1)\n",
    "        return action.item(), log_prob_action, entropy, logits, state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearApproximator_FCV(nn.Module):\n",
    "    def __init__(self,state_shape,outputs,hidden_dims=(32,32)):\n",
    "        super(linearApproximator_FCV, self).__init__()\n",
    "        self.input_size = state_shape\n",
    "        self.out = outputs\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available()\\\n",
    "                                   else \"cpu\")\n",
    "        \n",
    "        self.fc1  = nn.Linear(self.input_size,hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(\\\n",
    "                                hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        \n",
    "        self.output_layer  = nn.Linear(hidden_dims[-1],self.out)\n",
    "        self.to(self.device)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, state_shape):\n",
    "        if not isinstance(state_shape, torch.Tensor):\n",
    "            state_shape = torch.tensor(state_shape, dtype=torch.float32)\n",
    "        state_shape = state_shape.to(self.device)\n",
    "            \n",
    "        x = self.fc1(state_shape)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x))\n",
    "        \n",
    "        state_value = self.output_layer(x)#logits, preferences of actions\n",
    "        return state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(action_model, state):\n",
    "    action, log_prob_action, entropy, logits\\\n",
    "                = action_model.full_pass(state)\n",
    "    return action, log_prob_action, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Multiprocess_env(env, local_env_seed, A2C_network):\n",
    "    env.seed(local_env_seed)\n",
    "    abort_after = 60\n",
    "    start = time.time()\n",
    "    terminate_env = False\n",
    "    ovr_rewards = []\n",
    "    \n",
    "    while not terminate_env:\n",
    "        delta = time.time() - start\n",
    "        state = env.reset()#set of n_envs\n",
    "        acc_rewards = 0\n",
    "        \n",
    "        n_steps = 0\n",
    "        reward_state = []\n",
    "        state_store = []\n",
    "        log_pa = []\n",
    "        entropy_store = []\n",
    "        \n",
    "        for step in count(start = 1):\n",
    "            action, log_prob, entropy = \\\n",
    "                    select_action(A2C_policy_network, state)\n",
    "            next_state, reward, done, info = \\\n",
    "                    env.step(action)\n",
    "            acc_rewards += reward\n",
    "            is_truncated = 'TimeLimit.truncated' in info and\\\n",
    "                                info['TimeLimit.truncated']\n",
    "            is_failure = done and not is_truncated\n",
    "            reward_store.append(reward)\n",
    "            state_store.append(state)\n",
    "            log_pa.append(log_prob)\n",
    "            entropy_store.append(entropy)\n",
    "            state = next_state\n",
    "            \n",
    "            if done==True or step - n_steps == max_steps:\n",
    "                if is_failure:\n",
    "                    reward_store[-1] = 0.0\n",
    "                if len(reward_store) < 2:\n",
    "                    continue\n",
    "                optimize_model(reward_state, log_pa,\\\n",
    "                    state_store, entropy_store, A2C_value_network,\\\n",
    "                    A2C_policy_network, A2C_policy_optimizer,\\\n",
    "                    A2C_value_optimizer, gamma, entropy_beta,\\\n",
    "                    gae_tau)\n",
    "                reward_store = []\n",
    "                state_store = []\n",
    "                log_pa = []\n",
    "                entropy_store = []\n",
    "                n_steps = step\n",
    "            if done == True:\n",
    "                ovr_rewards.append(acc_rewards)\n",
    "                break\n",
    "            if delta >= abort_after:\n",
    "                terminate_worker = True\n",
    "                break\n",
    "    return ovr_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A2C_(env,\n",
    "        n_envs,\n",
    "        gamma=0.99):\n",
    "    \n",
    "    obs_space = len(env.reset())\n",
    "    action_space = len(env.action_space.high)\n",
    "    A2C_network = linearApproximator_A2C(obs_space, action_space)\n",
    "    A2C_network_optimizer = torch.optim.Adam(A2C_network.parameters(),lr=0.0008, weight_decay = 0.01)\n",
    "    \n",
    "    for seed in range(n_envs):\n",
    "        #get trajectory from environment seed using model\n",
    "        \n",
    "        #optimize model\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
