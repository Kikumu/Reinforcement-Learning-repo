{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.nn.init import kaiming_uniform_#\n",
    "from matplotlib import pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "import random as rand\n",
    "from itertools import count\n",
    "from torch.distributions import Categorical\n",
    "main_seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<CartPoleEnv<CartPole-v0>>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "env.seed(0)\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearApproximator_A2C(nn.Module):\n",
    "    def __init__(self,state_shape,policy_outputs, state_value_output=1, hidden_dims=(32,32)):\n",
    "        super(linearApproximator_A2C, self).__init__()\n",
    "        self.input_size = state_shape\n",
    "        self.policy_outputs = policy_outputs\n",
    "        self.state_value_output = state_value_output\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available()\\\n",
    "                                   else \"cpu\")\n",
    "        \n",
    "        self.fc1  = nn.Linear(self.input_size,hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(\\\n",
    "                                hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        \n",
    "        self.policy_output_layer  = nn.Linear(hidden_dims[-1],self.policy_outputs)\n",
    "        self.state_value_output_layer = nn.Linear(hidden_dims[-1],self.state_value_output)\n",
    "        self.to(self.device)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, state_shape):\n",
    "        if not isinstance(state_shape, torch.Tensor):\n",
    "            state_shape = torch.tensor(state_shape, dtype=torch.float32)\n",
    "        state_shape = state_shape.to(self.device)\n",
    "            \n",
    "        x = self.fc1(state_shape)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x))\n",
    "        \n",
    "        logits = self.policy_output_layer(x)#logits, preferences of actions\n",
    "        state_value = self.state_value_output_layer(x)#predicted state value\n",
    "        return logits, state_value\n",
    "        \n",
    "    def full_pass(self, state):\n",
    "        if not isinstance(state_shape, torch.Tensor):\n",
    "            state_shape = torch.tensor(state_shape, dtype=torch.float32)\n",
    "        state = state.float().to(device)\n",
    "        logits, state_value = self.forward(state)\n",
    "        distribution = Categorical(logits=logits)\n",
    "        action = distribution.sample()#sample action\n",
    "        log_prob_action = distribution.log_prob(action).unsqueeze(-1)#gets prob of sampled action\n",
    "        entropy = distribution.entropy().unsqueeze(-1)\n",
    "        return action.item(), log_prob_action, entropy, logits, state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(policy_network, state, env):\n",
    "    logits, _ = policy_network.forward(state)\n",
    "    distribution = Categorical(logits=logits)\n",
    "    action = distribution.sample()\n",
    "    action = action.data.numpy().reshape(env.action_space.high.shape)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Env_trajectory(env, local_env_seed, A2C_network):\n",
    "    env.seed(local_env_seed)\n",
    "    torch.manual_seed(local_env_seed)\n",
    "    np.random.seed(local_env_seed)\n",
    "    random.seed(local_env_seed)\n",
    "    \n",
    "    abort_after = 60\n",
    "    start = time.time()\n",
    "    terminate_env = False\n",
    "    ovr_rewards = []\n",
    "    \n",
    "    while not terminate_env:\n",
    "        delta = time.time() - start\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A2C_(env,\n",
    "        n_envs,\n",
    "        gamma=0.99):\n",
    "    \n",
    "    obs_space = len(env.reset())\n",
    "    action_space = len(env.action_space.high)\n",
    "    A2C_network = linearApproximator_A2C(obs_space, action_space)\n",
    "    A2C_network_optimizer = torch.optim.Adam(A2C_network.parameters(),lr=0.0008, weight_decay = 0.01)\n",
    "    \n",
    "    for seed in range(n_envs):\n",
    "        #get trajectory from environment seed using model\n",
    "        \n",
    "        #optimize model\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
