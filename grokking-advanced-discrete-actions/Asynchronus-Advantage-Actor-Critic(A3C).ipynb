{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.nn.init import kaiming_uniform_#\n",
    "from matplotlib import pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "import random as rand\n",
    "from itertools import count\n",
    "from torch.distributions import Categorical\n",
    "main_seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<CartPoleEnv<CartPole-v0>>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "env.seed(0)\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearApproximator_FCDAP(nn.Module):\n",
    "    def __init__(self,state_shape,outputs,hidden_dims=(32,32)):\n",
    "        super(linearApproximator_FCDAP, self).__init__()\n",
    "        self.input_size = state_shape\n",
    "        self.out = outputs\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available()\\\n",
    "                                   else \"cpu\")\n",
    "        \n",
    "        self.fc1  = nn.Linear(self.input_size,hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(\\\n",
    "                                hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        \n",
    "        self.output_layer  = nn.Linear(hidden_dims[-1],self.out)\n",
    "        self.to(self.device)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, state_shape):\n",
    "        if not isinstance(state_shape, torch.Tensor):\n",
    "            state_shape = torch.tensor(state_shape, dtype=torch.float32)\n",
    "        state_shape = state_shape.to(self.device)\n",
    "            \n",
    "        x = self.fc1(state_shape)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x))\n",
    "        \n",
    "        logits = self.output_layer(x)#logits, preferences of actions\n",
    "        return logits\n",
    "        \n",
    "    def full_pass(self, state):\n",
    "        logits = self.forward(state).cpu()\n",
    "        distribution = Categorical(logits=logits)\n",
    "        action = distribution.sample()#sample action\n",
    "        log_prob_action = distribution.log_prob(action).unsqueeze(-1)#gets prob of sampled action\n",
    "        entropy = distribution.entropy().unsqueeze(-1)\n",
    "        return action.item(), log_prob_action, entropy, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearApproximator_FCV(nn.Module):\n",
    "    def __init__(self,state_shape,outputs,hidden_dims=(32,32)):\n",
    "        super(linearApproximator_FCV, self).__init__()\n",
    "        self.input_size = state_shape\n",
    "        self.out = outputs\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available()\\\n",
    "                                   else \"cpu\")\n",
    "        \n",
    "        self.fc1  = nn.Linear(self.input_size,hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(\\\n",
    "                                hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        \n",
    "        self.output_layer  = nn.Linear(hidden_dims[-1],self.out)\n",
    "        self.to(self.device)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, state_shape):\n",
    "        if not isinstance(state_shape, torch.Tensor):\n",
    "            state_shape = torch.tensor(state_shape, dtype=torch.float32)\n",
    "        state_shape = state_shape.to(self.device)\n",
    "            \n",
    "        x = self.fc1(state_shape)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x))\n",
    "        \n",
    "        state_value = self.output_layer(x)#logits, preferences of actions\n",
    "        return state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(env,main_seed,worker_rank,\\\n",
    "          global_policy_network,global_value_network\n",
    "          ):\n",
    "    \n",
    "    local_worker_seed = main_seed + worker_rank\n",
    "    #set seeds\n",
    "    env.seed(local_worker_seed)\n",
    "    torch.manual_seed(local_worker_seed)\n",
    "    np.random.seed(local_worker_seed)\n",
    "    random.seed(local_worker_seed)\n",
    "    \n",
    "    observation_space = len(env.reset())\n",
    "    action_space = env.action_space.n\n",
    "    \n",
    "    worker_policy_network = linearApproximator_FCDAP(observation_space,\\\n",
    "                                     action_space,hidden_dims=(128,64))\n",
    "    worker_value_network = linearApproximator_FCV(observation_space,\\\n",
    "                                     1,hidden_dims=(256,128))\n",
    "    \n",
    "    #load state dict and policy dict from global network to worker\n",
    "    worker_policy_network.load_state_dict(global_policy_network.state_dict())\n",
    "    worker_value_network.load_state_dict(global_value_network.state_dict())\n",
    "    \n",
    "    return worker_policy_network, worker_value_network, env\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(action_model, state):\n",
    "    action, log_prob_action, entropy, logits = action_model.full_pass(state)\n",
    "    return action, log_prob_action, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(reward_store, log_pa, states,entropies,\\\n",
    "                   local_value_network,local_policy_network,\\\n",
    "                   global_policy_optimizer, global_value_optimizer,\\\n",
    "                   global_policy_network, global_value_network,\\\n",
    "                   gamma,entropy_beta):\n",
    "    \n",
    "    Trajectory_length = len(reward_store)\n",
    "   \n",
    "    \n",
    "    discounts = np.logspace(0,Trajectory_length,\\\n",
    "                           num=Trajectory_length,base=gamma,\\\n",
    "                           endpoint=False)\n",
    "    \n",
    "    returns = np.array([np.sum(discounts[:Trajectory_length-t] * reward_store[t:]) for t in range(Trajectory_length)])\n",
    "    returns = torch.tensor(returns).unsqueeze(1)\n",
    "\n",
    "    \n",
    "    states = torch.tensor(states).float()\n",
    "    state_values = local_value_network(states)   \n",
    "    model_state_error = returns - state_values\n",
    "    model_state_loss = model_state_error.pow(2).mul(0.5).mean()\n",
    "    global_value_optimizer.zero_grad()\n",
    "    model_state_loss.backward()\n",
    "    \n",
    "    for local_params, global_params in zip(local_value_network.parameters(),\\\n",
    "                                          global_value_network.parameters()):\n",
    "        if global_params.grad is None:\n",
    "            global_params._grad = local_params.grad\n",
    "\n",
    "    global_value_optimizer.step()\n",
    "    local_value_network.load_state_dict(global_value_network.state_dict())\n",
    "    \n",
    "    log_pa = torch.stack(log_pa)\n",
    "    \n",
    "    discounts_tensor = torch.tensor(discounts).unsqueeze(1)\n",
    "    advantage = returns - state_values.detach()\n",
    "    local_policy_loss = -(advantage*log_pa*discounts_tensor).mean()\n",
    "    entropy = torch.tensor(entropies).unsqueeze(1)\n",
    "    entropy = -entropy.mean()\n",
    "    local_policy_loss +=entropy_beta*entropy\n",
    "    \n",
    "    global_policy_optimizer.zero_grad()\n",
    "    local_policy_loss.backward()\n",
    "    \n",
    "    #before backward pass of global param, we copy gradients\n",
    "    for local_param, global_param in zip(local_policy_network.parameters(),\\\n",
    "                                  global_policy_network.parameters()):\n",
    "        if global_param.grad is None:\n",
    "            global_param._grad = local_param.grad\n",
    "    \n",
    "    global_policy_optimizer.step()\n",
    "    \n",
    "    local_policy_network.load_state_dict(global_policy_network.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_worker(env,\\\n",
    "                 local_policy_network, local_value_network,\\\n",
    "                 global_policy_network, global_value_network,\\\n",
    "                 global_policy_optimizer, global_value_optimizer,\\\n",
    "                 max_steps, entropy_beta, gamma):\n",
    "    abort_after = 60*2 #abort after 30 sec\n",
    "    start = time.time()\n",
    "    terminate_worker = False\n",
    "    ovr_rewards = []\n",
    "    while not terminate_worker:\n",
    "        delta = time.time() - start\n",
    "        state = env.reset()\n",
    "        acc_rewards = 0\n",
    "        \n",
    "        n_steps = 0\n",
    "        reward_store = []\n",
    "        state_store = []\n",
    "        log_pa = [] \n",
    "        entropy_store = []\n",
    "        \n",
    "        for step in count(start=1):\n",
    "            action, log_prob_action, entropy = select_action(local_policy_network, state)\n",
    "            next_state, reward, done, info= env.step(action)\n",
    "            acc_rewards+=reward\n",
    "            is_truncated = 'TimeLimit.truncated' in info and\\\n",
    "                                info['TimeLimit.truncated']\n",
    "            is_failure = done and not is_truncated\n",
    "            \n",
    "            reward_store.append(reward)\n",
    "            state_store.append(state)\n",
    "            log_pa.append(log_prob_action)\n",
    "            entropy_store.append(entropy)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done == True or step - n_steps == max_steps:\n",
    "                #n_steps return remember...\n",
    "                if is_failure:#set reward to zero if final state is an actual termination \n",
    "                    #print(reward, step - n_steps,max_steps,done )\n",
    "                    reward_store[-1] = 0.0\n",
    "                \n",
    "                optimize_model(reward_store, log_pa, state_store,entropy_store,\\\n",
    "                    local_value_network,local_policy_network,\\\n",
    "                    global_policy_optimizer,global_value_optimizer,\\\n",
    "                    global_policy_network, global_value_network,\\\n",
    "                    gamma,entropy_beta)\n",
    "                \n",
    "                \n",
    "                reward_store = []\n",
    "                state_store = []\n",
    "                log_pa = [] \n",
    "                entropy_store = []\n",
    "                n_steps = step\n",
    "            if done == True:\n",
    "                ovr_rewards.append(acc_rewards)\n",
    "                break\n",
    "            #print(step - n_steps)\n",
    "            if step - n_steps == max_steps or delta >= abort_after:\n",
    "                #print(\"terminate_worker: \", terminate_worker)\n",
    "                terminate_worker = True\n",
    "                break\n",
    "    return ovr_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedAdam(torch.optim.Adam):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, amsgrad=False):\n",
    "        super(SharedAdam, self).__init__(\n",
    "            params, lr=lr, betas=betas, eps=eps, \n",
    "            weight_decay=weight_decay, amsgrad=amsgrad)\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'] = 0\n",
    "                state['shared_step'] = torch.zeros(1).share_memory_()\n",
    "                state['exp_avg'] = torch.zeros_like(p.data).share_memory_()\n",
    "                state['exp_avg_sq'] = torch.zeros_like(p.data).share_memory_()\n",
    "                if weight_decay:\n",
    "                    state['weight_decay'] = torch.zeros_like(p.data).share_memory_()\n",
    "                if amsgrad:\n",
    "                    state['max_exp_avg_sq'] = torch.zeros_like(p.data).share_memory_()\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                self.state[p]['steps'] = self.state[p]['shared_step'].item()\n",
    "                self.state[p]['shared_step'] += 1\n",
    "        super().step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A3C_(env, lr=0.0007, max_steps=100, entropy_beta=0.001, gamma = 0.99 ):\n",
    "    main_seed = 0\n",
    "    observation_space = len(env.reset())\n",
    "    action_space = env.action_space.n\n",
    "\n",
    "    global_policy_model = linearApproximator_FCDAP(observation_space,\\\n",
    "                                     action_space,hidden_dims=(128,64))\n",
    "    \n",
    "    global_value_model = linearApproximator_FCV(observation_space,\\\n",
    "                                     1,hidden_dims=(256,128))\n",
    "    \n",
    "    global_policy_optimizer = SharedAdam(global_policy_model.parameters(),lr=lr)\n",
    "    global_value_optimizer = SharedAdam(global_value_model.parameters(),lr=lr)\n",
    "    \n",
    "    worker_reward = []\n",
    "    for worker_rank in tqdm(range(10)):\n",
    "        worker_policy_model, worker_value_model, worker_env = worker(env,main_seed,worker_rank,\\\n",
    "                                                              global_policy_model,global_value_model)\n",
    "        \n",
    "        rwd = train_worker(worker_env,\\\n",
    "                 worker_policy_model, worker_value_model,\\\n",
    "                 global_policy_model, global_value_model,\\\n",
    "                 global_policy_optimizer, global_value_optimizer,\\\n",
    "                 max_steps, entropy_beta, gamma)\n",
    "        worker_reward.append(rwd)\n",
    "    return worker_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [20:00<00:00, 120.06s/it]\n"
     ]
    }
   ],
   "source": [
    "r = A3C_(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import uniform_filter1d\n",
    "arr = uniform_filter1d(r[9], size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "x = []\n",
    "y = []\n",
    "for i,e in enumerate(arr):\n",
    "    y.append(i)\n",
    "    x.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x26526341c88>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y, x)#tau 0.1, t = 10 0.01 decay, corrected, added min_sample sixe and batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
