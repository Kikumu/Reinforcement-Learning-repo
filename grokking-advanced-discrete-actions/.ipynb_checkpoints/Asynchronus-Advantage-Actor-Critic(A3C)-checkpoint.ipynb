{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch.nn.init import kaiming_uniform_#\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "import random as rand\n",
    "from itertools import count\n",
    "from torch.distributions import Categorical\n",
    "main_seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<CartPoleEnv<CartPole-v0>>>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "env.seed(0)\n",
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearApproximator_FCDAP(nn.Module):\n",
    "    def __init__(self,state_shape,outputs,hidden_dims=(32,32)):\n",
    "        super(linearApproximator_FCDAP, self).__init__()\n",
    "        self.input_size = state_shape\n",
    "        self.out = outputs\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available()\\\n",
    "                                   else \"cpu\")\n",
    "        \n",
    "        self.fc1  = nn.Linear(self.input_size,hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(\\\n",
    "                                hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        \n",
    "        self.output_layer  = nn.Linear(hidden_dims[-1],self.out)\n",
    "        self.to(self.device)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, state_shape):\n",
    "        if not isinstance(state_shape, torch.Tensor):\n",
    "            state_shape = torch.tensor(state_shape, dtype=torch.float32)\n",
    "        state_shape = state_shape.to(self.device)\n",
    "            \n",
    "        x = self.fc1(state_shape)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x))\n",
    "        \n",
    "        logits = self.output_layer(x)#logits, preferences of actions\n",
    "        return logits\n",
    "        \n",
    "    def full_pass(self, state):\n",
    "        logits = self.forward(state).cpu()\n",
    "        distribution = Categorical(logits=logits)\n",
    "        action = distribution.sample()#sample action\n",
    "        log_prob_action = distribution.log_prob(action).unsqueeze(-1)#gets prob of sampled action\n",
    "        entropy = distribution.entropy().unsqueeze(-1)\n",
    "        return action.item(), log_prob_action, entropy, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearApproximator_FCV(nn.Module):\n",
    "    def __init__(self,state_shape,outputs,hidden_dims=(32,32)):\n",
    "        super(linearApproximator_FCV, self).__init__()\n",
    "        self.input_size = state_shape\n",
    "        self.out = outputs\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available()\\\n",
    "                                   else \"cpu\")\n",
    "        \n",
    "        self.fc1  = nn.Linear(self.input_size,hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            hidden_layer = nn.Linear(\\\n",
    "                                hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        \n",
    "        self.output_layer  = nn.Linear(hidden_dims[-1],self.out)\n",
    "        self.to(self.device)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, state_shape):\n",
    "        if not isinstance(state_shape, torch.Tensor):\n",
    "            state_shape = torch.tensor(state_shape, dtype=torch.float32)\n",
    "        state_shape = state_shape.to(self.device)\n",
    "            \n",
    "        x = self.fc1(state_shape)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = F.relu(hidden_layer(x))\n",
    "        \n",
    "        state_value = self.output_layer(x)#logits, preferences of actions\n",
    "        return state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(env,main_seed,worker_rank,\\\n",
    "          global_policy_network,global_value_network\n",
    "          ):\n",
    "    \n",
    "    local_worker_seed = main_seed + worker_rank\n",
    "    #set seeds\n",
    "    env.seed(local_worker_seed)\n",
    "    torch.manual_seed(local_worker_seed)\n",
    "    np.random.seed(local_worker_seed)\n",
    "    random.seed(local_worker_seed)\n",
    "    \n",
    "    observation_space = len(env.reset())\n",
    "    action_space = env.action_space.n\n",
    "    \n",
    "    worker_policy_network = linearApproximator_FCDAP(observation_space,\\\n",
    "                                     action_space,hidden_dims=(128,64))\n",
    "    worker_value_network = linearApproximator_FCV(observation_space,\\\n",
    "                                     1,hidden_dims=(128,64))\n",
    "    \n",
    "    #load state dict and policy dict from global network to worker\n",
    "    worker_policy_network.load_state_dict(global_policy_network.state_dict())\n",
    "    worker_value_network.load_state_dict(global_value_network.state_dict())\n",
    "    \n",
    "    return worker_policy_network, worker_value_network, env\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(action_model, state):\n",
    "    action, log_prob_action, entropy, logits = action_model.full_pass(state)\n",
    "    return action, log_prob_action, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(reward_store, log_pa, states,entropies,\\\n",
    "                   local_value_network,local_policy_network,\\\n",
    "                   global_policy_optimizer, global_value_optimizer,\\\n",
    "                   global_policy_network, global_value_network,\\\n",
    "                   gamma,entropy_beta):\n",
    "    \n",
    "    Trajectory_length = len(reward_store)\n",
    "   \n",
    "    \n",
    "    discounts = np.logspace(0,Trajectory_length,\\\n",
    "                           num=Trajectory_length,base=gamma,\\\n",
    "                           endpoint=False)\n",
    "    \n",
    "    returns = np.array([np.sum(discounts[:Trajectory_length-t] * reward_store[t:]) for t in range(Trajectory_length)])\n",
    "    returns = torch.tensor(returns).unsqueeze(1)\n",
    "\n",
    "    \n",
    "    states = torch.tensor(states).float()\n",
    "    state_values = local_value_network(states)   \n",
    "    model_state_error = returns - state_values\n",
    "    model_state_loss = model_state_error.pow(2).mul(0.5).mean()\n",
    "    global_value_optimizer.zero_grad()\n",
    "    model_state_loss.backward()\n",
    "    \n",
    "    for local_params, global_params in zip(local_value_network.parameters(),\\\n",
    "                                          global_value_network.parameters()):\n",
    "        if global_params.grad is None:\n",
    "            global_params.grad = local_params.grad\n",
    "\n",
    "    global_value_optimizer.step()\n",
    "    local_value_network.load_state_dict(global_value_network.state_dict())\n",
    "    \n",
    "    log_pa = torch.stack(log_pa)\n",
    "    \n",
    "    discounts_tensor = torch.tensor(discounts).unsqueeze(1)\n",
    "    advantage = returns - state_values.detach()\n",
    "    policy_loss = -(advantage*log_pa*discounts_tensor).mean()\n",
    "    entropy = torch.tensor(entropies).unsqueeze(1)\n",
    "    entropy = -entropy.mean()\n",
    "    local_policy_loss +=entropy_beta*entropy\n",
    "    \n",
    "    global_policy_optimizer.zero_grad()\n",
    "    local_policy_loss.backward()\n",
    "    \n",
    "    #before backward pass of global param, we copy gradients\n",
    "    for local_param, global_param in zip(local_policy_network.paremeters(),\\\n",
    "                                  global_policy_network.parameters()):\n",
    "        if global_param.grad is None:\n",
    "            global_param.grad = local_param.grad\n",
    "    \n",
    "    global_policy_optimizer.step()\n",
    "    \n",
    "    local_policy_network.load_state_dict(global_policy_network.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_worker(env,\\\n",
    "                 local_policy_network, local_value_network,\\\n",
    "                 global_policy_network, global_value_network,\\\n",
    "                 global_policy_optimizer, global_value_optimizer,\\\n",
    "                 max_steps, entropy_beta):\n",
    "    \n",
    "    terminate_worker = False\n",
    "    while not terminate_worker:\n",
    "        state = env.reset()\n",
    "        acc_rewards = 0\n",
    "        \n",
    "        n_steps = 0\n",
    "        reward_store = []\n",
    "        state_store = []\n",
    "        log_pa = [] \n",
    "        entropy_store = []\n",
    "        \n",
    "        for step in count(start=1):\n",
    "            action, log_prob_action, entropy = select_action(local_policy_network, state)\n",
    "            next_state, reward, done, info= env.step(action)\n",
    "            \n",
    "            is_truncated = 'TimeLimit.truncated' in info and\\\n",
    "                                info['TimeLimit.truncated']\n",
    "            is_failure = done and not is_truncated\n",
    "            \n",
    "            reward_store.append(reward)\n",
    "            state_store(state)\n",
    "            log_pa.append(log_prob_action)\n",
    "            entropy_store.append(entropy)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done == True or step - n_steps == max_steps:\n",
    "                #n_steps return remember...\n",
    "                if is_failure:#set reward to zero if final state is an actual termination \n",
    "                    reward[-1] = 0\n",
    "                \n",
    "                optimize_model(reward_store, log_pa, state_store,entropy_store,\\\n",
    "                    local_value_network,local_policy_network,\\\n",
    "                    global_policy_optimizer,global_value_optimizer,\\\n",
    "                    global_policy_network, global_value_network,\\\n",
    "                    gamma,entropy_beta)\n",
    "                \n",
    "                \n",
    "                reward_store = []\n",
    "                state_store = []\n",
    "                log_pa = [] \n",
    "                entropy_store = []\n",
    "                n_steps = step\n",
    "            if done == True:\n",
    "                break\n",
    "            \n",
    "            if step - n_steps == max_steps:\n",
    "                terminate_worker = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedAdam(torch.optim.Adam):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, amsgrad=False):\n",
    "        super(SharedAdam, self).__init__(\n",
    "            params, lr=lr, betas=betas, eps=eps, \n",
    "            weight_decay=weight_decay, amsgrad=amsgrad)\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['step'] = 0\n",
    "                state['shared_step'] = torch.zeros(1).share_memory_()\n",
    "                state['exp_avg'] = torch.zeros_like(p.data).share_memory_()\n",
    "                state['exp_avg_sq'] = torch.zeros_like(p.data).share_memory_()\n",
    "                if weight_decay:\n",
    "                    state['weight_decay'] = torch.zeros_like(p.data).share_memory_()\n",
    "                if amsgrad:\n",
    "                    state['max_exp_avg_sq'] = torch.zeros_like(p.data).share_memory_()\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                self.state[p]['steps'] = self.state[p]['shared_step'].item()\n",
    "                self.state[p]['shared_step'] += 1\n",
    "        super().step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A3C_(env, lr=1e-5, max_steps=100, entropy_beta=0.001 ):\n",
    "    main_seed = 0\n",
    "    observation_space = len(env.reset())\n",
    "    action_space = env.action_space.n\n",
    "\n",
    "    global_policy_model = linearApproximator_FCDAP(observation_space,\\\n",
    "                                     action_space,hidden_dims=(128,64))\n",
    "    \n",
    "    global_value_model = linearApproximator_FCV(observation_space,\\\n",
    "                                     1,hidden_dims=(128,64))\n",
    "    \n",
    "    global_policy_optimizer = SharedAdam(global_policy_model.parameters(),lr=lr)\n",
    "    global_value_optimizer = SharedAdam(global_value_model.parameters(),lr=lr)\n",
    "    \n",
    "    \n",
    "    for worker_rank in range(10):\n",
    "        worker_policy_model, worker_value_model, worker_env = worker(env,main_seed,worker_rank,\\\n",
    "                                                              global_policy_model,global_value_model)\n",
    "        \n",
    "        train_worker(worker_env,\\\n",
    "                 worker_policy_model, worker_value_model,\\\n",
    "                 global_policy_model, global_value_model,\\\n",
    "                 global_policy_optimizer, global_value_optimizer,\\\n",
    "                 max_steps, entropy_beta)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
