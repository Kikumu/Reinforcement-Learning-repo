{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "import random as rand\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self,capacity):   \n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "        \n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count%self.capacity] = experience\n",
    "        self.push_count+=1\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return rand.sample(self.memory,batch_size)\n",
    "    \n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory)>=batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensors(experiences):\n",
    "    batch = Xp(*zip(*experiences))\n",
    "    state = np.stack(batch.state) #stack\n",
    "    action = np.stack(batch.action)\n",
    "    next_state = np.stack(batch.next_state)\n",
    "    reward = np.stack(batch.reward)\n",
    "    done = np.stack(batch.done)\n",
    "    abs_td_error = np.stack(batch.abs_td_error)\n",
    "    return state,action,next_state,reward,done,abs_td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Experience(state=5, action=6, next_state=7, reward=8, done=9, abs_td_error=10)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "Xp = namedtuple('Experience',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done', 'abs_td_error'))\n",
    "Xp_points = Xp(5,6,7,8,9,10)\n",
    "Xp_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearApproximator(nn.Module):\n",
    "    def __init__(self,state_shape,n_fc1,n_fc2, action_n):\n",
    "        super(linearApproximator, self).__init__()\n",
    "        self.input_size = state_shape\n",
    "        self.n_fc1 = n_fc1\n",
    "        self.n_fc2 = n_fc2\n",
    "        self.out = action_n\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.fc1  = nn.Linear(self.input_size,self.n_fc1)\n",
    "        self.fc2  = nn.Linear(self.n_fc1,self.n_fc2)\n",
    "        \n",
    "        self.state_value = nn.Linear(self.n_fc2, 1)\n",
    "        self.advantage_actions  = nn.Linear(self.n_fc2,self.out)\n",
    "        \n",
    "        \n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, state_shape):\n",
    "        x = self.fc1(state_shape)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        \n",
    "        actions_ = self.advantage_actions(x)\n",
    "        actions_ = F.relu(actions_)\n",
    "        state_value_=self.state_value(x)\n",
    "        state_value_=F.relu(state_value_)\n",
    "        state_value_ = state_value_.expand_as(actions_)\n",
    "        #adabantage function equation pg:311 - 316\n",
    "        q = state_value_ + (actions_ - actions_.mean().expand_as(actions_))\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_networks(online_network, target_network, tau):\n",
    "    #polyak averaging: pg:319\n",
    "    for target_weights, online_weights in zip(target_network.parameters(), online_network.parameters()):\n",
    "        target_weight_update = (1.0 - tau)*target_weights.data\n",
    "        online_weight_update = tau*online_weights.data\n",
    "        sum_up = target_weight_update + online_weight_update\n",
    "        target_weights.data.copy_(sum_up)\n",
    "    return online_network, target_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_online_model(experience_samples, online_network, target_network, gamma, optimizer, weights):\n",
    "    states, actions, next_states, rewards, done = extract_tensors(experience_samples)\n",
    "    \n",
    "    states = torch.tensor(states).float()\n",
    "    actions = torch.tensor(actions)\n",
    "    actions = actions.type(torch.LongTensor)\n",
    "    actions = actions.unsqueeze(1)\n",
    "    next_states=torch.tensor(next_states).float()\n",
    "    rewards = torch.tensor(rewards).float()\n",
    "    done = torch.tensor(done).float()\n",
    "    weights = torch.tensor(weights).float()\n",
    "    \n",
    "    q_online_next_states = online_network(next_states)#we now take next_states from online network \n",
    "    q_online_next_states = q_online_next_states.max(1)[1]#we now take the indices and not the values from online network\n",
    "    q_online_next_states = q_online_next_states.unsqueeze(1)#iindices\n",
    "     \n",
    "    \n",
    "    q_target_next_states_action = target_network(next_states)\n",
    "    q_target_next_states_action = q_target_next_states_action.detach()#always make sure detach on target net\n",
    "    q_target_next_states_action = q_target_next_states_action.gather(1, q_online_next_states)\n",
    "    #print(q_target_next_states_action.shape)\n",
    "    done = done.unsqueeze(1)\n",
    "    \n",
    "    #print(q_target_next_states_action.shape, done.shape)\n",
    "    \n",
    "    q_target_next_states_action *=(1 - done) \n",
    "    rewards = rewards.unsqueeze(1)\n",
    "    q_target = rewards + (gamma*q_target_next_states_action)\n",
    "    \n",
    "    \n",
    "    q_online_state = online_network(states)\n",
    "    q_online_state = q_online_state.gather(1, actions)\n",
    "    \n",
    "    q_u_loss = torch.nn.SmoothL1Loss()\n",
    "    q_u_loss = q_u_loss(q_online_state,q_target)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    q_u_loss.backward()\n",
    "    optimizer.step()\n",
    "    return online_network, target_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, model, epsilon):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    state = torch.tensor(state).float()\n",
    "    with torch.no_grad():\n",
    "        q_values= model(state).cpu().detach()\n",
    "        q_values = q_values.data.numpy().squeeze()\n",
    "    if np.random.rand() > epsilon:\n",
    "        action = np.argmax(q_values)\n",
    "    else:\n",
    "        action = np.random.randint(len(q_values))\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_decay_linear(init_eps, min_eps, decay_ratio, timestep, timestep_max):\n",
    "    decay_t_step = timestep_max*decay_ratio\n",
    "    epsilon = 1 - timestep/decay_t_step\n",
    "    epsilon *= init_eps-min_eps\n",
    "    epsilon+=min_eps\n",
    "    epsilon = np.clip(epsilon, min_eps, init_eps)\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_error(online_model, state, next_state):\n",
    "    q_val = online_model(state)\n",
    "    q_val = q_val.cpu.detach()#detatch\n",
    "    q_val = q_val.max()[0]#take values of max\n",
    "    q_val_nxt = online_model(next_state)\n",
    "    q_val_nxt = q_val_nxt.cpu.detach()#detach\n",
    "    q_val_nxt = q_val_nxt.max()[0]\n",
    "    return q_val, q_val_nxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prioritize_samples(experience_samples, alpha, beta):\n",
    "    state,action,next_state,reward,done,abs_td_error = experience_samples\n",
    "    #rank based\n",
    "    print(\"abs tensor shape: \", abs_td_error.shape)\n",
    "    abs_td_error  = abs_td_error.unsqueeze(1)\n",
    "    abs_td_error, indices_ = abs_td_error.sort(0, descending=True)#big to small\n",
    "    \n",
    "    indices = np.arange(1, len(abs_td_error)+1)\n",
    "    priorities = 1.0/indices\n",
    "    priorities = priorities**alpha#scale by alpha\n",
    "    probabilities = priorities/=priorities.sum(pr, axis=0)#sums up to 1(or 0.999999)\n",
    "    \n",
    "    assert np.isclose(probabilities.sum(), 1.0)#ensures probs add up to 1\n",
    "    \n",
    "    number_of_samples  = len(probabilities)\n",
    "    weight_importance_ = number_of_samples*probabilities\n",
    "    weight_importance_ = weight_importance_**-beta\n",
    "    \n",
    "    #downscale the weights, max==1, everything else lower\n",
    "    \n",
    "    \n",
    "    return weight_priority#we will sample according to this value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Duelling_DDQN_PER(env,\n",
    "         gamma=0.9,\n",
    "         memory_size = 50000,\n",
    "         init_epsilon=1.0,\n",
    "         min_epsilon=0.3,\n",
    "         epsilon_decay_ratio=0.4,\n",
    "         tau = 0.3,\n",
    "         target_update = 30,\n",
    "         min_sample_size=320,\n",
    "         batch_size = 64,\n",
    "         n_ep=20000,\n",
    "         priority_epsilon_constant = 0.01,\n",
    "         max_t_steps = 100000):\n",
    "    \n",
    "    action_space = env.action_space.n\n",
    "    observation_space = len(env.reset())\n",
    "    hidden_1 = 512\n",
    "    hidden_2 = 128\n",
    "    \n",
    "    online_network = linearApproximator(observation_space, hidden_1, hidden_2, action_space)\n",
    "    target_network = linearApproximator(observation_space, hidden_1, hidden_2, action_space)\n",
    "    target_network.eval()\n",
    "    target_network = freeze_model(target_network)\n",
    "    \n",
    "    optimizer = torch.optim.RMSprop(online_network.parameters(),lr=0.0007)\n",
    "    memory = ReplayMemory(memory_size)\n",
    "    \n",
    "    t_step = 0 #important\n",
    "    reward_per_ep = []\n",
    "    \n",
    "    for e in tqdm(range(n_ep)):\n",
    "        state = env.reset()\n",
    "        reward_accumulated = 0\n",
    "        while True:\n",
    "            epsilon = epsilon_decay_linear(init_epsilon, min_epsilon, epsilon_decay_ratio, t_step, max_t_steps)\n",
    "            action = select_action(state, online_network, epsilon)\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            q_state_a, q_next_state_a = query_error(online_model, state, next_state)\n",
    "            #convert them to numpy\n",
    "            #calculate shock value and append error to replay memory for PER analysis\n",
    "            td_target = reward + gamma*q_next_state_a\n",
    "            td_error = abs(td_target - q_state_a)#shock value\n",
    "            \n",
    "            reward_accumulated+=reward\n",
    "            is_truncated = 'TimeLimit.truncated' in info and\\\n",
    "                                info['TimeLimit.truncated']\n",
    "            is_failure = done and not is_truncated\n",
    "           \n",
    "            memory.push(Xp(state, action, next_state, reward, is_failure, td_error))\n",
    "            state = next_state\n",
    "            t_step+=1\n",
    "            if memory.can_provide_sample(min_sample_size):\n",
    "                #we on;y update td_errors for samples used to update the network - meaning batch_size?\n",
    "                experience_samples = memory.sample(batch_size)\n",
    "                online_network, target_network = update_online_model(experience_samples, online_network, target_network, gamma, optimizer)\n",
    "            if t_step%target_update:\n",
    "                online_network, target_network = update_networks(online_network, target_network, tau)\n",
    "            if done == True:\n",
    "                reward_per_ep.append(reward_accumulated)\n",
    "                break\n",
    "            if t_step > max_t_steps:\n",
    "                return online_network, reward_per_ep\n",
    "    online_network, reward_per_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                               | 21/20000 [00:02<33:44,  9.87it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-dc330a29dfb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDuelling_DDQN_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-32e7ff543b37>\u001b[0m in \u001b[0;36mDuelling_DDQN_\u001b[1;34m(env, gamma, memory_size, init_epsilon, min_epsilon, epsilon_decay_ratio, tau, target_update, min_sample_size, batch_size, n_ep, max_t_steps)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepsilon_decay_linear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_epsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_epsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon_decay_ratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_t_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monline_network\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mreward_accumulated\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-8aff3ab190e8>\u001b[0m in \u001b[0;36mselect_action\u001b[1;34m(state, model, epsilon)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mq_values\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, rewards = Duelling_DDQN_PER(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9752],\n",
       "        [ 0.6793],\n",
       "        [ 0.1374],\n",
       "        [-1.0932],\n",
       "        [ 1.2527],\n",
       "        [-0.5082],\n",
       "        [ 0.4897]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> x = torch.randn(7, 1)\n",
    "print(x.shape)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sorted = x.sort(0, descending=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2527],\n",
       "        [ 0.6793],\n",
       "        [ 0.4897],\n",
       "        [ 0.1374],\n",
       "        [-0.5082],\n",
       "        [-0.9752],\n",
       "        [-1.0932]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9776],\n",
       "        [-0.7620],\n",
       "        [-0.5711],\n",
       "        [ 0.4845],\n",
       "        [ 0.8246],\n",
       "        [ 1.5001],\n",
       "        [-0.5815]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> sorted, indices = torch.sort(x)\n",
    ">>> sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_arr = np.arange(1, len(x_sorted) + 1)\n",
    "_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.5       , 0.33333333, 0.25      , 0.2       ,\n",
       "       0.16666667, 0.14285714])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr2 = 1/_arr\n",
    "arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7,)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = arr2**2\n",
    "pr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr/=np.sum(pr, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66146445, 0.16536611, 0.07349605, 0.04134153, 0.02645858,\n",
       "       0.01837401, 0.01349927])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
