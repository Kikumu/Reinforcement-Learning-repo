{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "import random as rand\n",
    "from itertools import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self,capacity):   \n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "        \n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count%self.capacity] = experience\n",
    "        self.push_count+=1\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        return rand.sample(self.memory,batch_size)\n",
    "    \n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory)>=batch_size\n",
    "    \n",
    "    def update_td_error(self, sampled_experiences):\n",
    "        indexes = []\n",
    "        for sampled_idx,sampled_exp in enumerate(sampled_experiences):\n",
    "            for mem_idx, mem_exp in enumerate(self.memory):\n",
    "                if mem_exp.timestep == sampled_exp.timestep:\n",
    "                    self.memory[mem_idx] = sampled_exp #update memory\n",
    "                    break\n",
    "                     \n",
    "    def get_memory_values(self):\n",
    "        return self.memory    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensors(experiences):\n",
    "    batch = Xp(*zip(*experiences))\n",
    "    state = np.stack(batch.state) #stack\n",
    "    action = np.stack(batch.action)\n",
    "    next_state = np.stack(batch.next_state)\n",
    "    reward = np.stack(batch.reward)\n",
    "    done = np.stack(batch.done)\n",
    "    abs_td_error = np.stack(batch.abs_td_error)\n",
    "    timestep = np.stack(batch.timestep)\n",
    "    return state,action,next_state,reward,done,abs_td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_experiences(state, action, next_state, reward, done, abs_error, timestep):\n",
    "    exp_list = []\n",
    "    exp = (state, action, next_state, reward, done, abs_error, timestep)\n",
    "    for e in exp:\n",
    "        state, action, next_state, reward, is_failure, td_error, t_step = e\n",
    "        exp_list.append(\\\n",
    "                        Xp(state, action, next_state, reward, is_failure, td_error, t_step))\n",
    "    return exp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Experience(state=5, action=6, next_state=7, reward=8, done=9, abs_td_error=10, timestep=11)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import namedtuple\n",
    "Xp = namedtuple('Experience',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done', 'abs_td_error','timestep'))\n",
    "Xp_points = Xp(5,6,7,8,9,10,11)\n",
    "Xp_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearApproximator(nn.Module):\n",
    "    def __init__(self,state_shape,n_fc1,n_fc2, action_n):\n",
    "        super(linearApproximator, self).__init__()\n",
    "        self.input_size = state_shape\n",
    "        self.n_fc1 = n_fc1\n",
    "        self.n_fc2 = n_fc2\n",
    "        self.out = action_n\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.fc1  = nn.Linear(self.input_size,self.n_fc1)\n",
    "        self.fc2  = nn.Linear(self.n_fc1,self.n_fc2)\n",
    "        \n",
    "        self.state_value = nn.Linear(self.n_fc2, 1)\n",
    "        self.advantage_actions  = nn.Linear(self.n_fc2,self.out)\n",
    "        \n",
    "        \n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, state_shape):\n",
    "        x = self.fc1(state_shape)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        \n",
    "        actions_ = self.advantage_actions(x)\n",
    "        actions_ = F.relu(actions_)\n",
    "        state_value_=self.state_value(x)\n",
    "        state_value_=F.relu(state_value_)\n",
    "        state_value_ = state_value_.expand_as(actions_)\n",
    "        #adabantage function equation pg:311 - 316\n",
    "        q = state_value_ + (actions_ - actions_.mean().expand_as(actions_))\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_networks(online_network, target_network, tau):\n",
    "    #polyak averaging: pg:319\n",
    "    for target_weights, online_weights in zip(target_network.parameters(), online_network.parameters()):\n",
    "        target_weight_update = (1.0 - tau)*target_weights.data\n",
    "        online_weight_update = tau*online_weights.data\n",
    "        sum_up = target_weight_update + online_weight_update\n",
    "        target_weights.data.copy_(sum_up)\n",
    "    return online_network, target_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_online_model(experience_samples, online_network, target_network, gamma, optimizer,\\\n",
    "                        weighted_importance, indices):\n",
    "    states, actions, next_states, rewards, done, td_errors, timesteps = extract_tensors(experience_samples)\n",
    "    \n",
    "    states = torch.tensor(states).float()\n",
    "    actions = torch.tensor(actions)\n",
    "    actions = actions.type(torch.LongTensor)\n",
    "    actions = actions.unsqueeze(1)\n",
    "    next_states=torch.tensor(next_states).float()\n",
    "    rewards = torch.tensor(rewards).float()\n",
    "    done = torch.tensor(done).float()\n",
    "    weighted_importance = torch.tensor(weighted_importance).float()\n",
    "    \n",
    "    #rearrange to follow indices\n",
    "    #torch.gather(src, 0, index), gather each one by batch and rearrange batch according to indices\n",
    "    states = torch.gather(states, 0, indices)\n",
    "    actions = torch.gather(actions, 0, indices)\n",
    "    next_states = torch.gather(next_states, 0, indices)\n",
    "    rewards = torch.gather(rewards, 0, indices)\n",
    "    done = torch.gather(done, 0, indices)\n",
    "    \n",
    "    q_online_next_states = online_network(next_states)#we now take next_states from online network \n",
    "    q_online_next_states = q_online_next_states.max(1)[1]#we now take the indices and not the values from online network\n",
    "    q_online_next_states = q_online_next_states.unsqueeze(1)#iindices\n",
    "     \n",
    "    \n",
    "    q_target_next_states_action = target_network(next_states)\n",
    "    q_target_next_states_action = q_target_next_states_action.detach()#always make sure detach on target net\n",
    "    q_target_next_states_action = q_target_next_states_action.gather(1, q_online_next_states)\n",
    "    #print(q_target_next_states_action.shape)\n",
    "    done = done.unsqueeze(1)\n",
    "    \n",
    "    #print(q_target_next_states_action.shape, done.shape)\n",
    "    \n",
    "    q_target_next_states_action *=(1 - done) \n",
    "    rewards = rewards.unsqueeze(1)\n",
    "    q_target = rewards + (gamma*q_target_next_states_action)\n",
    "    \n",
    "    \n",
    "    q_online_state = online_network(states)\n",
    "    q_online_state = q_online_state.gather(1, actions)\n",
    "    \n",
    "    q_online_state*= weighted_importance\n",
    "    q_target*=weighted_importance\n",
    "    \n",
    "    abs_error = abs(q_online_state - q_target)#update experience errors\n",
    "    \n",
    "    q_u_loss = torch.nn.SmoothL1Loss()\n",
    "    q_u_loss = q_u_loss(q_online_state,q_target)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    q_u_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    states, actions, next_states, rewards, done, td_errors, timesteps = extract_tensors(experience_samples)\n",
    "    experiences_rebuilded = rebuild_experiences(states, actions, next_states, rewards, done, abs_error, timesteps)\n",
    "    \n",
    "    return online_network, target_network, experiences_rebuilded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, model, epsilon):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    state = torch.tensor(state).float()\n",
    "    with torch.no_grad():\n",
    "        q_values= model(state).cpu().detach()\n",
    "        q_values = q_values.data.numpy().squeeze()\n",
    "    if np.random.rand() > epsilon:\n",
    "        action = np.argmax(q_values)\n",
    "    else:\n",
    "        action = np.random.randint(len(q_values))\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_decay_linear(init_eps, min_eps, decay_ratio, timestep, timestep_max):\n",
    "    decay_t_step = timestep_max*decay_ratio\n",
    "    epsilon = 1 - timestep/decay_t_step\n",
    "    epsilon *= init_eps-min_eps\n",
    "    epsilon+=min_eps\n",
    "    epsilon = np.clip(epsilon, min_eps, init_eps)\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_error(online_model, state, next_state):\n",
    "    q_val = online_model(state)\n",
    "    q_val = q_val.detach().cpu().numpy()#detatch\n",
    "    q_val = q_val.max()[0]#take values of max\n",
    "    q_val_nxt = online_model(next_state)\n",
    "    q_val_nxt = q_val_nxt.detach().cpu().numpy()#detach\n",
    "    q_val_nxt = q_val_nxt.max()[0]#\n",
    "    return q_val, q_val_nxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prioritize_samples(experience_samples, alpha, beta):\n",
    "    state,action,next_state,reward,done,abs_td_error = experience_samples\n",
    "    #rank based\n",
    "    print(\"abs tensor shape: \", abs_td_error.shape)\n",
    "    abs_td_error  = abs_td_error.unsqueeze(1)\n",
    "    abs_td_error, indices_ = abs_td_error.sort(0, descending=True)#big to small\n",
    "    \n",
    "    indices = np.arange(1, len(abs_td_error)+1)\n",
    "    priorities = 1.0/indices\n",
    "    priorities = priorities**alpha#scale by alpha\n",
    "    probabilities = priorities/priorities.sum(pr, axis=0)#sums up to 1(or 0.999999)\n",
    "    \n",
    "    assert np.isclose(probabilities.sum(), 1.0)#ensures probs add up to 1\n",
    "    \n",
    "    number_of_samples  = len(probabilities)\n",
    "    weight_importance_ = number_of_samples*probabilities\n",
    "    weight_importance_ = weight_importance_**-beta\n",
    "    \n",
    "    #downscale the weights, max==1, everything else lower\n",
    "    weight_importance_max = np.max(weight_importance_)\n",
    "    weight_importance_scaled = weight_importance_/weight_importance_max\n",
    "    \n",
    "    return weight_importance_scaled, indices_ #return weight important samples, return indices for re_arranging sampled experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Duelling_DDQN_PER(env,\n",
    "         gamma=0.9,\n",
    "         alpha_pr=0.01,\n",
    "         beta_pr=0.01,\n",
    "         memory_size = 50000,\n",
    "         init_epsilon=1.0,\n",
    "         min_epsilon=0.3,\n",
    "         epsilon_decay_ratio=0.4,\n",
    "         tau = 0.3,\n",
    "         target_update = 30,\n",
    "         min_sample_size=320,\n",
    "         batch_size = 64,\n",
    "         n_ep=20000,\n",
    "         priority_epsilon_constant = 0.01,\n",
    "         max_t_steps = 100000):\n",
    "    \n",
    "    action_space = env.action_space.n\n",
    "    observation_space = len(env.reset())\n",
    "    hidden_1 = 512\n",
    "    hidden_2 = 128\n",
    "    \n",
    "    online_network = linearApproximator(observation_space, hidden_1, hidden_2, action_space)\n",
    "    target_network = linearApproximator(observation_space, hidden_1, hidden_2, action_space)\n",
    "    target_network.eval()\n",
    "    target_network = freeze_model(target_network)\n",
    "    \n",
    "    optimizer = torch.optim.RMSprop(online_network.parameters(),lr=0.0007)\n",
    "    memory = ReplayMemory(memory_size)\n",
    "    \n",
    "    t_step = 0 #important\n",
    "    reward_per_ep = []\n",
    "    \n",
    "    for e in tqdm(range(n_ep)):\n",
    "        state = env.reset()\n",
    "        reward_accumulated = 0\n",
    "        while True:\n",
    "            epsilon = epsilon_decay_linear(init_epsilon, min_epsilon, epsilon_decay_ratio, t_step, max_t_steps)\n",
    "            action = select_action(state, online_network, epsilon)\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            q_state_a, q_next_state_a = query_error(online_model, state, next_state)\n",
    "            #convert them to numpy\n",
    "            #calculate shock value and append error to replay memory for PER analysis\n",
    "            td_target = reward + gamma*q_next_state_a\n",
    "            td_error = abs(td_target - q_state_a)#shock value\n",
    "            \n",
    "            reward_accumulated+=reward\n",
    "            is_truncated = 'TimeLimit.truncated' in info and\\\n",
    "                                info['TimeLimit.truncated']\n",
    "            is_failure = done and not is_truncated\n",
    "           \n",
    "            memory.push(Xp(state, action, next_state, reward, is_failure, td_error, t_step))\n",
    "            state = next_state\n",
    "            t_step+=1\n",
    "            if memory.can_provide_sample(min_sample_size):\n",
    "                #we only update td_errors for samples used to update the network - meaning batch_size?\n",
    "                experience_samples = memory.sample(batch_size)\n",
    "                weighted_importance, indices = prioritize_samples(experience_samples, alpha, beta)\n",
    "                online_network, target_network, rebuilded_exp = update_online_model(experience_samples, online_network,\\\n",
    "                                                                                    target_network, gamma, optimizer,\\\n",
    "                                                                                    weighted_importance, indices)\n",
    "                memory.update_td_error(rebuilded_exp)\n",
    "            if t_step%target_update:\n",
    "                online_network, target_network = update_networks(online_network, target_network, tau)\n",
    "            if done == True:\n",
    "                reward_per_ep.append(reward_accumulated)\n",
    "                break\n",
    "            if t_step > max_t_steps:\n",
    "                return online_network, reward_per_ep\n",
    "    online_network, reward_per_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                               | 21/20000 [00:02<33:44,  9.87it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-dc330a29dfb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDuelling_DDQN_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-32e7ff543b37>\u001b[0m in \u001b[0;36mDuelling_DDQN_\u001b[1;34m(env, gamma, memory_size, init_epsilon, min_epsilon, epsilon_decay_ratio, tau, target_update, min_sample_size, batch_size, n_ep, max_t_steps)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepsilon_decay_linear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_epsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_epsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon_decay_ratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_t_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monline_network\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mreward_accumulated\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-8aff3ab190e8>\u001b[0m in \u001b[0;36mselect_action\u001b[1;34m(state, model, epsilon)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mq_values\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, rewards = Duelling_DDQN_PER(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5445],\n",
       "        [-0.0126],\n",
       "        [-2.1726],\n",
       "        [ 0.5998],\n",
       "        [ 0.3943],\n",
       "        [-0.5349],\n",
       "        [-0.5002]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> x = torch.randn(7, 1)\n",
    "print(x.shape)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sorted, index = x.sort(0, descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5998],\n",
       "        [ 0.3943],\n",
       "        [-0.0126],\n",
       "        [-0.5002],\n",
       "        [-0.5349],\n",
       "        [-1.5445],\n",
       "        [-2.1726]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3],\n",
       "        [4],\n",
       "        [1],\n",
       "        [6],\n",
       "        [5],\n",
       "        [0],\n",
       "        [2]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_arr = np.arange(1, len(x_sorted) + 1)\n",
    "_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.5       , 0.33333333, 0.25      , 0.2       ,\n",
       "       0.16666667, 0.14285714])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr2 = 1/_arr\n",
    "arr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.25      , 0.11111111, 0.0625    , 0.04      ,\n",
       "       0.02777778, 0.02040816])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = arr2**2\n",
    "pr.shape\n",
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr/=np.sum(pr, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66146445, 0.16536611, 0.07349605, 0.04134153, 0.02645858,\n",
       "       0.01837401, 0.01349927])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6614644462860121"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.25      , 0.11111111, 0.0625    , 0.04      ,\n",
       "       0.02777778, 0.02040816])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr/np.max(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = torch.arange(1, 8)\n",
    "src.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = index.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = index.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_clone = src.clone()\n",
    "src_clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6, 7])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 1, 6, 5, 0, 2])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index#4,5,2,7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 5, 2, 7, 6, 1, 3])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(src, 0, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 5 8 7 6 0 2 4 1 9]\n",
      "[(6, 2), (2, 8)]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "l = np.arange(10)  # example list\n",
    "random.shuffle(l) # we shuffle the list\n",
    "print(l) # outputs [4, 1, 5, 0, 6, 7, 9, 2, 8, 3]\n",
    "index_value = random.sample(list(enumerate(l)), 2)\n",
    "print(index_value) # outputs [(4, 6), (6, 9)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ReplayMemory(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(10):\n",
    "    memory.push(Xp(t,t,t,t,t,t,t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_m = memory.sample(1)\n",
    "(s_m[0].timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Experience(state=100, action=100, next_state=100, reward=100, done=100, abs_td_error=100, timestep=100),\n",
       " Experience(state=1, action=1, next_state=1, reward=1, done=1, abs_td_error=1, timestep=1),\n",
       " Experience(state=2, action=2, next_state=2, reward=2, done=2, abs_td_error=2, timestep=2),\n",
       " Experience(state=3, action=3, next_state=3, reward=3, done=3, abs_td_error=3, timestep=3),\n",
       " Experience(state=4, action=4, next_state=4, reward=4, done=4, abs_td_error=4, timestep=4),\n",
       " Experience(state=5, action=5, next_state=5, reward=5, done=5, abs_td_error=5, timestep=5),\n",
       " Experience(state=6, action=6, next_state=6, reward=6, done=6, abs_td_error=6, timestep=6),\n",
       " Experience(state=7, action=7, next_state=7, reward=7, done=7, abs_td_error=7, timestep=7),\n",
       " Experience(state=8, action=8, next_state=8, reward=8, done=8, abs_td_error=8, timestep=8),\n",
       " Experience(state=9, action=9, next_state=9, reward=9, done=9, abs_td_error=9, timestep=9)]"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_m = memory.get_memory_values()\n",
    "t_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "for a,b in enumerate(s_m):\n",
    "    for x, y in enumerate(t_m):\n",
    "        if y.timestep == b.timestep:\n",
    "            t_m[x] = Xp(100,100,100,100,100,100,100)\n",
    "            print(y[a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Experience(state=100, action=100, next_state=100, reward=100, done=100, abs_td_error=100, timestep=100),\n",
       " Experience(state=1, action=1, next_state=1, reward=1, done=1, abs_td_error=1, timestep=1),\n",
       " Experience(state=2, action=2, next_state=2, reward=2, done=2, abs_td_error=2, timestep=2),\n",
       " Experience(state=3, action=3, next_state=3, reward=3, done=3, abs_td_error=3, timestep=3),\n",
       " Experience(state=4, action=4, next_state=4, reward=4, done=4, abs_td_error=4, timestep=4),\n",
       " Experience(state=5, action=5, next_state=5, reward=5, done=5, abs_td_error=5, timestep=5),\n",
       " Experience(state=100, action=100, next_state=100, reward=100, done=100, abs_td_error=100, timestep=100),\n",
       " Experience(state=7, action=7, next_state=7, reward=7, done=7, abs_td_error=7, timestep=7),\n",
       " Experience(state=8, action=8, next_state=8, reward=8, done=8, abs_td_error=8, timestep=8),\n",
       " Experience(state=9, action=9, next_state=9, reward=9, done=9, abs_td_error=9, timestep=9)]"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x, y in enumerate(t_m) if y[-1] == s_m[0].timestep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Experience' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-304-5a67d6b29772>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcnt_m\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_m\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_m\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mexp_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mexp_m\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m             \u001b[0mexp_m\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;31m#update td_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Experience' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "#[x for t_step in enumerate(t_m) if y[1] == 7]\n",
    "for cnt_, exp_ in enumerate(s_m):    \n",
    "    for cnt_m, exp_m in enumerate(t_m):\n",
    "        if exp_[-1] == exp_m[-1]:\n",
    "            exp_m[-2] = 100#update td_error\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x, y in enumerate(t_m) if y[-1] == s_m[0].timestep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_m[1].timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-294-eb577fa1101a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_m\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "for x, y in enumerate(t_m):\n",
    "    print(y[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'map' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-266-781aa6b5ebd7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_m\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms_m\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'map' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "f = operator.itemgetter(0)\n",
    "map(f, t_m).index(s_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\scowt\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(s_m == t_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\scowt\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-227-ee9ff9dd4d9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_m\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ms_m\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'all'"
     ]
    }
   ],
   "source": [
    "np.where((t_m == (s_m))).all(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(t_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
