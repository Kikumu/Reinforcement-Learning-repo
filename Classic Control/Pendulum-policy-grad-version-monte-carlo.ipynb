{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee54f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from random import seed\n",
    "from sklearn import preprocessing\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ef4543",
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_layer(nn.Module):\n",
    "    def __init__(self, lr, input_size, linear_out):\n",
    "        super(linear_layer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.linear_out = linear_out\n",
    "        self.lr = lr\n",
    "        self.n_hidden_fc = 400\n",
    "        \n",
    "        \n",
    "        self.fc   = nn.Linear(self.input_size,self.n_hidden_fc)\n",
    "        self.fc1  = nn.Linear(self.n_hidden_fc,self.n_hidden_fc)\n",
    "        self.fc2  = nn.Linear(self.n_hidden_fc,self.n_hidden_fc)\n",
    "        self.fc3  = nn.Linear(self.n_hidden_fc,self.n_hidden_fc)\n",
    "        self.fc4  = nn.Linear(self.n_hidden_fc,self.linear_out)\n",
    "        \n",
    "        self.optimizer = optim.Adam(params = self.parameters() ,lr = lr)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc(x))\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = F.relu(self.fc3(out))\n",
    "        out = self.fc4(out)\n",
    "        return out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0b13d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class policy_gradient_agent():\n",
    "    def __init__(self, lr, n_features, seq_length, outputs, gamma = 0.88, num_layers = 3):\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.reward_memory = []\n",
    "        self.action_memory = []\n",
    "        #self.policy = LSTM_V2(lr, n_features, seq_length, outputs).to(device)\n",
    "        self.length = n_features\n",
    "        self.width  = seq_length\n",
    "        self.policy = linear_layer(lr, self.length*self.width, outputs).to(device)\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        prob_outputs = F.softmax(self.policy.forward(state))\n",
    "        action_probs = torch.distributions.Categorical(prob_outputs) #model random choice out of distributions\n",
    "        action = action_probs.sample()\n",
    "        log_prob = action_probs.log_prob(action)\n",
    "        \n",
    "        self.action_memory.append(log_prob)\n",
    "        return action.item()\n",
    "    \n",
    "    def store_rewards(self,reward):\n",
    "        self.reward_memory.append(reward)\n",
    "        \n",
    "    def learn(self):\n",
    "        self.policy.optimizer.zero_grad()\n",
    "        G = np.zeros_like(self.reward_memory,dtype = np.float64)\n",
    "        for t in range(len(self.reward_memory)):\n",
    "            G_sum = 0\n",
    "            pwr   = 0\n",
    "            for k in range(t,len(self.reward_memory)):\n",
    "                G_sum += np.power(self.gamma, pwr)*self.reward_memory[k]\n",
    "                pwr+=1\n",
    "            G[t] = G_sum\n",
    "            #print(G[t])\n",
    "        G = torch.tensor(G, dtype = float).to(device)\n",
    "        loss = 0\n",
    "        for g_t, action_log_prob in zip(G, self.action_memory):\n",
    "            loss+=-g_t*action_log_prob\n",
    "        loss.backward()\n",
    "        self.policy.optimizer.step()\n",
    "        \n",
    "        self.reward_memory = []\n",
    "        self.action_memory = []\n",
    "        \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba236de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_running_curve(scores, x, figure_file):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range (len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i - 100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title('Running average of previous 100 scores')\n",
    "    plt.savefig(figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bd7c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvManager():\n",
    "    def __init__(self, device,environment):\n",
    "        self.device = device\n",
    "        #self.env = gym.make(environment).unwrapped\n",
    "        self.env = gym.make(environment)\n",
    "        self.env.reset()\n",
    "        self.current_screen = None\n",
    "        self.done = False\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        self.current_screen = None\n",
    "        \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode)\n",
    "        \n",
    "    def num_actions_available(self):\n",
    "        return self.env.action_space\n",
    "        \n",
    "    def take_action(self, action):   \n",
    "        _, reward, self.done, _ = self.env.step([action])\n",
    "        return reward\n",
    "    \n",
    "    def just_starting(self):\n",
    "        return self.current_screen is None\n",
    "    \n",
    "    def get_state(self):\n",
    "        if self.just_starting() or self.done:\n",
    "            self.current_screen = self.get_processed_screen()\n",
    "            black_screen = torch.zeros_like(self.current_screen)\n",
    "            return black_screen\n",
    "        else:\n",
    "            s1 = self.current_screen\n",
    "            s2 = self.get_processed_screen()\n",
    "            self.current_screen = s2\n",
    "            return s2 - s1\n",
    "    \n",
    "    def get_screen_height(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[2]\n",
    "    \n",
    "    def get_screen_width(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[3]\n",
    "       \n",
    "    def get_processed_screen(self):\n",
    "        screen = em.render('rgb_array')\n",
    "        rgb_weights = [0.2989, 0.5870, 0.1140]\n",
    "        grayscale_image = np.dot(screen[...,:3], rgb_weights) \n",
    "        screen = grayscale_image.transpose((0, 1)) # PyTorch expects CHW\n",
    "        #print(type(screen)) # numpy\n",
    "        screen = self.crop_screen(screen)\n",
    "        return self.transform_screen_data(screen)\n",
    "    \n",
    "    def crop_screen(self, screen):\n",
    "        screen_height = screen.shape[0]\n",
    "        screen_width  = screen.shape[1]\n",
    "        #print('screen height(top/bottom): ',screen_height)\n",
    "        #print('screen height(left/right): ',screen_width)\n",
    "        # Strip off top and bottom\n",
    "        top = int(screen_height * 0.2)\n",
    "        #print('top: ',top)\n",
    "        bottom = int(screen_height * 0.8)\n",
    "        #print('bottom: ',bottom)\n",
    "        \n",
    "        \n",
    "        #strip off left/right\n",
    "        left  = int(screen_width * 0.2)\n",
    "        #print('left: ',left)\n",
    "        right = int(screen_width * 0.8)\n",
    "        #print('right: ',right)\n",
    "        \n",
    "        screen = screen[top:bottom, left:right]\n",
    "        return screen\n",
    "    \n",
    "    \n",
    "    def transform_screen_data(self, screen):       \n",
    "        # Convert to float, rescale, convert to tensor\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "        \n",
    "        # Use torchvision package to compose image transforms\n",
    "        resize = T.Compose([\n",
    "            T.ToPILImage()\n",
    "            ,T.Resize((40,90))\n",
    "            ,T.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        return resize(screen).unsqueeze(0).to(self.device) # add a batch dimension (BCHW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ff859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_action_bins(bin_size,min_val,max_val):\n",
    "    action_bin_array = np.linspace(min_val, max_val, num=bin_size)\n",
    "    return action_bin_array\n",
    "\n",
    "action_bin_array = set_action_bins(100,-2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b47b9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0672e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "em = EnvManager(device,'Pendulum-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03712e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_num = len(action_bin_array)\n",
    "agent = policy_gradient_agent(5e-6,em.get_screen_width(),em.get_screen_height(),output_num)\n",
    "scores = []\n",
    "for episode in range(5000):\n",
    "    em.reset()\n",
    "    state = em.get_state()\n",
    "    score = 0\n",
    "    for timestep in count():\n",
    "        #state   = state.squeeze()\n",
    "        #state   = state.unsqueeze(dim = 0)\n",
    "        #print(state.shape)\n",
    "        state = state.flatten()\n",
    "        #print(state.shape)\n",
    "        action_idx = agent.select_action(state)\n",
    "        reward     = em.take_action([action_bin_array[action_idx]])\n",
    "        score+=reward\n",
    "        agent.store_rewards(reward)\n",
    "        next_state = em.get_state()\n",
    "        state = next_state\n",
    "        if em.done:\n",
    "            agent.learn()\n",
    "            scores.append(score)\n",
    "            avg_score = np.mean(scores[-100:])\n",
    "            print('episode: ', episode, 'score%.2f '% score, 'avg_scr %.2f'%avg_score)\n",
    "            break\n",
    "x = [episode + 1 for i in range(len(scores))]\n",
    "fname = 'graph'\n",
    "save = fname + '.png'\n",
    "plot_learning_curve(scores,x,save)\n",
    "em.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5087f196",
   "metadata": {},
   "outputs": [],
   "source": [
    "em.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2225a052",
   "metadata": {},
   "outputs": [],
   "source": [
    "5e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a969749d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
