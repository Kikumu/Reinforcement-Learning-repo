{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "102f8117",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from random import seed\n",
    "from sklearn import preprocessing\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60b0d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_layer(nn.Module):\n",
    "    def __init__(self, lr, input_size, linear_out):\n",
    "        super(linear_layer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.linear_out = linear_out\n",
    "        self.lr = lr\n",
    "        self.n_hidden_fc = 1000\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.fc   = nn.Linear(self.input_size,self.n_hidden_fc)\n",
    "        self.fc1  = nn.Linear(self.n_hidden_fc,self.n_hidden_fc)\n",
    "        self.fc2  = nn.Linear(self.n_hidden_fc,self.n_hidden_fc)\n",
    "        \n",
    "        self.critic = nn.Linear(self.n_hidden_fc,1)\n",
    "        self.actor  = nn.Linear(self.n_hidden_fc,self.linear_out)\n",
    "        \n",
    "        self.optimizer = optim.Adam(params = self.parameters() ,lr = lr)\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc(x)).to(self.device)\n",
    "        out = F.relu(self.fc1(out)).to(self.device)\n",
    "        out = F.relu(self.fc2(out)).to(self.device)\n",
    "        \n",
    "        critic_ = self.critic(out) #value function approximator\n",
    "        actor_  = self.actor(out)  #deterministic policy approximator \n",
    "        return actor_,critic_   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a2dafd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, lr, input_dims, actions, gamma=0.99):\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.input_dims = input_dims\n",
    "        self.actions = actions\n",
    "        self.actor_critic_network = linear_layer(self.lr,self.input_dims,self.actions)\n",
    "        self.log_prob_action = None #TD(0)\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        state = state.flatten().to(self.actor_critic_network.device)\n",
    "        actor_,critic_ = self.actor_critic_network(state)\n",
    "        probabilities = F.softmax(actor_, dim = 0)#makes them all add up to 1\n",
    "        cat_distr = torch.distributions.Categorical(probabilities)#reshapes distribution such that one action is more\n",
    "        #likely to be picked than another\n",
    "        action_taken = cat_distr.sample()#select a sample action based on custom distribution made by network and torch\n",
    "        self.log_prob_action = cat_distr.log_prob(action_taken)#log prob of action from distribution\n",
    "        return action_taken.item()\n",
    "    \n",
    "    def learn(self, state, reward, next_state):\n",
    "        self.actor_critic_network.optimizer.zero_grad()#ensures gradients arent carried to next timestep\n",
    "        reward = torch.tensor(reward).to(self.actor_critic_network.device)\n",
    "        state = state.flatten()\n",
    "        next_state = next_state.flatten()\n",
    "        \n",
    "        _,current_state_critic = self.actor_critic_network(state)\n",
    "        _,next_state_critic = self.actor_critic_network(next_state)\n",
    "        \n",
    "        G_t = (reward + self.gamma*next_state_critic) - current_state_critic\n",
    "        \n",
    "        actor_loss = G_t*-self.log_prob_action\n",
    "        critic_loss = G_t**2.0\n",
    "        \n",
    "        (critic_loss + actor_loss).backward()\n",
    "        \n",
    "        self.actor_critic_network.optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "812aade4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_running_curve(scores, x, figure_file):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range (len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i - 100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title('Running average of previous 100 scores')\n",
    "    plt.savefig(figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a58e9619",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvManager():\n",
    "    def __init__(self, device,environment):\n",
    "        self.device = device\n",
    "        #self.env = gym.make(environment).unwrapped\n",
    "        self.env = gym.make(environment)\n",
    "        self.env.reset()\n",
    "        self.current_screen = None\n",
    "        self.done = False\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        self.current_screen = None\n",
    "        \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode)\n",
    "        \n",
    "    def num_actions_available(self):\n",
    "        return self.env.action_space\n",
    "        \n",
    "    def take_action(self, action):   \n",
    "        _, reward, self.done, _ = self.env.step([action])\n",
    "        return reward\n",
    "    \n",
    "    def just_starting(self):\n",
    "        return self.current_screen is None\n",
    "    \n",
    "    def get_state(self):\n",
    "        if self.just_starting() or self.done:\n",
    "            self.current_screen = self.get_processed_screen()\n",
    "            black_screen = torch.zeros_like(self.current_screen)\n",
    "            return black_screen\n",
    "        else:\n",
    "            s1 = self.current_screen\n",
    "            s2 = self.get_processed_screen()\n",
    "            self.current_screen = s2\n",
    "            return s2 - s1\n",
    "    \n",
    "    def get_screen_height(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[2]\n",
    "    \n",
    "    def get_screen_width(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[3]\n",
    "       \n",
    "    def get_processed_screen(self):\n",
    "        screen = em.render('rgb_array')\n",
    "        rgb_weights = [0.2989, 0.5870, 0.1140]\n",
    "        grayscale_image = np.dot(screen[...,:3], rgb_weights) \n",
    "        screen = grayscale_image.transpose((0, 1)) # PyTorch expects CHW\n",
    "        #print(type(screen)) # numpy\n",
    "        screen = self.crop_screen(screen)\n",
    "        return self.transform_screen_data(screen)\n",
    "    \n",
    "    def crop_screen(self, screen):\n",
    "        screen_height = screen.shape[0]\n",
    "        screen_width  = screen.shape[1]\n",
    "        #print('screen height(top/bottom): ',screen_height)\n",
    "        #print('screen height(left/right): ',screen_width)\n",
    "        # Strip off top and bottom\n",
    "        top = int(screen_height * 0.2)\n",
    "        #print('top: ',top)\n",
    "        bottom = int(screen_height * 0.8)\n",
    "        #print('bottom: ',bottom)\n",
    "        \n",
    "        \n",
    "        #strip off left/right\n",
    "        left  = int(screen_width * 0.2)\n",
    "        #print('left: ',left)\n",
    "        right = int(screen_width * 0.8)\n",
    "        #print('right: ',right)\n",
    "        \n",
    "        screen = screen[top:bottom, left:right]\n",
    "        return screen\n",
    "    \n",
    "    \n",
    "    def transform_screen_data(self, screen):       \n",
    "        # Convert to float, rescale, convert to tensor\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "        \n",
    "        # Use torchvision package to compose image transforms\n",
    "        resize = T.Compose([\n",
    "            T.ToPILImage()\n",
    "            ,T.Resize((40,90))\n",
    "            ,T.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        return resize(screen).unsqueeze(0).to(self.device) # add a batch dimension (BCHW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11e8d718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_action_bins(bin_size,min_val,max_val):\n",
    "    action_bin_array = np.linspace(min_val, max_val, num=bin_size)\n",
    "    return action_bin_array\n",
    "\n",
    "action_bin_array = set_action_bins(100,-2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc19e949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2c68f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "em = EnvManager(device,'Pendulum-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719201dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  0 score-880.51  avg_scr -880.51\n",
      "episode:  1 score-1152.16  avg_scr -1016.33\n",
      "episode:  2 score-1524.04  avg_scr -1185.57\n",
      "episode:  3 score-903.53  avg_scr -1115.06\n",
      "episode:  4 score-1676.47  avg_scr -1227.34\n",
      "episode:  5 score-1840.08  avg_scr -1329.46\n",
      "episode:  6 score-1852.89  avg_scr -1404.24\n",
      "episode:  7 score-1414.17  avg_scr -1405.48\n",
      "episode:  8 score-1299.71  avg_scr -1393.73\n",
      "episode:  9 score-885.93  avg_scr -1342.95\n",
      "episode:  10 score-1172.69  avg_scr -1327.47\n",
      "episode:  11 score-1730.95  avg_scr -1361.09\n",
      "episode:  12 score-969.97  avg_scr -1331.01\n",
      "episode:  13 score-1198.52  avg_scr -1321.54\n",
      "episode:  14 score-968.58  avg_scr -1298.01\n",
      "episode:  15 score-1164.88  avg_scr -1289.69\n",
      "episode:  16 score-1532.03  avg_scr -1303.95\n",
      "episode:  17 score-968.98  avg_scr -1285.34\n",
      "episode:  18 score-1586.03  avg_scr -1301.16\n",
      "episode:  19 score-1070.15  avg_scr -1289.61\n",
      "episode:  20 score-1389.70  avg_scr -1294.38\n",
      "episode:  21 score-1687.41  avg_scr -1312.24\n",
      "episode:  22 score-867.72  avg_scr -1292.92\n",
      "episode:  23 score-975.03  avg_scr -1279.67\n",
      "episode:  24 score-1617.04  avg_scr -1293.17\n",
      "episode:  25 score-979.91  avg_scr -1281.12\n",
      "episode:  26 score-963.57  avg_scr -1269.36\n",
      "episode:  27 score-1330.16  avg_scr -1271.53\n",
      "episode:  28 score-1397.66  avg_scr -1275.88\n",
      "episode:  29 score-1435.12  avg_scr -1281.19\n",
      "episode:  30 score-1020.37  avg_scr -1272.77\n",
      "episode:  31 score-1234.86  avg_scr -1271.59\n",
      "episode:  32 score-1745.49  avg_scr -1285.95\n",
      "episode:  33 score-884.25  avg_scr -1274.13\n",
      "episode:  34 score-1552.36  avg_scr -1282.08\n",
      "episode:  35 score-1424.98  avg_scr -1286.05\n",
      "episode:  36 score-1632.43  avg_scr -1295.41\n",
      "episode:  37 score-1437.66  avg_scr -1299.16\n",
      "episode:  38 score-1063.07  avg_scr -1293.10\n",
      "episode:  39 score-1372.71  avg_scr -1295.09\n",
      "episode:  40 score-1165.59  avg_scr -1291.94\n",
      "episode:  41 score-905.11  avg_scr -1282.73\n",
      "episode:  42 score-862.41  avg_scr -1272.95\n",
      "episode:  43 score-908.92  avg_scr -1264.68\n",
      "episode:  44 score-989.10  avg_scr -1258.55\n",
      "episode:  45 score-751.09  avg_scr -1247.52\n",
      "episode:  46 score-1008.09  avg_scr -1242.43\n",
      "episode:  47 score-1475.42  avg_scr -1247.28\n",
      "episode:  48 score-990.49  avg_scr -1242.04\n",
      "episode:  49 score-1719.82  avg_scr -1251.60\n",
      "episode:  50 score-1098.76  avg_scr -1248.60\n",
      "episode:  51 score-1158.77  avg_scr -1246.87\n",
      "episode:  52 score-1581.65  avg_scr -1253.19\n",
      "episode:  53 score-1724.26  avg_scr -1261.91\n",
      "episode:  54 score-768.64  avg_scr -1252.94\n",
      "episode:  55 score-862.53  avg_scr -1245.97\n",
      "episode:  56 score-1058.03  avg_scr -1242.67\n",
      "episode:  57 score-1251.74  avg_scr -1242.83\n",
      "episode:  58 score-1747.93  avg_scr -1251.39\n",
      "episode:  59 score-983.49  avg_scr -1246.93\n",
      "episode:  60 score-1305.46  avg_scr -1247.89\n",
      "episode:  61 score-879.96  avg_scr -1241.95\n",
      "episode:  62 score-965.04  avg_scr -1237.56\n",
      "episode:  63 score-892.64  avg_scr -1232.17\n",
      "episode:  64 score-901.07  avg_scr -1227.07\n",
      "episode:  65 score-1591.46  avg_scr -1232.59\n",
      "episode:  66 score-824.87  avg_scr -1226.51\n",
      "episode:  67 score-1312.20  avg_scr -1227.77\n",
      "episode:  68 score-1277.17  avg_scr -1228.49\n",
      "episode:  69 score-1619.60  avg_scr -1234.07\n",
      "episode:  70 score-1784.23  avg_scr -1241.82\n",
      "episode:  71 score-1642.06  avg_scr -1247.38\n",
      "episode:  72 score-1151.80  avg_scr -1246.07\n",
      "episode:  73 score-1733.30  avg_scr -1252.66\n",
      "episode:  74 score-1472.82  avg_scr -1255.59\n",
      "episode:  75 score-1811.74  avg_scr -1262.91\n",
      "episode:  76 score-1775.25  avg_scr -1269.56\n",
      "episode:  77 score-1347.12  avg_scr -1270.56\n",
      "episode:  78 score-872.37  avg_scr -1265.52\n",
      "episode:  79 score-1334.16  avg_scr -1266.37\n",
      "episode:  80 score-947.78  avg_scr -1262.44\n",
      "episode:  81 score-1376.06  avg_scr -1263.83\n",
      "episode:  82 score-1434.91  avg_scr -1265.89\n",
      "episode:  83 score-1456.84  avg_scr -1268.16\n",
      "episode:  84 score-1265.40  avg_scr -1268.13\n",
      "episode:  85 score-1346.77  avg_scr -1269.04\n",
      "episode:  86 score-1609.24  avg_scr -1272.95\n",
      "episode:  87 score-1483.09  avg_scr -1275.34\n",
      "episode:  88 score-1645.21  avg_scr -1279.50\n",
      "episode:  89 score-1072.77  avg_scr -1277.20\n",
      "episode:  90 score-1274.33  avg_scr -1277.17\n",
      "episode:  91 score-1304.59  avg_scr -1277.47\n",
      "episode:  92 score-1266.64  avg_scr -1277.35\n",
      "episode:  93 score-1136.23  avg_scr -1275.85\n",
      "episode:  94 score-1702.18  avg_scr -1280.34\n",
      "episode:  95 score-927.85  avg_scr -1276.66\n",
      "episode:  96 score-1252.12  avg_scr -1276.41\n",
      "episode:  97 score-975.81  avg_scr -1273.34\n",
      "episode:  98 score-1191.96  avg_scr -1272.52\n",
      "episode:  99 score-1051.12  avg_scr -1270.31\n",
      "episode:  100 score-943.52  avg_scr -1270.94\n",
      "episode:  101 score-1339.64  avg_scr -1272.81\n",
      "episode:  102 score-1728.17  avg_scr -1274.85\n",
      "episode:  103 score-1430.95  avg_scr -1280.13\n",
      "episode:  104 score-1658.09  avg_scr -1279.94\n",
      "episode:  105 score-1680.36  avg_scr -1278.35\n",
      "episode:  106 score-883.44  avg_scr -1268.65\n",
      "episode:  107 score-1513.21  avg_scr -1269.64\n",
      "episode:  108 score-1611.77  avg_scr -1272.76\n",
      "episode:  109 score-1174.04  avg_scr -1275.65\n",
      "episode:  110 score-1682.21  avg_scr -1280.74\n",
      "episode:  111 score-1486.64  avg_scr -1278.30\n",
      "episode:  112 score-1065.55  avg_scr -1279.25\n",
      "episode:  113 score-1699.06  avg_scr -1284.26\n",
      "episode:  114 score-1457.41  avg_scr -1289.15\n",
      "episode:  115 score-1565.49  avg_scr -1293.15\n",
      "episode:  116 score-1614.76  avg_scr -1293.98\n",
      "episode:  117 score-996.17  avg_scr -1294.25\n",
      "episode:  118 score-1695.44  avg_scr -1295.35\n",
      "episode:  119 score-911.59  avg_scr -1293.76\n",
      "episode:  120 score-892.60  avg_scr -1288.79\n",
      "episode:  121 score-1050.10  avg_scr -1282.42\n",
      "episode:  122 score-1185.71  avg_scr -1285.60\n",
      "episode:  123 score-865.52  avg_scr -1284.50\n",
      "episode:  124 score-983.00  avg_scr -1278.16\n",
      "episode:  125 score-836.66  avg_scr -1276.73\n",
      "episode:  126 score-1782.98  avg_scr -1284.92\n",
      "episode:  127 score-1389.48  avg_scr -1285.52\n",
      "episode:  128 score-975.95  avg_scr -1281.30\n",
      "episode:  129 score-974.70  avg_scr -1276.69\n",
      "episode:  130 score-966.04  avg_scr -1276.15\n",
      "episode:  131 score-810.77  avg_scr -1271.91\n",
      "episode:  132 score-967.12  avg_scr -1264.13\n",
      "episode:  133 score-868.49  avg_scr -1263.97\n",
      "episode:  134 score-1068.93  avg_scr -1259.13\n",
      "episode:  135 score-1071.34  avg_scr -1255.60\n",
      "episode:  136 score-1127.20  avg_scr -1250.55\n",
      "episode:  137 score-989.33  avg_scr -1246.06\n",
      "episode:  138 score-1288.91  avg_scr -1248.32\n",
      "episode:  139 score-1546.99  avg_scr -1250.06\n",
      "episode:  140 score-890.19  avg_scr -1247.31\n",
      "episode:  141 score-1070.43  avg_scr -1248.96\n",
      "episode:  142 score-1472.85  avg_scr -1255.07\n",
      "episode:  143 score-1479.42  avg_scr -1260.77\n",
      "episode:  144 score-857.61  avg_scr -1259.46\n",
      "episode:  145 score-808.84  avg_scr -1260.04\n",
      "episode:  146 score-1625.99  avg_scr -1266.21\n",
      "episode:  147 score-1596.76  avg_scr -1267.43\n",
      "episode:  148 score-1308.22  avg_scr -1270.60\n",
      "episode:  149 score-1166.48  avg_scr -1265.07\n",
      "episode:  150 score-1850.63  avg_scr -1272.59\n",
      "episode:  151 score-1308.95  avg_scr -1274.09\n",
      "episode:  152 score-1202.41  avg_scr -1270.30\n",
      "episode:  153 score-1739.18  avg_scr -1270.45\n",
      "episode:  154 score-1067.56  avg_scr -1273.44\n",
      "episode:  155 score-1699.51  avg_scr -1281.81\n",
      "episode:  156 score-1163.82  avg_scr -1282.87\n",
      "episode:  157 score-1029.07  avg_scr -1280.64\n",
      "episode:  158 score-863.33  avg_scr -1271.79\n",
      "episode:  159 score-1290.13  avg_scr -1274.86\n",
      "episode:  160 score-784.97  avg_scr -1269.65\n",
      "episode:  161 score-1423.84  avg_scr -1275.09\n",
      "episode:  162 score-1413.69  avg_scr -1279.58\n",
      "episode:  163 score-1086.73  avg_scr -1281.52\n",
      "episode:  164 score-1137.52  avg_scr -1283.89\n",
      "episode:  165 score-1714.94  avg_scr -1285.12\n",
      "episode:  166 score-1177.22  avg_scr -1288.64\n",
      "episode:  167 score-1282.90  avg_scr -1288.35\n",
      "episode:  168 score-1118.49  avg_scr -1286.76\n",
      "episode:  169 score-973.25  avg_scr -1280.30\n",
      "episode:  170 score-1087.85  avg_scr -1273.34\n",
      "episode:  171 score-886.00  avg_scr -1265.78\n",
      "episode:  172 score-1355.08  avg_scr -1267.81\n",
      "episode:  173 score-1889.90  avg_scr -1269.37\n",
      "episode:  174 score-1070.23  avg_scr -1265.35\n",
      "episode:  175 score-1168.56  avg_scr -1258.92\n",
      "episode:  176 score-1418.52  avg_scr -1255.35\n",
      "episode:  177 score-1169.45  avg_scr -1253.57\n",
      "episode:  178 score-1379.76  avg_scr -1258.65\n",
      "episode:  179 score-1078.11  avg_scr -1256.09\n",
      "episode:  180 score-889.35  avg_scr -1255.50\n",
      "episode:  181 score-1455.63  avg_scr -1256.30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  182 score-1435.82  avg_scr -1256.31\n",
      "episode:  183 score-1636.78  avg_scr -1258.11\n",
      "episode:  184 score-1203.57  avg_scr -1257.49\n",
      "episode:  185 score-907.98  avg_scr -1253.10\n",
      "episode:  186 score-1751.01  avg_scr -1254.52\n",
      "episode:  187 score-1449.12  avg_scr -1254.18\n",
      "episode:  188 score-1079.78  avg_scr -1248.52\n",
      "episode:  189 score-1611.60  avg_scr -1253.91\n",
      "episode:  190 score-891.10  avg_scr -1250.08\n",
      "episode:  191 score-865.47  avg_scr -1245.69\n",
      "episode:  192 score-892.74  avg_scr -1241.95\n",
      "episode:  193 score-1666.02  avg_scr -1247.25\n",
      "episode:  194 score-979.05  avg_scr -1240.02\n",
      "episode:  195 score-816.51  avg_scr -1238.90\n",
      "episode:  196 score-972.52  avg_scr -1236.11\n",
      "episode:  197 score-1359.69  avg_scr -1239.95\n",
      "episode:  198 score-1426.76  avg_scr -1242.29\n",
      "episode:  199 score-1450.81  avg_scr -1246.29\n",
      "episode:  200 score-746.00  avg_scr -1244.32\n",
      "episode:  201 score-1015.84  avg_scr -1241.08\n",
      "episode:  202 score-977.91  avg_scr -1233.57\n",
      "episode:  203 score-1332.23  avg_scr -1232.59\n",
      "episode:  204 score-1734.83  avg_scr -1233.35\n",
      "episode:  205 score-1188.67  avg_scr -1228.44\n",
      "episode:  206 score-1079.30  avg_scr -1230.40\n",
      "episode:  207 score-1411.40  avg_scr -1229.38\n",
      "episode:  208 score-982.11  avg_scr -1223.08\n",
      "episode:  209 score-1166.81  avg_scr -1223.01\n",
      "episode:  210 score-1300.30  avg_scr -1219.19\n",
      "episode:  211 score-1547.57  avg_scr -1219.80\n",
      "episode:  212 score-1518.23  avg_scr -1224.33\n",
      "episode:  213 score-866.62  avg_scr -1216.00\n",
      "episode:  214 score-766.81  avg_scr -1209.10\n",
      "episode:  215 score-974.04  avg_scr -1203.18\n",
      "episode:  216 score-1295.55  avg_scr -1199.99\n",
      "episode:  217 score-1654.36  avg_scr -1206.57\n",
      "episode:  218 score-983.60  avg_scr -1199.45\n",
      "episode:  219 score-1497.60  avg_scr -1205.31\n",
      "episode:  220 score-1544.41  avg_scr -1211.83\n",
      "episode:  221 score-1288.07  avg_scr -1214.21\n",
      "episode:  222 score-1584.27  avg_scr -1218.20\n",
      "episode:  223 score-1397.11  avg_scr -1223.51\n",
      "episode:  224 score-751.69  avg_scr -1221.20\n",
      "episode:  225 score-972.70  avg_scr -1222.56\n",
      "episode:  226 score-882.03  avg_scr -1213.55\n",
      "episode:  227 score-1461.64  avg_scr -1214.27\n",
      "episode:  228 score-1178.07  avg_scr -1216.29\n",
      "episode:  229 score-966.37  avg_scr -1216.21\n",
      "episode:  230 score-888.20  avg_scr -1215.43\n",
      "episode:  231 score-1751.41  avg_scr -1224.84\n",
      "episode:  232 score-1574.84  avg_scr -1230.92\n",
      "episode:  233 score-1167.73  avg_scr -1233.91\n",
      "episode:  234 score-1342.38  avg_scr -1236.64\n",
      "episode:  235 score-961.88  avg_scr -1235.55\n",
      "episode:  236 score-1343.73  avg_scr -1237.71\n",
      "episode:  237 score-1069.46  avg_scr -1238.51\n"
     ]
    }
   ],
   "source": [
    "output_num = len(action_bin_array)\n",
    "agent = Agent(5e-6,em.get_screen_width()*em.get_screen_height(),output_num)\n",
    "scores = []\n",
    "for episode in range(2000):\n",
    "    em.reset()\n",
    "    state = em.get_state()\n",
    "    score = 0\n",
    "    for timestep in count():\n",
    "        action_idx = agent.select_action(state)\n",
    "        reward     = em.take_action([action_bin_array[action_idx]])\n",
    "        score+=reward\n",
    "        next_state = em.get_state()\n",
    "        agent.learn(state, reward, next_state)\n",
    "        state = next_state\n",
    "        if em.done:\n",
    "            scores.append(score)\n",
    "            avg_score = np.mean(scores[-100:])\n",
    "            print('episode: ', episode, 'score%.2f '% score, 'avg_scr %.2f'%avg_score)\n",
    "            break\n",
    "x = [episode + 1 for i in range(len(scores))]\n",
    "fname = 'graph'\n",
    "save = fname + '.png'\n",
    "plot_learning_curve(scores,x,save)\n",
    "em.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52decc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "em.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b16d781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
