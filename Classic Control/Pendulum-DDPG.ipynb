{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba256f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from random import seed\n",
    "from sklearn import preprocessing\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9efa463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvManager():\n",
    "    def __init__(self, device,environment):\n",
    "        self.device = device\n",
    "        #self.env = gym.make(environment).unwrapped\n",
    "        self.env = gym.make(environment)\n",
    "        self.env.reset()\n",
    "        self.current_screen = None\n",
    "        self.done = False\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        self.current_screen = None\n",
    "        \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode)\n",
    "        \n",
    "    def num_actions_available(self):\n",
    "        return self.env.action_space\n",
    "        \n",
    "    def take_action(self, action):   \n",
    "        _, reward, self.done, _ = self.env.step([action])\n",
    "        return reward\n",
    "    \n",
    "    def just_starting(self):\n",
    "        return self.current_screen is None\n",
    "    \n",
    "    def get_state(self):\n",
    "        if self.just_starting() or self.done:\n",
    "            self.current_screen = self.get_processed_screen()\n",
    "            black_screen = torch.zeros_like(self.current_screen)\n",
    "            return black_screen\n",
    "        else:\n",
    "            s1 = self.current_screen\n",
    "            s2 = self.get_processed_screen()\n",
    "            self.current_screen = s2\n",
    "            return s2 - s1\n",
    "    \n",
    "    def get_screen_height(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[2]\n",
    "    \n",
    "    def get_screen_width(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[3]\n",
    "       \n",
    "    def get_processed_screen(self):\n",
    "        screen = em.render('rgb_array')\n",
    "        rgb_weights = [0.2989, 0.5870, 0.1140]\n",
    "        grayscale_image = np.dot(screen[...,:3], rgb_weights) \n",
    "        screen = grayscale_image.transpose((0, 1)) # PyTorch expects CHW\n",
    "        #print(type(screen)) # numpy\n",
    "        screen = self.crop_screen(screen)\n",
    "        return self.transform_screen_data(screen)\n",
    "    \n",
    "    def crop_screen(self, screen):\n",
    "        screen_height = screen.shape[0]\n",
    "        screen_width  = screen.shape[1]\n",
    "        #print('screen height(top/bottom): ',screen_height)\n",
    "        #print('screen height(left/right): ',screen_width)\n",
    "        # Strip off top and bottom\n",
    "        top = int(screen_height * 0.2)\n",
    "        #print('top: ',top)\n",
    "        bottom = int(screen_height * 0.8)\n",
    "        #print('bottom: ',bottom)\n",
    "        \n",
    "        \n",
    "        #strip off left/right\n",
    "        left  = int(screen_width * 0.2)\n",
    "        #print('left: ',left)\n",
    "        right = int(screen_width * 0.8)\n",
    "        #print('right: ',right)\n",
    "        \n",
    "        screen = screen[top:bottom, left:right]\n",
    "        return screen\n",
    "    \n",
    "    \n",
    "    def transform_screen_data(self, screen):       \n",
    "        # Convert to float, rescale, convert to tensor\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "        \n",
    "        # Use torchvision package to compose image transforms\n",
    "        resize = T.Compose([\n",
    "            T.ToPILImage()\n",
    "            ,T.Resize((40,90))\n",
    "            ,T.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        return resize(screen).unsqueeze(0).to(self.device) # add a batch dimension (BCHW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d54542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "        \n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff9b69ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensors(experiences):\n",
    "    # Convert batch of Experiences to Experience of batches\n",
    "    batch = Experience(*zip(*experiences))\n",
    "\n",
    "    t1 = torch.stack(batch.state)\n",
    "    t2 = torch.stack(batch.action)\n",
    "    t3 = torch.stack(batch.reward)\n",
    "    t4 = torch.stack(batch.next_state)\n",
    "\n",
    "    return (t1,t2,t3,t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c74dee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    ('state', 'action', 'reward', 'next_state')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7801eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OU_noise():\n",
    "    def __init__(self,mu,sigma=0.15,theta=0.15,dt=1e-2,x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu    = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt    = dt\n",
    "        self.x0    = x0\n",
    "        self.reset()\n",
    "        \n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev)* \\\n",
    "                self.dt +self.sigma * np.sqrt(self.dt)*np.random.normal(size = self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "    \n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fad7838",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_network(nn.Module):\n",
    "    def __init__(self, lr, input_size, fc1, fc2, linear_out):\n",
    "        super(Actor_network, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.linear_out = linear_out\n",
    "        self.lr = lr\n",
    "        self.n_hidden_fc1 = fc1\n",
    "        self.n_hidden_fc2 = fc2\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.fc1  = nn.Linear(self.input_size,self.n_hidden_fc1)\n",
    "        self.fc2  = nn.Linear(self.n_hidden_fc1,self.n_hidden_fc2)\n",
    "        self.fc3  = nn.Linear(self.n_hidden_fc2,self.linear_out)\n",
    "        \n",
    "        self.ln1 =nn.LayerNorm(self.n_hidden_fc1)\n",
    "        self.ln2 =nn.LayerNorm(self.n_hidden_fc2)\n",
    "        \n",
    "        fc1 = 1.0/np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        self.fc1.weight.data.uniform_(-fc1,fc1)\n",
    "        self.fc1.bias.data.uniform_(-fc1,fc1)\n",
    "        \n",
    "        fc2 = 1.0/np.sqrt(self.fc2.weight.data.size()[0])\n",
    "        self.fc2.weight.data.uniform_(-fc2,fc2)\n",
    "        self.fc2.bias.data.uniform_(-fc2,fc2)\n",
    "        \n",
    "        fc3 = 0.003\n",
    "        self.fc3.weight.data.uniform_(-fc3,fc3)\n",
    "        self.fc3.bias.data.uniform_(-fc3,fc3)\n",
    "        \n",
    "        self.optimizer = optim.Adam(params = self.parameters() ,lr = lr)\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, states):\n",
    "        states = self.fc1(states)\n",
    "        states = self.ln1(states)\n",
    "        states = F.relu(states)\n",
    "        \n",
    "        states = self.fc2(states)\n",
    "        states = self.ln2(states)\n",
    "        states = F.relu(states)\n",
    "        \n",
    "        states = self.fc3(states)\n",
    "        out = torch.tanh(states)*2.0 #multiply by max/min env space, tanh because of -1 1 min maxing\n",
    "    \n",
    "        return out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1eceeb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic_network(nn.Module):\n",
    "    def __init__(self, lr, input_size, fc1, fc2, linear_out):\n",
    "        super(Critic_network, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.linear_out = linear_out\n",
    "        self.lr = lr\n",
    "        self.n_hidden_fc1 = fc1\n",
    "        self.n_hidden_fc2 = fc2\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.fc1   = nn.Linear(self.input_size,self.n_hidden_fc1)\n",
    "        self.fc2  = nn.Linear(self.n_hidden_fc1,self.n_hidden_fc2)\n",
    "        \n",
    "        self.action_values = nn.Linear(self.linear_out,self.n_hidden_fc2)\n",
    "        \n",
    "        self.state_q_value = nn.Linear(self.n_hidden_fc2, 1)\n",
    "        \n",
    "        #normalize data cause we are sampling from so many diff environments\n",
    "        self.ln1 = nn.LayerNorm(self.n_hidden_fc1) \n",
    "        self.ln2 = nn.LayerNorm(self.n_hidden_fc2)\n",
    "        \n",
    "        #initialize weights and biases according to number of neurons per laer involved\n",
    "        fc1 = 1.0/np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        self.fc1.weight.data.uniform_(-fc1,fc1)\n",
    "        self.fc1.bias.data.uniform_(-fc1,fc1)\n",
    "        \n",
    "        fc2 = 1.0/np.sqrt(self.fc2.weight.data.size()[0])\n",
    "        self.fc2.weight.data.uniform_(-fc2,fc2)\n",
    "        self.fc2.bias.data.uniform_(-fc2,fc2)\n",
    "        \n",
    "        fc3 = 1.0/np.sqrt(self.action_values.weight.data.size()[0])\n",
    "        self.action_values.weight.data.uniform_(-fc3,fc3)\n",
    "        self.action_values.bias.data.uniform_(-fc3,fc3)\n",
    "        \n",
    "        #final output layer initialize by 0.003 as desired by paper\n",
    "        self.state_q_value.weight.data.uniform_(-0.003,0.003)\n",
    "        self.state_q_value.bias.data.uniform_(-0.003,0.003)\n",
    "        \n",
    "        self.optimizer = optim.Adam(params = self.parameters() ,lr = lr, weight_decay = 0.01)#weight decay?\n",
    "        self.to(self.device)\n",
    "        \n",
    "        \n",
    "    def forward(self, states, actions):\n",
    "        states = self.fc1(states).to(self.device)\n",
    "        states = self.ln1(states)\n",
    "        states = F.relu(states)\n",
    "        \n",
    "        states = self.fc2(states)\n",
    "        states = self.ln2(states)\n",
    "        #states = F.relu(states)\n",
    "        \n",
    "        action_v = self.action_values(actions)#action nn has same output size as states\n",
    "        state_action_add = F.relu(torch.add(action_v,states))\n",
    "        \n",
    "        final_q = self.state_q_value(state_action_add)\n",
    "         \n",
    "        return final_q  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef8bcaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,alpha,beta,input_dim,tau,n_actions,max_replay_mem,fc1_,fc2_,batch_size = 250,gamma = 0.99):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        \n",
    "        self.memory = ReplayMemory(max_replay_mem)\n",
    "        \n",
    "        self.noise = OU_noise(mu = np.zeros(n_actions))\n",
    "        \n",
    "        self.actor = Actor_network(alpha,input_dim,fc1_,fc2_,linear_out=n_actions)\n",
    "        \n",
    "        self.t_actor= Actor_network(alpha,input_dim,fc1_,fc2_,linear_out=n_actions)\n",
    "        \n",
    "        self.critic= Critic_network(beta,input_dim,fc1_,fc2_,linear_out=n_actions)\n",
    "        \n",
    "        self.t_critic=Critic_network(beta,input_dim,fc1_,fc2_,linear_out=n_actions)\n",
    "        \n",
    "        self.update_network_parameters(tau=1)\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        self.actor.eval()\n",
    "        state = state.flatten()\n",
    "        mu = self.actor(state).to(self.actor.device)\n",
    "        mu_noise = mu + torch.tensor(self.noise(), dtype=torch.float).to(self.actor.device)\n",
    "        self.actor.train()\n",
    "        return mu_noise.cpu().detach().numpy()[0]\n",
    "    \n",
    "    def store_memory(self, state, action, reward, next_state):\n",
    "        state = state.flatten()\n",
    "        next_state = next_state.flatten()\n",
    "        self.memory.push(Experience(state,action,reward,next_state))\n",
    "        \n",
    "    def learn(self):\n",
    "        if self.memory.can_provide_sample(self.batch_size):\n",
    "            states,actions,rewards,next_states   = extract_tensors(self.memory.sample(self.batch_size))\n",
    "            states = torch.tensor(states, dtype  = torch.float).to(self.actor.device)\n",
    "            actions = torch.tensor(actions, dtype= torch.float).to(self.actor.device)\n",
    "            rewards = torch.tensor(rewards, dtype= torch.float).to(self.actor.device)\n",
    "            next_states = torch.tensor(next_states, dtype=torch.float).to(self.actor.device)\n",
    "            \n",
    "            target_actions_network = self.t_actor(next_states)\n",
    "            target_critic_network = self.t_critic(next_states,target_actions_network)\n",
    "            #print('states crit: ', states.shape,'actions: ', actions.shape)\n",
    "            critic_network = self.critic(states,actions)\n",
    "            #print('critic: ',critic_network.shape)\n",
    "            #print('critic_t: ',target_critic_network.shape)\n",
    "            target_critic_network = target_critic_network.view(-1)\n",
    "            rewards = rewards.view(-1)\n",
    "            #print('critic_t_: ',target_critic_network.shape)\n",
    "            #print('rewards: ',rewards.shape)\n",
    "            target = rewards+self.gamma*target_critic_network\n",
    "            \n",
    "            #print('target: ', target.shape)\n",
    "            target = target.view(self.batch_size,1)\n",
    "            #print('target prime: ', target.shape)\n",
    "            self.critic.optimizer.zero_grad()\n",
    "            #target and critic must be same size\n",
    "            #print('critic: ',critic_network.shape)\n",
    "            critic_loss = F.mse_loss(target,critic_network)\n",
    "            critic_loss.backward()\n",
    "            self.critic.optimizer.step()\n",
    "            \n",
    "            #actor loss\n",
    "            self.actor.optimizer.zero_grad()\n",
    "            actor_loss = -self.critic(states,self.actor(states))\n",
    "            actor_loss = torch.mean(actor_loss)\n",
    "            actor_loss.backward()\n",
    "            self.actor.optimizer.step()\n",
    "            \n",
    "            self.update_network_parameters()\n",
    "            \n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "        \n",
    "        actor_params = self.actor.named_parameters()\n",
    "        critic_params = self.critic.named_parameters()\n",
    "        target_actor_params = self.t_actor.named_parameters()\n",
    "        target_critic_params = self.t_critic.named_parameters()\n",
    "        \n",
    "        critic_dict = dict(critic_params)\n",
    "        actor_dict = dict(actor_params)\n",
    "        target_critic_dict = dict(target_critic_params)\n",
    "        target_actor_dict = dict(target_actor_params)\n",
    "        \n",
    "        for weight in critic_dict:\n",
    "            critic_dict[weight] = tau*critic_dict[weight].clone() + (1-tau)*\\\n",
    "                target_critic_dict[weight].clone()\n",
    "        \n",
    "        for weight in actor_dict:\n",
    "            actor_dict[weight] = tau*actor_dict[weight].clone() + (1-tau)*\\\n",
    "                target_actor_dict[weight].clone()\n",
    "        \n",
    "        self.t_critic.load_state_dict(critic_dict)\n",
    "        self.t_actor.load_state_dict(actor_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34931b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 250\n",
    "gamma = 0.7\n",
    "eps_start = 1\n",
    "eps_end = 0.001\n",
    "eps_decay = 0.0001\n",
    "target_update = 10\n",
    "memory_size = 10000\n",
    "lr = 0.001\n",
    "num_episodes = 10000#for more episodes for better results\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81e1996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 40\n",
      "episode:  0 score-1494.03  avg_scr -1494.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-37-7b433e5aede2>:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  states = torch.tensor(states, dtype  = torch.float).to(self.actor.device)\n",
      "<ipython-input-37-7b433e5aede2>:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  actions = torch.tensor(actions, dtype= torch.float).to(self.actor.device)\n",
      "<ipython-input-37-7b433e5aede2>:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards = torch.tensor(rewards, dtype= torch.float).to(self.actor.device)\n",
      "<ipython-input-37-7b433e5aede2>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  next_states = torch.tensor(next_states, dtype=torch.float).to(self.actor.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  1 score-1608.67  avg_scr -1551.35\n",
      "episode:  2 score-1515.85  avg_scr -1539.52\n",
      "episode:  3 score-1625.27  avg_scr -1560.95\n",
      "episode:  4 score-1654.32  avg_scr -1579.63\n",
      "episode:  5 score-1540.66  avg_scr -1573.13\n",
      "episode:  6 score-1180.43  avg_scr -1517.03\n",
      "episode:  7 score-1338.10  avg_scr -1494.67\n",
      "episode:  8 score-974.88  avg_scr -1436.91\n",
      "episode:  9 score-1374.39  avg_scr -1430.66\n",
      "episode:  10 score-1340.82  avg_scr -1422.49\n",
      "episode:  11 score-1296.65  avg_scr -1412.01\n",
      "episode:  12 score-1488.12  avg_scr -1417.86\n",
      "episode:  13 score-1385.02  avg_scr -1415.51\n",
      "episode:  14 score-1619.29  avg_scr -1429.10\n",
      "episode:  15 score-1281.63  avg_scr -1419.88\n",
      "episode:  16 score-1379.19  avg_scr -1417.49\n",
      "episode:  17 score-994.02  avg_scr -1393.96\n",
      "episode:  18 score-1595.69  avg_scr -1404.58\n",
      "episode:  19 score-1437.53  avg_scr -1406.23\n",
      "episode:  20 score-1301.30  avg_scr -1401.23\n",
      "episode:  21 score-1437.72  avg_scr -1402.89\n",
      "episode:  22 score-1373.89  avg_scr -1401.63\n",
      "episode:  23 score-1315.35  avg_scr -1398.03\n",
      "episode:  24 score-1347.96  avg_scr -1396.03\n",
      "episode:  25 score-1162.18  avg_scr -1387.04\n",
      "episode:  26 score-1217.53  avg_scr -1380.76\n",
      "episode:  27 score-1073.89  avg_scr -1369.80\n",
      "episode:  28 score-1039.36  avg_scr -1358.40\n",
      "episode:  29 score-1196.29  avg_scr -1353.00\n",
      "episode:  30 score-1312.42  avg_scr -1351.69\n",
      "episode:  31 score-1214.08  avg_scr -1347.39\n",
      "episode:  32 score-1127.03  avg_scr -1340.71\n",
      "episode:  33 score-1444.52  avg_scr -1343.77\n",
      "episode:  34 score-1271.63  avg_scr -1341.71\n",
      "episode:  35 score-1119.90  avg_scr -1335.54\n",
      "episode:  36 score-1214.36  avg_scr -1332.27\n",
      "episode:  37 score-962.60  avg_scr -1322.54\n",
      "episode:  38 score-1363.08  avg_scr -1323.58\n",
      "episode:  39 score-1173.16  avg_scr -1319.82\n",
      "episode:  40 score-1496.92  avg_scr -1324.14\n",
      "episode:  41 score-1506.88  avg_scr -1328.49\n",
      "episode:  42 score-1241.95  avg_scr -1326.48\n",
      "episode:  43 score-1134.80  avg_scr -1322.12\n",
      "episode:  44 score-1142.28  avg_scr -1318.13\n",
      "episode:  45 score-1294.85  avg_scr -1317.62\n",
      "episode:  46 score-1085.13  avg_scr -1312.67\n",
      "episode:  47 score-1322.80  avg_scr -1312.88\n",
      "episode:  48 score-1088.55  avg_scr -1308.31\n",
      "episode:  49 score-1027.63  avg_scr -1302.69\n",
      "episode:  50 score-1191.68  avg_scr -1300.52\n",
      "episode:  51 score-1108.43  avg_scr -1296.82\n",
      "episode:  52 score-1052.83  avg_scr -1292.22\n",
      "episode:  53 score-1178.29  avg_scr -1290.11\n",
      "episode:  54 score-1041.29  avg_scr -1285.58\n",
      "episode:  55 score-986.51  avg_scr -1280.24\n",
      "episode:  56 score-1483.74  avg_scr -1283.81\n",
      "episode:  57 score-856.58  avg_scr -1276.45\n",
      "episode:  58 score-1090.25  avg_scr -1273.29\n",
      "episode:  59 score-1298.90  avg_scr -1273.72\n",
      "episode:  60 score-1187.12  avg_scr -1272.30\n",
      "episode:  61 score-1424.71  avg_scr -1274.76\n",
      "episode:  62 score-1237.13  avg_scr -1274.16\n",
      "episode:  63 score-893.56  avg_scr -1268.21\n",
      "episode:  64 score-975.41  avg_scr -1263.71\n",
      "episode:  65 score-875.83  avg_scr -1257.83\n",
      "episode:  66 score-1304.40  avg_scr -1258.53\n",
      "episode:  67 score-869.80  avg_scr -1252.81\n",
      "episode:  68 score-750.09  avg_scr -1245.52\n",
      "episode:  69 score-885.75  avg_scr -1240.38\n",
      "episode:  70 score-1222.99  avg_scr -1240.14\n",
      "episode:  71 score-1279.77  avg_scr -1240.69\n",
      "episode:  72 score-756.94  avg_scr -1234.06\n",
      "episode:  73 score-1098.17  avg_scr -1232.23\n",
      "episode:  74 score-1306.27  avg_scr -1233.21\n",
      "episode:  75 score-755.23  avg_scr -1226.92\n",
      "episode:  76 score-635.50  avg_scr -1219.24\n",
      "episode:  77 score-1076.08  avg_scr -1217.41\n",
      "episode:  78 score-635.68  avg_scr -1210.04\n",
      "episode:  79 score-1069.56  avg_scr -1208.29\n",
      "episode:  80 score-1082.51  avg_scr -1206.74\n",
      "episode:  81 score-655.78  avg_scr -1200.02\n",
      "episode:  82 score-1120.55  avg_scr -1199.06\n",
      "episode:  83 score-746.29  avg_scr -1193.67\n",
      "episode:  84 score-1266.39  avg_scr -1194.52\n",
      "episode:  85 score-860.02  avg_scr -1190.64\n",
      "episode:  86 score-1262.53  avg_scr -1191.46\n",
      "episode:  87 score-747.44  avg_scr -1186.42\n",
      "episode:  88 score-1135.03  avg_scr -1185.84\n",
      "episode:  89 score-1166.88  avg_scr -1185.63\n",
      "episode:  90 score-1144.84  avg_scr -1185.18\n",
      "episode:  91 score-967.35  avg_scr -1182.81\n",
      "episode:  92 score-1158.91  avg_scr -1182.55\n",
      "episode:  93 score-1091.76  avg_scr -1181.59\n",
      "episode:  94 score-1141.31  avg_scr -1181.16\n",
      "episode:  95 score-618.74  avg_scr -1175.31\n",
      "episode:  96 score-1256.97  avg_scr -1176.15\n",
      "episode:  97 score-1176.19  avg_scr -1176.15\n",
      "episode:  98 score-841.41  avg_scr -1172.77\n",
      "episode:  99 score-1100.20  avg_scr -1172.04\n",
      "episode:  100 score-974.16  avg_scr -1166.84\n",
      "episode:  101 score-1223.40  avg_scr -1162.99\n",
      "episode:  102 score-938.94  avg_scr -1157.22\n",
      "episode:  103 score-858.66  avg_scr -1149.56\n",
      "episode:  104 score-840.33  avg_scr -1141.42\n",
      "episode:  105 score-882.74  avg_scr -1134.84\n",
      "episode:  106 score-1121.70  avg_scr -1134.25\n",
      "episode:  107 score-1198.72  avg_scr -1132.86\n",
      "episode:  108 score-630.25  avg_scr -1129.41\n",
      "episode:  109 score-860.29  avg_scr -1124.27\n",
      "episode:  110 score-870.42  avg_scr -1119.56\n",
      "episode:  111 score-630.76  avg_scr -1112.90\n",
      "episode:  112 score-762.30  avg_scr -1105.65\n",
      "episode:  113 score-916.61  avg_scr -1100.96\n",
      "episode:  114 score-1174.72  avg_scr -1096.52\n",
      "episode:  115 score-1127.67  avg_scr -1094.98\n",
      "episode:  116 score-1132.22  avg_scr -1092.51\n",
      "episode:  117 score-849.57  avg_scr -1091.06\n",
      "episode:  118 score-903.54  avg_scr -1084.14\n",
      "episode:  119 score-904.17  avg_scr -1078.81\n",
      "episode:  120 score-793.69  avg_scr -1073.73\n",
      "episode:  121 score-637.17  avg_scr -1065.73\n",
      "episode:  122 score-1004.53  avg_scr -1062.03\n",
      "episode:  123 score-531.54  avg_scr -1054.19\n",
      "episode:  124 score-902.36  avg_scr -1049.74\n",
      "episode:  125 score-877.58  avg_scr -1046.89\n",
      "episode:  126 score-857.99  avg_scr -1043.30\n",
      "episode:  127 score-758.89  avg_scr -1040.15\n",
      "episode:  128 score-1084.81  avg_scr -1040.60\n",
      "episode:  129 score-740.54  avg_scr -1036.04\n",
      "episode:  130 score-853.12  avg_scr -1031.45\n",
      "episode:  131 score-872.49  avg_scr -1028.04\n",
      "episode:  132 score-740.26  avg_scr -1024.17\n",
      "episode:  133 score-857.95  avg_scr -1018.30\n",
      "episode:  134 score-955.33  avg_scr -1015.14\n",
      "episode:  135 score-727.88  avg_scr -1011.22\n",
      "episode:  136 score-659.35  avg_scr -1005.67\n",
      "episode:  137 score-866.57  avg_scr -1004.71\n",
      "episode:  138 score-746.24  avg_scr -998.54\n",
      "episode:  139 score-906.06  avg_scr -995.87\n",
      "episode:  140 score-736.63  avg_scr -988.27\n",
      "episode:  141 score-857.04  avg_scr -981.77\n",
      "episode:  142 score-628.66  avg_scr -975.63\n",
      "episode:  143 score-630.83  avg_scr -970.60\n",
      "episode:  144 score-489.46  avg_scr -964.07\n",
      "episode:  145 score-515.05  avg_scr -956.27\n",
      "episode:  146 score-623.22  avg_scr -951.65\n",
      "episode:  147 score-620.04  avg_scr -944.62\n",
      "episode:  148 score-748.92  avg_scr -941.23\n",
      "episode:  149 score-736.81  avg_scr -938.32\n",
      "episode:  150 score-681.59  avg_scr -933.22\n",
      "episode:  151 score-502.15  avg_scr -927.15\n",
      "episode:  152 score-524.69  avg_scr -921.87\n",
      "episode:  153 score-772.86  avg_scr -917.82\n",
      "episode:  154 score-506.76  avg_scr -912.47\n",
      "episode:  155 score-779.22  avg_scr -910.40\n",
      "episode:  156 score-308.82  avg_scr -898.65\n",
      "episode:  157 score-924.92  avg_scr -899.33\n",
      "episode:  158 score-503.83  avg_scr -893.47\n",
      "episode:  159 score-640.76  avg_scr -886.89\n",
      "episode:  160 score-987.17  avg_scr -884.89\n",
      "episode:  161 score-377.29  avg_scr -874.41\n",
      "episode:  162 score-743.95  avg_scr -869.48\n",
      "episode:  163 score-1072.58  avg_scr -871.27\n",
      "episode:  164 score-256.66  avg_scr -864.09\n",
      "episode:  165 score-261.43  avg_scr -857.94\n",
      "episode:  166 score-1018.60  avg_scr -855.08\n",
      "episode:  167 score-265.15  avg_scr -849.04\n",
      "episode:  168 score-371.34  avg_scr -845.25\n",
      "episode:  169 score-136.64  avg_scr -837.76\n",
      "episode:  170 score-376.45  avg_scr -829.29\n",
      "episode:  171 score-136.69  avg_scr -817.86\n",
      "episode:  172 score-136.19  avg_scr -811.65\n",
      "episode:  173 score-501.18  avg_scr -805.69\n",
      "episode:  174 score-382.28  avg_scr -796.45\n",
      "episode:  175 score-15.85  avg_scr -789.05\n",
      "episode:  176 score-598.90  avg_scr -788.69\n",
      "episode:  177 score-523.25  avg_scr -783.16\n",
      "episode:  178 score-628.97  avg_scr -783.09\n",
      "episode:  179 score-384.43  avg_scr -776.24\n",
      "episode:  180 score-372.88  avg_scr -769.14\n",
      "episode:  181 score-336.68  avg_scr -765.95\n",
      "episode:  182 score-756.95  avg_scr -762.32\n",
      "episode:  183 score-378.10  avg_scr -758.63\n",
      "episode:  184 score-768.14  avg_scr -753.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  185 score-848.85  avg_scr -753.54\n",
      "episode:  186 score-250.97  avg_scr -743.42\n",
      "episode:  187 score-781.63  avg_scr -743.77\n",
      "episode:  188 score-132.30  avg_scr -733.74\n",
      "episode:  189 score-747.92  avg_scr -729.55\n",
      "episode:  190 score-258.34  avg_scr -720.68\n",
      "episode:  191 score-531.63  avg_scr -716.33\n",
      "episode:  192 score-498.31  avg_scr -709.72\n",
      "episode:  193 score-288.11  avg_scr -701.68\n",
      "episode:  194 score-786.32  avg_scr -698.13\n",
      "episode:  195 score-263.34  avg_scr -694.58\n",
      "episode:  196 score-385.00  avg_scr -685.86\n",
      "episode:  197 score-514.34  avg_scr -679.24\n",
      "episode:  198 score-855.93  avg_scr -679.39\n",
      "episode:  199 score-768.51  avg_scr -676.07\n",
      "episode:  200 score-138.35  avg_scr -667.71\n",
      "episode:  201 score-256.01  avg_scr -658.04\n",
      "episode:  202 score-800.99  avg_scr -656.66\n",
      "episode:  203 score-132.10  avg_scr -649.39\n",
      "episode:  204 score-630.36  avg_scr -647.29\n",
      "episode:  205 score-376.97  avg_scr -642.24\n",
      "episode:  206 score-627.75  avg_scr -637.30\n",
      "episode:  207 score-617.85  avg_scr -631.49\n",
      "episode:  208 score-659.72  avg_scr -631.78\n",
      "episode:  209 score-253.86  avg_scr -625.72\n",
      "episode:  210 score-640.52  avg_scr -623.42\n",
      "episode:  211 score-660.34  avg_scr -623.71\n",
      "episode:  212 score-505.43  avg_scr -621.15\n",
      "episode:  213 score-785.51  avg_scr -619.83\n",
      "episode:  214 score-379.87  avg_scr -611.89\n",
      "episode:  215 score-494.81  avg_scr -605.56\n",
      "episode:  216 score-640.14  avg_scr -600.64\n",
      "episode:  217 score-936.57  avg_scr -601.51\n",
      "episode:  218 score-262.23  avg_scr -595.09\n",
      "episode:  219 score-829.34  avg_scr -594.35\n",
      "episode:  220 score-507.68  avg_scr -591.49\n",
      "episode:  221 score-377.58  avg_scr -588.89\n",
      "episode:  222 score-259.59  avg_scr -581.44\n",
      "episode:  223 score-251.09  avg_scr -578.64\n",
      "episode:  224 score-386.68  avg_scr -573.48\n",
      "episode:  225 score-482.95  avg_scr -569.53\n",
      "episode:  226 score-277.09  avg_scr -563.72\n",
      "episode:  227 score-503.94  avg_scr -561.17\n",
      "episode:  228 score-751.27  avg_scr -557.84\n",
      "episode:  229 score-375.19  avg_scr -554.19\n",
      "episode:  230 score-504.97  avg_scr -550.70\n",
      "episode:  231 score-254.47  avg_scr -544.52\n",
      "episode:  232 score-253.17  avg_scr -539.65\n",
      "episode:  233 score-134.40  avg_scr -532.42\n",
      "episode:  234 score-504.48  avg_scr -527.91\n",
      "episode:  235 score-501.79  avg_scr -525.65\n",
      "episode:  236 score-136.91  avg_scr -520.42\n",
      "episode:  237 score-373.26  avg_scr -515.49\n",
      "episode:  238 score-670.58  avg_scr -514.73\n",
      "episode:  239 score-382.75  avg_scr -509.50\n",
      "episode:  240 score-616.01  avg_scr -508.29\n",
      "episode:  241 score-137.19  avg_scr -501.10\n",
      "episode:  242 score-260.69  avg_scr -497.42\n",
      "episode:  243 score-617.45  avg_scr -497.28\n",
      "episode:  244 score-253.55  avg_scr -494.92\n",
      "episode:  245 score-133.43  avg_scr -491.11\n",
      "episode:  246 score-253.61  avg_scr -487.41\n",
      "episode:  247 score-494.19  avg_scr -486.15\n",
      "episode:  248 score-259.30  avg_scr -481.26\n",
      "episode:  249 score-259.66  avg_scr -476.48\n",
      "episode:  250 score-383.25  avg_scr -473.50\n",
      "episode:  251 score-133.59  avg_scr -469.82\n",
      "episode:  252 score-813.17  avg_scr -472.70\n",
      "episode:  253 score-639.12  avg_scr -471.36\n",
      "episode:  254 score-387.26  avg_scr -470.17\n",
      "episode:  255 score-621.38  avg_scr -468.59\n",
      "episode:  256 score-499.81  avg_scr -470.50\n",
      "episode:  257 score-515.42  avg_scr -466.40\n",
      "episode:  258 score-502.38  avg_scr -466.39\n",
      "episode:  259 score-257.65  avg_scr -462.56\n",
      "episode:  260 score-254.81  avg_scr -455.24\n",
      "episode:  261 score-372.93  avg_scr -455.19\n",
      "episode:  262 score-373.01  avg_scr -451.48\n",
      "episode:  263 score-137.41  avg_scr -442.13\n",
      "episode:  264 score-13.58  avg_scr -439.70\n",
      "episode:  265 score-553.46  avg_scr -442.62\n",
      "episode:  266 score-259.72  avg_scr -435.03\n",
      "episode:  267 score-248.51  avg_scr -434.87\n",
      "episode:  268 score-377.29  avg_scr -434.92\n",
      "episode:  269 score-256.47  avg_scr -436.12\n",
      "episode:  270 score-651.84  avg_scr -438.88\n",
      "episode:  271 score-257.45  avg_scr -440.08\n",
      "episode:  272 score-137.57  avg_scr -440.10\n",
      "episode:  273 score-375.10  avg_scr -438.84\n",
      "episode:  274 score-257.53  avg_scr -437.59\n",
      "episode:  275 score-816.60  avg_scr -445.60\n",
      "episode:  276 score-129.00  avg_scr -440.90\n",
      "episode:  277 score-253.66  avg_scr -438.20\n",
      "episode:  278 score-259.35  avg_scr -434.51\n",
      "episode:  279 score-369.74  avg_scr -434.36\n",
      "episode:  280 score-15.71  avg_scr -430.79\n",
      "episode:  281 score-903.77  avg_scr -436.46\n",
      "episode:  282 score-375.46  avg_scr -432.64\n",
      "episode:  283 score-128.94  avg_scr -430.15\n",
      "episode:  284 score-136.64  avg_scr -423.84\n",
      "episode:  285 score-133.98  avg_scr -416.69\n",
      "episode:  286 score-255.94  avg_scr -416.74\n",
      "episode:  287 score-134.60  avg_scr -410.27\n",
      "episode:  288 score-247.71  avg_scr -411.42\n",
      "episode:  289 score-630.63  avg_scr -410.25\n",
      "episode:  290 score-507.06  avg_scr -412.74\n",
      "episode:  291 score-138.49  avg_scr -408.81\n",
      "episode:  292 score-622.46  avg_scr -410.05\n",
      "episode:  293 score-248.21  avg_scr -409.65\n",
      "episode:  294 score-368.19  avg_scr -405.47\n",
      "episode:  295 score-376.49  avg_scr -406.60\n",
      "episode:  296 score-841.27  avg_scr -411.16\n",
      "episode:  297 score-507.22  avg_scr -411.09\n",
      "episode:  298 score-258.55  avg_scr -405.12\n",
      "episode:  299 score-176.22  avg_scr -399.19\n",
      "episode:  300 score-256.61  avg_scr -400.38\n",
      "episode:  301 score-137.63  avg_scr -399.19\n",
      "episode:  302 score-250.10  avg_scr -393.68\n",
      "episode:  303 score-139.93  avg_scr -393.76\n",
      "episode:  304 score-541.57  avg_scr -392.87\n",
      "episode:  305 score-374.66  avg_scr -392.85\n",
      "episode:  306 score-133.39  avg_scr -387.91\n",
      "episode:  307 score-502.35  avg_scr -386.75\n",
      "episode:  308 score-372.14  avg_scr -383.88\n",
      "episode:  309 score-264.28  avg_scr -383.98\n",
      "episode:  310 score-255.95  avg_scr -380.13\n",
      "episode:  311 score-138.56  avg_scr -374.92\n",
      "episode:  312 score-271.11  avg_scr -372.57\n",
      "episode:  313 score-405.60  avg_scr -368.77\n",
      "episode:  314 score-652.87  avg_scr -371.50\n",
      "episode:  315 score-131.78  avg_scr -367.87\n",
      "episode:  316 score-132.42  avg_scr -362.80\n",
      "episode:  317 score-713.64  avg_scr -360.57\n",
      "episode:  318 score-377.93  avg_scr -361.72\n",
      "episode:  319 score-226.21  avg_scr -355.69\n",
      "episode:  320 score-250.04  avg_scr -353.12\n",
      "episode:  321 score-386.76  avg_scr -353.21\n",
      "episode:  322 score-402.60  avg_scr -354.64\n",
      "episode:  323 score-499.31  avg_scr -357.12\n",
      "episode:  324 score-350.57  avg_scr -356.76\n",
      "episode:  325 score-504.01  avg_scr -356.97\n",
      "episode:  326 score-253.00  avg_scr -356.73\n",
      "episode:  327 score-132.59  avg_scr -353.02\n",
      "episode:  328 score-261.35  avg_scr -348.12\n",
      "episode:  329 score-363.01  avg_scr -347.99\n",
      "episode:  330 score-258.78  avg_scr -345.53\n",
      "episode:  331 score-136.32  avg_scr -344.35\n",
      "episode:  332 score-133.34  avg_scr -343.15\n",
      "episode:  333 score-124.80  avg_scr -343.06\n",
      "episode:  334 score-141.90  avg_scr -339.43\n",
      "episode:  335 score-254.99  avg_scr -336.96\n",
      "episode:  336 score-238.87  avg_scr -337.98\n",
      "episode:  337 score-395.10  avg_scr -338.20\n",
      "episode:  338 score-241.84  avg_scr -333.91\n",
      "episode:  339 score-132.90  avg_scr -331.41\n",
      "episode:  340 score-390.42  avg_scr -329.16\n",
      "episode:  341 score-253.59  avg_scr -330.32\n",
      "episode:  342 score-353.89  avg_scr -331.25\n",
      "episode:  343 score-378.30  avg_scr -328.86\n",
      "episode:  344 score-375.01  avg_scr -330.08\n",
      "episode:  345 score-137.17  avg_scr -330.12\n",
      "episode:  346 score-255.92  avg_scr -330.14\n",
      "episode:  347 score-505.27  avg_scr -330.25\n",
      "episode:  348 score-506.82  avg_scr -332.72\n",
      "episode:  349 score-394.42  avg_scr -334.07\n",
      "episode:  350 score-375.57  avg_scr -334.00\n",
      "episode:  351 score-251.90  avg_scr -335.18\n",
      "episode:  352 score-347.35  avg_scr -330.52\n"
     ]
    }
   ],
   "source": [
    "em = EnvManager(device,'Pendulum-v0')\n",
    "#(self,alpha,beta,input_dim,tau,n_actions,max_replay_mem,fc1_,fc2_,batch_size = 250,gamma = 0.99)\n",
    "agent = Agent(alpha=0.0001,beta = 0.001,input_dim = em.get_screen_width()*em.get_screen_height(),tau = 0.001,n_actions = 1,max_replay_mem = 100000,fc1_=1000,fc2_=1000)\n",
    "print(em.get_screen_width(),em.get_screen_height())\n",
    "n_games = 1000\n",
    "scores = []\n",
    "for episode in range(n_games):\n",
    "    em.reset()\n",
    "    state = em.get_state()\n",
    "    agent.noise.reset()\n",
    "    score = 0\n",
    "    for timestep in count():\n",
    "        action_idx = agent.choose_action(state)\n",
    "        reward     = em.take_action(action_idx)\n",
    "        score+=reward\n",
    "        next_state = em.get_state()\n",
    "        action_idx = torch.tensor([action_idx], device=device)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        agent.store_memory(state,action_idx,reward, next_state)\n",
    "        agent.learn()\n",
    "        state = next_state\n",
    "        if em.done:\n",
    "            scores.append(score)\n",
    "            avg_score = np.mean(scores[-100:])\n",
    "            print('episode: ', episode, 'score%.2f '% score, 'avg_scr %.2f'%avg_score)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50a193ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "em.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6683216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
