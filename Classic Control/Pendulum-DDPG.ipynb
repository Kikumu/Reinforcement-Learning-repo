{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b34bff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from random import seed\n",
    "from sklearn import preprocessing\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1977ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvManager():\n",
    "    def __init__(self, device,environment):\n",
    "        self.device = device\n",
    "        #self.env = gym.make(environment).unwrapped\n",
    "        self.env = gym.make(environment)\n",
    "        self.env.reset()\n",
    "        self.current_screen = None\n",
    "        self.done = False\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        self.current_screen = None\n",
    "        \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode)\n",
    "        \n",
    "    def num_actions_available(self):\n",
    "        return self.env.action_space\n",
    "        \n",
    "    def take_action(self, action):   \n",
    "        _, reward, self.done, _ = self.env.step([action])\n",
    "        return reward\n",
    "    \n",
    "    def just_starting(self):\n",
    "        return self.current_screen is None\n",
    "    \n",
    "    def get_state(self):\n",
    "        if self.just_starting() or self.done:\n",
    "            self.current_screen = self.get_processed_screen()\n",
    "            black_screen = torch.zeros_like(self.current_screen)\n",
    "            return black_screen\n",
    "        else:\n",
    "            s1 = self.current_screen\n",
    "            s2 = self.get_processed_screen()\n",
    "            self.current_screen = s2\n",
    "            return s2 - s1\n",
    "    \n",
    "    def get_screen_height(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[2]\n",
    "    \n",
    "    def get_screen_width(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[3]\n",
    "       \n",
    "    def get_processed_screen(self):\n",
    "        screen = em.render('rgb_array')\n",
    "        rgb_weights = [0.2989, 0.5870, 0.1140]\n",
    "        grayscale_image = np.dot(screen[...,:3], rgb_weights) \n",
    "        screen = grayscale_image.transpose((0, 1)) # PyTorch expects CHW\n",
    "        #print(type(screen)) # numpy\n",
    "        screen = self.crop_screen(screen)\n",
    "        return self.transform_screen_data(screen)\n",
    "    \n",
    "    def crop_screen(self, screen):\n",
    "        screen_height = screen.shape[0]\n",
    "        screen_width  = screen.shape[1]\n",
    "        #print('screen height(top/bottom): ',screen_height)\n",
    "        #print('screen height(left/right): ',screen_width)\n",
    "        # Strip off top and bottom\n",
    "        top = int(screen_height * 0.2)\n",
    "        #print('top: ',top)\n",
    "        bottom = int(screen_height * 0.8)\n",
    "        #print('bottom: ',bottom)\n",
    "        \n",
    "        \n",
    "        #strip off left/right\n",
    "        left  = int(screen_width * 0.2)\n",
    "        #print('left: ',left)\n",
    "        right = int(screen_width * 0.8)\n",
    "        #print('right: ',right)\n",
    "        \n",
    "        screen = screen[top:bottom, left:right]\n",
    "        return screen\n",
    "    \n",
    "    \n",
    "    def transform_screen_data(self, screen):       \n",
    "        # Convert to float, rescale, convert to tensor\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "        \n",
    "        # Use torchvision package to compose image transforms\n",
    "        resize = T.Compose([\n",
    "            T.ToPILImage()\n",
    "            ,T.Resize((40,90))\n",
    "            ,T.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        return resize(screen).unsqueeze(0).to(self.device) # add a batch dimension (BCHW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "932cce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "        \n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51501f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensors(experiences):\n",
    "    # Convert batch of Experiences to Experience of batches\n",
    "    batch = Experience(*zip(*experiences))\n",
    "\n",
    "    t1 = torch.stack(batch.state)\n",
    "    t2 = torch.stack(batch.action)\n",
    "    t3 = torch.stack(batch.reward)\n",
    "    t4 = torch.stack(batch.next_state)\n",
    "\n",
    "    return (t1,t2,t3,t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d2fbaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    ('state', 'action', 'reward', 'next_state')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8ea8d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OU_noise():\n",
    "    def __init__(self,mu,sigma=0.15,theta=0.15,dt=1e-2,x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu    = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt    = dt\n",
    "        self.x0    = x0\n",
    "        self.reset()\n",
    "        \n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev)* \\\n",
    "                self.dt +self.sigma * np.sqrt(self.dt)*np.random.normal(size = self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "    \n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef930741",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_network(nn.Module):\n",
    "    def __init__(self, lr, input_size, fc1, fc2, linear_out):\n",
    "        super(Actor_network, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.linear_out = linear_out\n",
    "        self.lr = lr\n",
    "        self.n_hidden_fc1 = fc1\n",
    "        self.n_hidden_fc2 = fc2\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.fc1  = nn.Linear(self.input_size,self.n_hidden_fc1)\n",
    "        self.fc2  = nn.Linear(self.n_hidden_fc1,self.n_hidden_fc2)\n",
    "        self.fc3  = nn.Linear(self.n_hidden_fc2,self.linear_out)\n",
    "        \n",
    "        self.ln1 =nn.LayerNorm(self.n_hidden_fc1)\n",
    "        self.ln2 =nn.LayerNorm(self.n_hidden_fc2)\n",
    "        \n",
    "        fc1 = 1.0/np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        self.fc1.weight.data.uniform_(-fc1,fc1)\n",
    "        self.fc1.bias.data.uniform_(-fc1,fc1)\n",
    "        \n",
    "        fc2 = 1.0/np.sqrt(self.fc2.weight.data.size()[0])\n",
    "        self.fc2.weight.data.uniform_(-fc2,fc2)\n",
    "        self.fc2.bias.data.uniform_(-fc2,fc2)\n",
    "        \n",
    "        fc3 = 0.003\n",
    "        self.fc3.weight.data.uniform_(-fc3,fc3)\n",
    "        self.fc3.bias.data.uniform_(-fc3,fc3)\n",
    "        \n",
    "        self.optimizer = optim.Adam(params = self.parameters() ,lr = lr)\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, states):\n",
    "        states = self.fc1(states)\n",
    "        states = self.ln1(states)\n",
    "        states = F.relu(states)\n",
    "        \n",
    "        states = self.fc2(states)\n",
    "        states = self.ln2(states)\n",
    "        states = F.relu(states)\n",
    "        \n",
    "        states = self.fc3(states)\n",
    "        out = torch.tanh(states)*2.0 #multiply by max/min env space, tanh because of -1 1 min maxing\n",
    "    \n",
    "        return out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb71ad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic_network(nn.Module):\n",
    "    def __init__(self, lr, input_size, fc1, fc2, linear_out):\n",
    "        super(Critic_network, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.linear_out = linear_out\n",
    "        self.lr = lr\n",
    "        self.n_hidden_fc1 = fc1\n",
    "        self.n_hidden_fc2 = fc2\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.fc1   = nn.Linear(self.input_size,self.n_hidden_fc1)\n",
    "        self.fc2  = nn.Linear(self.n_hidden_fc1,self.n_hidden_fc2)\n",
    "        \n",
    "        self.action_values = nn.Linear(self.linear_out,self.n_hidden_fc2)\n",
    "        \n",
    "        self.state_q_value = nn.Linear(self.n_hidden_fc2, 1)\n",
    "        \n",
    "        #normalize data cause we are sampling from so many diff environments\n",
    "        self.ln1 = nn.LayerNorm(self.n_hidden_fc1) \n",
    "        self.ln2 = nn.LayerNorm(self.n_hidden_fc2)\n",
    "        \n",
    "        #initialize weights and biases according to number of neurons per laer involved\n",
    "        fc1 = 1.0/np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        self.fc1.weight.data.uniform_(-fc1,fc1)\n",
    "        self.fc1.bias.data.uniform_(-fc1,fc1)\n",
    "        \n",
    "        fc2 = 1.0/np.sqrt(self.fc2.weight.data.size()[0])\n",
    "        self.fc2.weight.data.uniform_(-fc2,fc2)\n",
    "        self.fc2.bias.data.uniform_(-fc2,fc2)\n",
    "        \n",
    "        fc3 = 1.0/np.sqrt(self.action_values.weight.data.size()[0])\n",
    "        self.action_values.weight.data.uniform_(-fc3,fc3)\n",
    "        self.action_values.bias.data.uniform_(-fc3,fc3)\n",
    "        \n",
    "        #final output layer initialize by 0.003 as desired by paper\n",
    "        self.state_q_value.weight.data.uniform_(-0.003,0.003)\n",
    "        self.state_q_value.bias.data.uniform_(-0.003,0.003)\n",
    "        \n",
    "        self.optimizer = optim.Adam(params = self.parameters() ,lr = lr, weight_decay = 0.01)#weight decay?\n",
    "        self.to(self.device)\n",
    "        \n",
    "        \n",
    "    def forward(self, states, actions):\n",
    "        states = self.fc1(states).to(self.device)\n",
    "        states = self.ln1(states)\n",
    "        states = F.relu(states)\n",
    "        \n",
    "        states = self.fc2(states)\n",
    "        states = self.ln2(states)\n",
    "        #states = F.relu(states)\n",
    "        \n",
    "        action_v = self.action_values(actions)#action nn has same output size as states\n",
    "        state_action_add = F.relu(torch.add(action_v,states))\n",
    "        \n",
    "        final_q = self.state_q_value(state_action_add)\n",
    "         \n",
    "        return final_q  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bdfd63c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,alpha,beta,input_dim,tau,n_actions,max_replay_mem,fc1_,fc2_,batch_size = 250,gamma = 0.99):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        \n",
    "        self.memory = ReplayMemory(max_replay_mem)\n",
    "        \n",
    "        self.noise = OU_noise(mu = np.zeros(n_actions))\n",
    "        \n",
    "        self.actor = Actor_network(alpha,input_dim,fc1_,fc2_,linear_out=n_actions)\n",
    "        \n",
    "        self.t_actor= Actor_network(alpha,input_dim,fc1_,fc2_,linear_out=n_actions)\n",
    "        \n",
    "        self.critic= Critic_network(beta,input_dim,fc1_,fc2_,linear_out=n_actions)\n",
    "        \n",
    "        self.t_critic=Critic_network(beta,input_dim,fc1_,fc2_,linear_out=n_actions)\n",
    "        \n",
    "        self.update_network_parameters(tau=1)\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        self.actor.eval()\n",
    "        state = state.flatten()\n",
    "        mu = self.actor(state).to(self.actor.device)\n",
    "        mu_noise = mu + torch.tensor(self.noise(), dtype=torch.float).to(self.actor.device)\n",
    "        self.actor.train()\n",
    "        return mu_noise.cpu().detach().numpy()[0]\n",
    "    \n",
    "    def store_memory(self, state, action, reward, next_state):\n",
    "        state = state.flatten()\n",
    "        next_state = next_state.flatten()\n",
    "        self.memory.push(Experience(state,action,reward,next_state))\n",
    "        \n",
    "    def learn(self):\n",
    "        if self.memory.can_provide_sample(self.batch_size):\n",
    "            states,actions,rewards,next_states   = extract_tensors(self.memory.sample(self.batch_size))\n",
    "            states = torch.tensor(states, dtype  = torch.float).to(self.actor.device)\n",
    "            actions = torch.tensor(actions, dtype= torch.float).to(self.actor.device)\n",
    "            rewards = torch.tensor(rewards, dtype= torch.float).to(self.actor.device)\n",
    "            next_states = torch.tensor(next_states, dtype=torch.float).to(self.actor.device)\n",
    "            \n",
    "            target_actions_network = self.t_actor(next_states)\n",
    "            target_critic_network = self.t_critic(next_states,target_actions_network)\n",
    "            #print('states crit: ', states.shape,'actions: ', actions.shape)\n",
    "            critic_network = self.critic(states,actions)\n",
    "            #print('critic: ',critic_network.shape)\n",
    "            #print('critic_t: ',target_critic_network.shape)\n",
    "            target_critic_network = target_critic_network.view(-1)\n",
    "            rewards = rewards.view(-1)\n",
    "            #print('critic_t_: ',target_critic_network.shape)\n",
    "            #print('rewards: ',rewards.shape)\n",
    "            target = rewards+self.gamma*target_critic_network\n",
    "            \n",
    "            #print('target: ', target.shape)\n",
    "            target = target.view(self.batch_size,1)\n",
    "            #print('target prime: ', target.shape)\n",
    "            self.critic.optimizer.zero_grad()\n",
    "            #target and critic must be same size\n",
    "            #print('critic: ',critic_network.shape)\n",
    "            critic_loss = F.mse_loss(target,critic_network)\n",
    "            critic_loss.backward()\n",
    "            self.critic.optimizer.step()\n",
    "            \n",
    "            #actor loss\n",
    "            self.actor.optimizer.zero_grad()\n",
    "            actor_loss = -self.critic(states,self.actor(states))\n",
    "            actor_loss = torch.mean(actor_loss)\n",
    "            actor_loss.backward()\n",
    "            self.actor.optimizer.step()\n",
    "            \n",
    "            self.update_network_parameters()\n",
    "            \n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "        \n",
    "        actor_params = self.actor.named_parameters()\n",
    "        critic_params = self.critic.named_parameters()\n",
    "        target_actor_params = self.t_actor.named_parameters()\n",
    "        target_critic_params = self.t_critic.named_parameters()\n",
    "        \n",
    "        critic_dict = dict(critic_params)\n",
    "        actor_dict = dict(actor_params)\n",
    "        target_critic_dict = dict(target_critic_params)\n",
    "        target_actor_dict = dict(target_actor_params)\n",
    "        \n",
    "        for weight in critic_dict:\n",
    "            critic_dict[weight] = tau*critic_dict[weight].clone() + (1-tau)*\\\n",
    "                target_critic_dict[weight].clone()\n",
    "        \n",
    "        for weight in actor_dict:\n",
    "            actor_dict[weight] = tau*actor_dict[weight].clone() + (1-tau)*\\\n",
    "                target_actor_dict[weight].clone()\n",
    "        \n",
    "        self.t_critic.load_state_dict(critic_dict)\n",
    "        self.t_actor.load_state_dict(actor_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "36cc79e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 250\n",
    "gamma = 0.7\n",
    "eps_start = 1\n",
    "eps_end = 0.001\n",
    "eps_decay = 0.0001\n",
    "target_update = 10\n",
    "memory_size = 10000\n",
    "lr = 0.001\n",
    "num_episodes = 10000#for more episodes for better results\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adf12ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 40\n",
      "episode:  0 score-1494.03  avg_scr -1494.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-37-7b433e5aede2>:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  states = torch.tensor(states, dtype  = torch.float).to(self.actor.device)\n",
      "<ipython-input-37-7b433e5aede2>:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  actions = torch.tensor(actions, dtype= torch.float).to(self.actor.device)\n",
      "<ipython-input-37-7b433e5aede2>:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards = torch.tensor(rewards, dtype= torch.float).to(self.actor.device)\n",
      "<ipython-input-37-7b433e5aede2>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  next_states = torch.tensor(next_states, dtype=torch.float).to(self.actor.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  1 score-1608.67  avg_scr -1551.35\n",
      "episode:  2 score-1515.85  avg_scr -1539.52\n",
      "episode:  3 score-1625.27  avg_scr -1560.95\n",
      "episode:  4 score-1654.32  avg_scr -1579.63\n",
      "episode:  5 score-1540.66  avg_scr -1573.13\n",
      "episode:  6 score-1180.43  avg_scr -1517.03\n",
      "episode:  7 score-1338.10  avg_scr -1494.67\n",
      "episode:  8 score-974.88  avg_scr -1436.91\n",
      "episode:  9 score-1374.39  avg_scr -1430.66\n",
      "episode:  10 score-1340.82  avg_scr -1422.49\n",
      "episode:  11 score-1296.65  avg_scr -1412.01\n",
      "episode:  12 score-1488.12  avg_scr -1417.86\n",
      "episode:  13 score-1385.02  avg_scr -1415.51\n",
      "episode:  14 score-1619.29  avg_scr -1429.10\n",
      "episode:  15 score-1281.63  avg_scr -1419.88\n",
      "episode:  16 score-1379.19  avg_scr -1417.49\n",
      "episode:  17 score-994.02  avg_scr -1393.96\n",
      "episode:  18 score-1595.69  avg_scr -1404.58\n",
      "episode:  19 score-1437.53  avg_scr -1406.23\n",
      "episode:  20 score-1301.30  avg_scr -1401.23\n",
      "episode:  21 score-1437.72  avg_scr -1402.89\n",
      "episode:  22 score-1373.89  avg_scr -1401.63\n",
      "episode:  23 score-1315.35  avg_scr -1398.03\n",
      "episode:  24 score-1347.96  avg_scr -1396.03\n",
      "episode:  25 score-1162.18  avg_scr -1387.04\n",
      "episode:  26 score-1217.53  avg_scr -1380.76\n",
      "episode:  27 score-1073.89  avg_scr -1369.80\n",
      "episode:  28 score-1039.36  avg_scr -1358.40\n",
      "episode:  29 score-1196.29  avg_scr -1353.00\n",
      "episode:  30 score-1312.42  avg_scr -1351.69\n",
      "episode:  31 score-1214.08  avg_scr -1347.39\n",
      "episode:  32 score-1127.03  avg_scr -1340.71\n",
      "episode:  33 score-1444.52  avg_scr -1343.77\n",
      "episode:  34 score-1271.63  avg_scr -1341.71\n",
      "episode:  35 score-1119.90  avg_scr -1335.54\n",
      "episode:  36 score-1214.36  avg_scr -1332.27\n",
      "episode:  37 score-962.60  avg_scr -1322.54\n",
      "episode:  38 score-1363.08  avg_scr -1323.58\n",
      "episode:  39 score-1173.16  avg_scr -1319.82\n",
      "episode:  40 score-1496.92  avg_scr -1324.14\n",
      "episode:  41 score-1506.88  avg_scr -1328.49\n",
      "episode:  42 score-1241.95  avg_scr -1326.48\n",
      "episode:  43 score-1134.80  avg_scr -1322.12\n",
      "episode:  44 score-1142.28  avg_scr -1318.13\n",
      "episode:  45 score-1294.85  avg_scr -1317.62\n",
      "episode:  46 score-1085.13  avg_scr -1312.67\n",
      "episode:  47 score-1322.80  avg_scr -1312.88\n",
      "episode:  48 score-1088.55  avg_scr -1308.31\n",
      "episode:  49 score-1027.63  avg_scr -1302.69\n",
      "episode:  50 score-1191.68  avg_scr -1300.52\n",
      "episode:  51 score-1108.43  avg_scr -1296.82\n",
      "episode:  52 score-1052.83  avg_scr -1292.22\n",
      "episode:  53 score-1178.29  avg_scr -1290.11\n",
      "episode:  54 score-1041.29  avg_scr -1285.58\n",
      "episode:  55 score-986.51  avg_scr -1280.24\n",
      "episode:  56 score-1483.74  avg_scr -1283.81\n",
      "episode:  57 score-856.58  avg_scr -1276.45\n",
      "episode:  58 score-1090.25  avg_scr -1273.29\n",
      "episode:  59 score-1298.90  avg_scr -1273.72\n",
      "episode:  60 score-1187.12  avg_scr -1272.30\n",
      "episode:  61 score-1424.71  avg_scr -1274.76\n",
      "episode:  62 score-1237.13  avg_scr -1274.16\n",
      "episode:  63 score-893.56  avg_scr -1268.21\n",
      "episode:  64 score-975.41  avg_scr -1263.71\n",
      "episode:  65 score-875.83  avg_scr -1257.83\n",
      "episode:  66 score-1304.40  avg_scr -1258.53\n",
      "episode:  67 score-869.80  avg_scr -1252.81\n",
      "episode:  68 score-750.09  avg_scr -1245.52\n",
      "episode:  69 score-885.75  avg_scr -1240.38\n",
      "episode:  70 score-1222.99  avg_scr -1240.14\n",
      "episode:  71 score-1279.77  avg_scr -1240.69\n",
      "episode:  72 score-756.94  avg_scr -1234.06\n",
      "episode:  73 score-1098.17  avg_scr -1232.23\n",
      "episode:  74 score-1306.27  avg_scr -1233.21\n",
      "episode:  75 score-755.23  avg_scr -1226.92\n",
      "episode:  76 score-635.50  avg_scr -1219.24\n",
      "episode:  77 score-1076.08  avg_scr -1217.41\n",
      "episode:  78 score-635.68  avg_scr -1210.04\n",
      "episode:  79 score-1069.56  avg_scr -1208.29\n",
      "episode:  80 score-1082.51  avg_scr -1206.74\n",
      "episode:  81 score-655.78  avg_scr -1200.02\n",
      "episode:  82 score-1120.55  avg_scr -1199.06\n",
      "episode:  83 score-746.29  avg_scr -1193.67\n",
      "episode:  84 score-1266.39  avg_scr -1194.52\n",
      "episode:  85 score-860.02  avg_scr -1190.64\n",
      "episode:  86 score-1262.53  avg_scr -1191.46\n",
      "episode:  87 score-747.44  avg_scr -1186.42\n",
      "episode:  88 score-1135.03  avg_scr -1185.84\n",
      "episode:  89 score-1166.88  avg_scr -1185.63\n",
      "episode:  90 score-1144.84  avg_scr -1185.18\n",
      "episode:  91 score-967.35  avg_scr -1182.81\n",
      "episode:  92 score-1158.91  avg_scr -1182.55\n",
      "episode:  93 score-1091.76  avg_scr -1181.59\n",
      "episode:  94 score-1141.31  avg_scr -1181.16\n",
      "episode:  95 score-618.74  avg_scr -1175.31\n",
      "episode:  96 score-1256.97  avg_scr -1176.15\n",
      "episode:  97 score-1176.19  avg_scr -1176.15\n",
      "episode:  98 score-841.41  avg_scr -1172.77\n",
      "episode:  99 score-1100.20  avg_scr -1172.04\n",
      "episode:  100 score-974.16  avg_scr -1166.84\n",
      "episode:  101 score-1223.40  avg_scr -1162.99\n",
      "episode:  102 score-938.94  avg_scr -1157.22\n",
      "episode:  103 score-858.66  avg_scr -1149.56\n",
      "episode:  104 score-840.33  avg_scr -1141.42\n",
      "episode:  105 score-882.74  avg_scr -1134.84\n",
      "episode:  106 score-1121.70  avg_scr -1134.25\n",
      "episode:  107 score-1198.72  avg_scr -1132.86\n",
      "episode:  108 score-630.25  avg_scr -1129.41\n",
      "episode:  109 score-860.29  avg_scr -1124.27\n",
      "episode:  110 score-870.42  avg_scr -1119.56\n",
      "episode:  111 score-630.76  avg_scr -1112.90\n",
      "episode:  112 score-762.30  avg_scr -1105.65\n",
      "episode:  113 score-916.61  avg_scr -1100.96\n",
      "episode:  114 score-1174.72  avg_scr -1096.52\n",
      "episode:  115 score-1127.67  avg_scr -1094.98\n",
      "episode:  116 score-1132.22  avg_scr -1092.51\n",
      "episode:  117 score-849.57  avg_scr -1091.06\n",
      "episode:  118 score-903.54  avg_scr -1084.14\n",
      "episode:  119 score-904.17  avg_scr -1078.81\n",
      "episode:  120 score-793.69  avg_scr -1073.73\n",
      "episode:  121 score-637.17  avg_scr -1065.73\n",
      "episode:  122 score-1004.53  avg_scr -1062.03\n",
      "episode:  123 score-531.54  avg_scr -1054.19\n",
      "episode:  124 score-902.36  avg_scr -1049.74\n",
      "episode:  125 score-877.58  avg_scr -1046.89\n",
      "episode:  126 score-857.99  avg_scr -1043.30\n",
      "episode:  127 score-758.89  avg_scr -1040.15\n",
      "episode:  128 score-1084.81  avg_scr -1040.60\n",
      "episode:  129 score-740.54  avg_scr -1036.04\n",
      "episode:  130 score-853.12  avg_scr -1031.45\n",
      "episode:  131 score-872.49  avg_scr -1028.04\n",
      "episode:  132 score-740.26  avg_scr -1024.17\n"
     ]
    }
   ],
   "source": [
    "em = EnvManager(device,'Pendulum-v0')\n",
    "#(self,alpha,beta,input_dim,tau,n_actions,max_replay_mem,fc1_,fc2_,batch_size = 250,gamma = 0.99)\n",
    "agent = Agent(alpha=0.0001,beta = 0.001,input_dim = em.get_screen_width()*em.get_screen_height(),tau = 0.001,n_actions = 1,max_replay_mem = 100000,fc1_=1000,fc2_=1000)\n",
    "print(em.get_screen_width(),em.get_screen_height())\n",
    "n_games = 1000\n",
    "scores = []\n",
    "for episode in range(n_games):\n",
    "    em.reset()\n",
    "    state = em.get_state()\n",
    "    agent.noise.reset()\n",
    "    score = 0\n",
    "    for timestep in count():\n",
    "        action_idx = agent.choose_action(state)\n",
    "        reward     = em.take_action(action_idx)\n",
    "        score+=reward\n",
    "        next_state = em.get_state()\n",
    "        action_idx = torch.tensor([action_idx], device=device)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        agent.store_memory(state,action_idx,reward, next_state)\n",
    "        agent.learn()\n",
    "        state = next_state\n",
    "        if em.done:\n",
    "            scores.append(score)\n",
    "            avg_score = np.mean(scores[-100:])\n",
    "            print('episode: ', episode, 'score%.2f '% score, 'avg_scr %.2f'%avg_score)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "afa9460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "em.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd4330d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
