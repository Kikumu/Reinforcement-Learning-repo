{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53d7b162",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from random import seed\n",
    "from sklearn import preprocessing\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "864be326",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvManager():\n",
    "    def __init__(self, device,environment):\n",
    "        self.device = device\n",
    "        #self.env = gym.make(environment).unwrapped\n",
    "        self.env = gym.make(environment)\n",
    "        self.env.reset()\n",
    "        self.current_screen = None\n",
    "        self.done = False\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        self.current_screen = None\n",
    "        \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode)\n",
    "        \n",
    "    def num_actions_available(self):\n",
    "        return self.env.action_space\n",
    "        \n",
    "    def take_action(self, action):   \n",
    "        _, reward, self.done, _ = self.env.step([action])\n",
    "        return reward\n",
    "    \n",
    "    def just_starting(self):\n",
    "        return self.current_screen is None\n",
    "    \n",
    "    def get_state(self):\n",
    "        if self.just_starting() or self.done:\n",
    "            self.current_screen = self.get_processed_screen()\n",
    "            black_screen = torch.zeros_like(self.current_screen)\n",
    "            return black_screen\n",
    "        else:\n",
    "            s1 = self.current_screen\n",
    "            s2 = self.get_processed_screen()\n",
    "            self.current_screen = s2\n",
    "            return s2 - s1\n",
    "    \n",
    "    def get_screen_height(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[2]\n",
    "    \n",
    "    def get_screen_width(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[3]\n",
    "       \n",
    "    def get_processed_screen(self):\n",
    "        screen = em.render('rgb_array')\n",
    "        rgb_weights = [0.2989, 0.5870, 0.1140]\n",
    "        grayscale_image = np.dot(screen[...,:3], rgb_weights) \n",
    "        screen = grayscale_image.transpose((0, 1)) # PyTorch expects CHW\n",
    "        #print(type(screen)) # numpy\n",
    "        screen = self.crop_screen(screen)\n",
    "        return self.transform_screen_data(screen)\n",
    "    \n",
    "    def crop_screen(self, screen):\n",
    "        screen_height = screen.shape[0]\n",
    "        screen_width  = screen.shape[1]\n",
    "        #print('screen height(top/bottom): ',screen_height)\n",
    "        #print('screen height(left/right): ',screen_width)\n",
    "        # Strip off top and bottom\n",
    "        top = int(screen_height * 0.2)\n",
    "        #print('top: ',top)\n",
    "        bottom = int(screen_height * 0.8)\n",
    "        #print('bottom: ',bottom)\n",
    "        \n",
    "        \n",
    "        #strip off left/right\n",
    "        left  = int(screen_width * 0.2)\n",
    "        #print('left: ',left)\n",
    "        right = int(screen_width * 0.8)\n",
    "        #print('right: ',right)\n",
    "        \n",
    "        screen = screen[top:bottom, left:right]\n",
    "        return screen\n",
    "    \n",
    "    \n",
    "    def transform_screen_data(self, screen):       \n",
    "        # Convert to float, rescale, convert to tensor\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "        \n",
    "        # Use torchvision package to compose image transforms\n",
    "        resize = T.Compose([\n",
    "            T.ToPILImage()\n",
    "            ,T.Resize((40,90))\n",
    "            ,T.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        return resize(screen).unsqueeze(0).to(self.device) # add a batch dimension (BCHW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "763b218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "        \n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d98b4d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensors(experiences):\n",
    "    # Convert batch of Experiences to Experience of batches\n",
    "    batch = Experience(*zip(*experiences))\n",
    "\n",
    "    t1 = torch.stack(batch.state)\n",
    "    t2 = torch.stack(batch.action)\n",
    "    t3 = torch.stack(batch.reward)\n",
    "    t4 = torch.stack(batch.next_state)\n",
    "\n",
    "    return (t1,t2,t3,t4)\n",
    "\n",
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    ('state', 'action', 'reward', 'next_state')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ad03e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_network(nn.Module):\n",
    "    def __init__(self, lr, input_size, fc1, fc2, linear_out):\n",
    "        super(Actor_network, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.linear_out = linear_out\n",
    "        self.lr = lr\n",
    "        self.n_hidden_fc1 = fc1\n",
    "        self.n_hidden_fc2 = fc2\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.fc1  = nn.Linear(self.input_size,self.n_hidden_fc1)\n",
    "        self.fc2  = nn.Linear(self.n_hidden_fc1,self.n_hidden_fc2)\n",
    "        self.fc3  = nn.Linear(self.n_hidden_fc2,self.linear_out)\n",
    "        \n",
    "        self.optimizer = optim.Adam(params = self.parameters() ,lr = lr)\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, states):\n",
    "        states = self.fc1(states)\n",
    "        states = F.relu(states)\n",
    "        \n",
    "        states = self.fc2(states)\n",
    "        states = F.relu(states)\n",
    "        \n",
    "        states = self.fc3(states)\n",
    "        out = torch.tanh(states)*2.0 #multiply by max/min env space, tanh because of -1 1 min maxing\n",
    "    \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "689a05bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic_network(nn.Module):\n",
    "    def __init__(self, lr, input_size, fc1, fc2, linear_out):\n",
    "        super(Critic_network, self).__init__()\n",
    "        self.input_size_cat = input_size + linear_out\n",
    "        self.linear_out = linear_out\n",
    "        self.lr = lr\n",
    "        self.n_hidden_fc1 = fc1\n",
    "        self.n_hidden_fc2 = fc2\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.fc1   = nn.Linear(self.input_size_cat,self.n_hidden_fc1)\n",
    "        self.fc2  = nn.Linear(self.n_hidden_fc1,self.n_hidden_fc2)\n",
    "        self.state_q_value = nn.Linear(self.n_hidden_fc2, 1)\n",
    "        \n",
    "        self.optimizer = optim.Adam(params = self.parameters() ,lr = lr)#weight decay?\n",
    "        self.to(self.device)\n",
    "        \n",
    "        \n",
    "    def forward(self, states, actions):\n",
    "        #print(self.input_size_cat)\n",
    "        #print(states.shape, actions.shape)\n",
    "        states = torch.cat((states,actions),dim = 1)\n",
    "        #print(states.shape)\n",
    "        states = self.fc1(states).to(self.device)\n",
    "        states = F.relu(states)\n",
    "        states = self.fc2(states)\n",
    "        states = F.relu(states)\n",
    "        final_q = self.state_q_value(states)\n",
    "         \n",
    "        return final_q  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59ec5398",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,alpha,beta,noise,input_dims,output_dims,tau,max_replay_mem,fc_1,fc_2,update_actor_counter = 5,pre_train_duration = 1000,batch_size = 250,gamma = 0.99):\n",
    "        self.input_dims = input_dims\n",
    "        self.n_actions = output_dims\n",
    "        self.hidden_1 = fc_1\n",
    "        self.hidden_2 = fc_2\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.noise = noise\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.pre_train_duration = pre_train_duration\n",
    "        self.time_step_counter = 0\n",
    "        self.update_actor_counter = update_actor_counter #delayed ddpg remember, we only been updating the critics\n",
    "        self.actor_learn_counter = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = ReplayMemory(max_replay_mem)\n",
    "        \n",
    "        \n",
    "        self.actor = Actor_network(self.alpha,self.input_dims,self.hidden_1,self.hidden_2,self.n_actions)\n",
    "        self.critic_1= Critic_network(self.beta,self.input_dims,self.hidden_1,self.hidden_2,self.n_actions)\n",
    "        self.critic_2= Critic_network(self.beta,self.input_dims,self.hidden_1,self.hidden_2,self.n_actions)\n",
    "        \n",
    "        self.t_actor = Actor_network(self.alpha,self.input_dims,self.hidden_1,self.hidden_2,self.n_actions)\n",
    "        self.t_critic_1= Critic_network(self.beta,self.input_dims,self.hidden_1,self.hidden_2,self.n_actions)\n",
    "        self.t_critic_2= Critic_network(self.beta,self.input_dims,self.hidden_1,self.hidden_2,self.n_actions)\n",
    "        \n",
    "        self.update_network_parameters(tau=1)\n",
    "        \n",
    "    def select_action(self,state):\n",
    "        if self.time_step_counter < self.pre_train_duration:\n",
    "            mu = torch.tensor(np.random.normal(loc = 0, scale = self.noise, size = self.n_actions)).to(self.actor.device)\n",
    "        else:\n",
    "            mu_noise = np.random.normal(loc = 0, scale = self.noise, size = self.n_actions)\n",
    "            state = state.flatten()\n",
    "            mu = self.actor(state)+torch.tensor(mu_noise).to(self.actor.device)\n",
    "        mu = torch.clamp(mu,-2,2)#set bounds of output to match min-max torque\n",
    "        self.time_step_counter+=1\n",
    "        return mu.cpu().detach().numpy()[0]\n",
    "    \n",
    "    def store_memory(self,state,action,reward,next_state):\n",
    "        state = state.flatten()\n",
    "        next_state = next_state.flatten()\n",
    "        action = torch.tensor([action], device=device)\n",
    "        reward =torch.tensor([reward], device=device)\n",
    "        self.memory.push(Experience(state,action,reward,next_state))\n",
    "        \n",
    "    def learn(self):\n",
    "        if self.memory.can_provide_sample(self.batch_size):\n",
    "            states,actions,rewards,next_states   = extract_tensors(self.memory.sample(self.batch_size))\n",
    "            states = torch.tensor(states, dtype  = torch.float).to(self.actor.device)\n",
    "            actions = torch.tensor(actions, dtype= torch.float).to(self.actor.device)\n",
    "            rewards = torch.tensor(rewards, dtype= torch.float).to(self.actor.device)\n",
    "            next_states = torch.tensor(next_states, dtype=torch.float).to(self.actor.device)\n",
    "            \n",
    "            t_actor_values = self.t_actor(next_states)\n",
    "            t_noise = torch.clamp(torch.tensor(np.random.normal(loc = 0, scale = 0.15)),-0.5,0.5)#between 0 - 2\n",
    "            t_actor_values = t_actor_values + t_noise\n",
    "            \n",
    "            t_actor_values = torch.clamp(t_actor_values,-2,2)#set bounds of output to match min-max torque\n",
    "        \n",
    "            q1_ = self.t_critic_1(next_states,t_actor_values)\n",
    "            q2_ = self.t_critic_2(next_states,t_actor_values)\n",
    "            \n",
    "            q1 = self.critic_1(states,actions)\n",
    "            q2 = self.critic_2(states,actions)\n",
    "            \n",
    "            q_prime_select = torch.min(q1_,q2_)\n",
    "            #take min then use it as update\n",
    "            target = rewards + self.gamma*(q_prime_select)\n",
    "            \n",
    "            self.critic_1.optimizer.zero_grad()\n",
    "            self.critic_2.optimizer.zero_grad()\n",
    "            #the target network is ALWAYS used to update the 'current' network\n",
    "            #the target network is 'updated' using tau. PLEASE REMEMBER THIS\n",
    "            critic1_loss = F.mse_loss(target,q1)\n",
    "            critic2_loss = F.mse_loss(target,q2)\n",
    "            \n",
    "            #backpropagate on 2 network always problematic so we add\n",
    "            critic_loss_ovr = critic1_loss + critic2_loss\n",
    "            critic_loss_ovr.backward()\n",
    "            self.critic_1.optimizer.step()\n",
    "            self.critic_2.optimizer.step()\n",
    "            \n",
    "            self.actor_learn_counter +=1 \n",
    "            \n",
    "            if self.actor_learn_counter % self.update_actor_counter==0:\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                main_actor_loss=self.critic_1(states, self.actor(states))\n",
    "                main_actor_loss=-torch.mean(main_actor_loss)\n",
    "                main_actor_loss.backward()\n",
    "                self.actor.optimizer.step()\n",
    "                \n",
    "                self.update_network_parameters()\n",
    "                \n",
    "    def update_network_parameters(self,tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "        #mains\n",
    "        actor_params = self.actor.named_parameters()\n",
    "        critic_1_params = self.critic_1.named_parameters()\n",
    "        critic_2_params = self.critic_2.named_parameters()\n",
    "        \n",
    "        #targets\n",
    "        target_actor_params = self.t_actor.named_parameters()\n",
    "        target_critic_1_params = self.t_critic_1.named_parameters()\n",
    "        target_critic_2_params = self.t_critic_2.named_parameters()\n",
    "        \n",
    "        #main dicts\n",
    "        actor_dict = dict(actor_params)\n",
    "        critic_1_dict = dict(critic_1_params)\n",
    "        critic_2_dict = dict(critic_2_params)\n",
    "        \n",
    "        #target_dicts\n",
    "        t_actor_dict = dict(target_actor_params)\n",
    "        t_critic_1_dict = dict(target_critic_1_params)\n",
    "        t_critic_2_dict = dict(target_critic_2_params)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for weight in critic_1_dict:\n",
    "            critic_1_dict[weight] = tau*critic_1_dict[weight].clone() + (1-tau)*\\\n",
    "                t_critic_1_dict[weight].clone()\n",
    "        \n",
    "        for weight in critic_2_dict:\n",
    "            critic_2_dict[weight] = tau*critic_2_dict[weight].clone() + (1-tau)*\\\n",
    "                t_critic_2_dict[weight].clone()\n",
    "        \n",
    "        for weight in actor_dict:\n",
    "            actor_dict[weight] = tau*actor_dict[weight].clone() + (1-tau)*\\\n",
    "                t_actor_dict[weight].clone()\n",
    "        \n",
    "        self.t_critic_1.load_state_dict(critic_1_dict)\n",
    "        self.t_critic_2.load_state_dict(critic_2_dict)\n",
    "        self.t_actor.load_state_dict(actor_dict)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b072406",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 250\n",
    "gamma = 0.7\n",
    "eps_start = 1\n",
    "eps_end = 0.001\n",
    "eps_decay = 0.0001\n",
    "target_update = 10\n",
    "memory_size = 10000\n",
    "lr = 0.001\n",
    "num_episodes = 10000#for more episodes for better results\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fafb0675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 40\n",
      "episode:  0 score-973.09  avg_scr -973.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-d3b7df2a1ec5>:51: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  states = torch.tensor(states, dtype  = torch.float).to(self.actor.device)\n",
      "<ipython-input-27-d3b7df2a1ec5>:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  actions = torch.tensor(actions, dtype= torch.float).to(self.actor.device)\n",
      "<ipython-input-27-d3b7df2a1ec5>:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rewards = torch.tensor(rewards, dtype= torch.float).to(self.actor.device)\n",
      "<ipython-input-27-d3b7df2a1ec5>:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  next_states = torch.tensor(next_states, dtype=torch.float).to(self.actor.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  1 score-1362.66  avg_scr -1167.88\n",
      "episode:  2 score-1126.54  avg_scr -1154.10\n",
      "episode:  3 score-1263.90  avg_scr -1181.55\n",
      "episode:  4 score-1727.79  avg_scr -1290.80\n",
      "episode:  5 score-1504.51  avg_scr -1326.42\n",
      "episode:  6 score-1503.40  avg_scr -1351.70\n",
      "episode:  7 score-1654.61  avg_scr -1389.56\n",
      "episode:  8 score-1255.75  avg_scr -1374.69\n",
      "episode:  9 score-1712.44  avg_scr -1408.47\n",
      "episode:  10 score-1655.77  avg_scr -1430.95\n",
      "episode:  11 score-1533.51  avg_scr -1439.50\n",
      "episode:  12 score-1562.13  avg_scr -1448.93\n",
      "episode:  13 score-1208.69  avg_scr -1431.77\n",
      "episode:  14 score-1620.96  avg_scr -1444.38\n",
      "episode:  15 score-1332.80  avg_scr -1437.41\n",
      "episode:  16 score-1453.19  avg_scr -1438.34\n",
      "episode:  17 score-1455.75  avg_scr -1439.30\n",
      "episode:  18 score-1833.31  avg_scr -1460.04\n",
      "episode:  19 score-1924.77  avg_scr -1483.28\n",
      "episode:  20 score-1562.45  avg_scr -1487.05\n",
      "episode:  21 score-1907.21  avg_scr -1506.15\n",
      "episode:  22 score-1884.64  avg_scr -1522.60\n",
      "episode:  23 score-1286.75  avg_scr -1512.78\n",
      "episode:  24 score-1466.92  avg_scr -1510.94\n",
      "episode:  25 score-1490.65  avg_scr -1510.16\n",
      "episode:  26 score-1682.96  avg_scr -1516.56\n",
      "episode:  27 score-1617.15  avg_scr -1520.15\n",
      "episode:  28 score-1646.52  avg_scr -1524.51\n",
      "episode:  29 score-1618.69  avg_scr -1527.65\n",
      "episode:  30 score-1511.98  avg_scr -1527.14\n",
      "episode:  31 score-1213.28  avg_scr -1517.34\n",
      "episode:  32 score-1515.22  avg_scr -1517.27\n",
      "episode:  33 score-1347.10  avg_scr -1512.27\n",
      "episode:  34 score-1480.90  avg_scr -1511.37\n",
      "episode:  35 score-1279.07  avg_scr -1504.92\n",
      "episode:  36 score-1590.45  avg_scr -1507.23\n",
      "episode:  37 score-1625.21  avg_scr -1510.33\n",
      "episode:  38 score-1538.30  avg_scr -1511.05\n",
      "episode:  39 score-1500.48  avg_scr -1510.79\n",
      "episode:  40 score-1538.48  avg_scr -1511.46\n",
      "episode:  41 score-1364.44  avg_scr -1507.96\n",
      "episode:  42 score-1088.50  avg_scr -1498.21\n",
      "episode:  43 score-1613.66  avg_scr -1500.83\n",
      "episode:  44 score-1141.13  avg_scr -1492.84\n",
      "episode:  45 score-1623.28  avg_scr -1495.67\n",
      "episode:  46 score-1074.14  avg_scr -1486.70\n",
      "episode:  47 score-1656.20  avg_scr -1490.24\n",
      "episode:  48 score-1421.71  avg_scr -1488.84\n",
      "episode:  49 score-1276.11  avg_scr -1484.58\n",
      "episode:  50 score-1664.77  avg_scr -1488.12\n",
      "episode:  51 score-1292.81  avg_scr -1484.36\n",
      "episode:  52 score-1253.92  avg_scr -1480.01\n",
      "episode:  53 score-1232.09  avg_scr -1475.42\n",
      "episode:  54 score-1285.13  avg_scr -1471.96\n",
      "episode:  55 score-1080.20  avg_scr -1464.97\n",
      "episode:  56 score-1626.14  avg_scr -1467.79\n",
      "episode:  57 score-1640.50  avg_scr -1470.77\n",
      "episode:  58 score-1087.40  avg_scr -1464.27\n",
      "episode:  59 score-1175.63  avg_scr -1459.46\n",
      "episode:  60 score-780.54  avg_scr -1448.33\n",
      "episode:  61 score-1061.22  avg_scr -1442.09\n",
      "episode:  62 score-1205.66  avg_scr -1438.34\n",
      "episode:  63 score-1211.12  avg_scr -1434.79\n",
      "episode:  64 score-848.64  avg_scr -1425.77\n",
      "episode:  65 score-973.28  avg_scr -1418.91\n",
      "episode:  66 score-894.36  avg_scr -1411.08\n",
      "episode:  67 score-971.04  avg_scr -1404.61\n",
      "episode:  68 score-1036.07  avg_scr -1399.27\n",
      "episode:  69 score-971.37  avg_scr -1393.16\n",
      "episode:  70 score-971.76  avg_scr -1387.22\n",
      "episode:  71 score-1057.85  avg_scr -1382.65\n",
      "episode:  72 score-1002.77  avg_scr -1377.44\n",
      "episode:  73 score-1069.13  avg_scr -1373.28\n",
      "episode:  74 score-1079.14  avg_scr -1369.36\n",
      "episode:  75 score-879.35  avg_scr -1362.91\n",
      "episode:  76 score-1113.86  avg_scr -1359.67\n",
      "episode:  77 score-1001.98  avg_scr -1355.09\n",
      "episode:  78 score-777.64  avg_scr -1347.78\n",
      "episode:  79 score-792.34  avg_scr -1340.84\n",
      "episode:  80 score-1571.98  avg_scr -1343.69\n",
      "episode:  81 score-932.49  avg_scr -1338.67\n",
      "episode:  82 score-866.68  avg_scr -1332.99\n",
      "episode:  83 score-1126.08  avg_scr -1330.52\n",
      "episode:  84 score-959.51  avg_scr -1326.16\n",
      "episode:  85 score-1089.26  avg_scr -1323.41\n",
      "episode:  86 score-759.74  avg_scr -1316.93\n",
      "episode:  87 score-1014.09  avg_scr -1313.49\n",
      "episode:  88 score-964.81  avg_scr -1309.57\n",
      "episode:  89 score-1081.93  avg_scr -1307.04\n",
      "episode:  90 score-749.24  avg_scr -1300.91\n",
      "episode:  91 score-862.04  avg_scr -1296.14\n",
      "episode:  92 score-1080.23  avg_scr -1293.82\n",
      "episode:  93 score-1044.12  avg_scr -1291.16\n",
      "episode:  94 score-874.99  avg_scr -1286.78\n",
      "episode:  95 score-941.94  avg_scr -1283.19\n",
      "episode:  96 score-1140.87  avg_scr -1281.72\n",
      "episode:  97 score-967.66  avg_scr -1278.52\n",
      "episode:  98 score-974.80  avg_scr -1275.45\n",
      "episode:  99 score-1142.60  avg_scr -1274.12\n",
      "episode:  100 score-1123.84  avg_scr -1275.63\n",
      "episode:  101 score-861.05  avg_scr -1270.61\n",
      "episode:  102 score-864.72  avg_scr -1267.99\n",
      "episode:  103 score-872.10  avg_scr -1264.07\n",
      "episode:  104 score-954.53  avg_scr -1256.34\n",
      "episode:  105 score-781.13  avg_scr -1249.11\n",
      "episode:  106 score-756.99  avg_scr -1241.64\n",
      "episode:  107 score-990.94  avg_scr -1235.01\n",
      "episode:  108 score-754.46  avg_scr -1229.99\n",
      "episode:  109 score-1031.48  avg_scr -1223.18\n",
      "episode:  110 score-766.29  avg_scr -1214.29\n",
      "episode:  111 score-746.17  avg_scr -1206.42\n",
      "episode:  112 score-984.31  avg_scr -1200.64\n",
      "episode:  113 score-989.40  avg_scr -1198.45\n",
      "episode:  114 score-878.18  avg_scr -1191.02\n",
      "episode:  115 score-928.02  avg_scr -1186.97\n",
      "episode:  116 score-760.40  avg_scr -1180.04\n",
      "episode:  117 score-763.87  avg_scr -1173.12\n",
      "episode:  118 score-1258.31  avg_scr -1167.37\n",
      "episode:  119 score-859.25  avg_scr -1156.72\n",
      "episode:  120 score-996.11  avg_scr -1151.05\n",
      "episode:  121 score-1019.48  avg_scr -1142.18\n",
      "episode:  122 score-853.75  avg_scr -1131.87\n",
      "episode:  123 score-1166.93  avg_scr -1130.67\n",
      "episode:  124 score-969.19  avg_scr -1125.69\n",
      "episode:  125 score-888.47  avg_scr -1119.67\n",
      "episode:  126 score-786.27  avg_scr -1110.70\n",
      "episode:  127 score-950.28  avg_scr -1104.04\n",
      "episode:  128 score-701.89  avg_scr -1094.59\n",
      "episode:  129 score-859.26  avg_scr -1087.00\n",
      "episode:  130 score-787.06  avg_scr -1079.75\n",
      "episode:  131 score-874.44  avg_scr -1076.36\n",
      "episode:  132 score-890.79  avg_scr -1070.11\n",
      "episode:  133 score-973.42  avg_scr -1066.38\n",
      "episode:  134 score-857.64  avg_scr -1060.14\n",
      "episode:  135 score-862.06  avg_scr -1055.97\n",
      "episode:  136 score-1070.46  avg_scr -1050.77\n",
      "episode:  137 score-972.33  avg_scr -1044.24\n",
      "episode:  138 score-550.01  avg_scr -1034.36\n",
      "episode:  139 score-742.73  avg_scr -1026.78\n",
      "episode:  140 score-864.88  avg_scr -1020.05\n",
      "episode:  141 score-968.24  avg_scr -1016.09\n",
      "episode:  142 score-749.15  avg_scr -1012.69\n",
      "episode:  143 score-850.81  avg_scr -1005.06\n",
      "episode:  144 score-718.31  avg_scr -1000.84\n",
      "episode:  145 score-765.38  avg_scr -992.26\n",
      "episode:  146 score-883.94  avg_scr -990.36\n",
      "episode:  147 score-517.61  avg_scr -978.97\n",
      "episode:  148 score-629.62  avg_scr -971.05\n",
      "episode:  149 score-744.36  avg_scr -965.73\n",
      "episode:  150 score-627.54  avg_scr -955.36\n",
      "episode:  151 score-882.95  avg_scr -951.26\n",
      "episode:  152 score-632.18  avg_scr -945.04\n",
      "episode:  153 score-845.90  avg_scr -941.18\n",
      "episode:  154 score-636.31  avg_scr -934.69\n",
      "episode:  155 score-757.96  avg_scr -931.47\n",
      "episode:  156 score-742.31  avg_scr -922.63\n",
      "episode:  157 score-701.19  avg_scr -913.24\n",
      "episode:  158 score-725.88  avg_scr -909.62\n",
      "episode:  159 score-627.52  avg_scr -904.14\n",
      "episode:  160 score-673.61  avg_scr -903.07\n",
      "episode:  161 score-747.88  avg_scr -899.94\n",
      "episode:  162 score-629.65  avg_scr -894.18\n",
      "episode:  163 score-690.90  avg_scr -888.98\n",
      "episode:  164 score-845.40  avg_scr -888.94\n",
      "episode:  165 score-815.29  avg_scr -887.37\n",
      "episode:  166 score-736.15  avg_scr -885.78\n",
      "episode:  167 score-634.11  avg_scr -882.41\n",
      "episode:  168 score-746.25  avg_scr -879.52\n",
      "episode:  169 score-650.16  avg_scr -876.30\n",
      "episode:  170 score-709.47  avg_scr -873.68\n",
      "episode:  171 score-637.97  avg_scr -869.48\n",
      "episode:  172 score-633.32  avg_scr -865.79\n",
      "episode:  173 score-770.32  avg_scr -862.80\n",
      "episode:  174 score-855.34  avg_scr -860.56\n",
      "episode:  175 score-633.82  avg_scr -858.11\n",
      "episode:  176 score-851.03  avg_scr -855.48\n",
      "episode:  177 score-748.16  avg_scr -852.94\n",
      "episode:  178 score-834.34  avg_scr -853.51\n",
      "episode:  179 score-1183.91  avg_scr -857.42\n",
      "episode:  180 score-738.71  avg_scr -849.09\n",
      "episode:  181 score-865.28  avg_scr -848.42\n",
      "episode:  182 score-744.62  avg_scr -847.20\n",
      "episode:  183 score-1088.67  avg_scr -846.82\n",
      "episode:  184 score-751.44  avg_scr -844.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  185 score-728.67  avg_scr -841.14\n",
      "episode:  186 score-739.41  avg_scr -840.93\n",
      "episode:  187 score-821.34  avg_scr -839.00\n",
      "episode:  188 score-619.65  avg_scr -835.55\n",
      "episode:  189 score-777.45  avg_scr -832.51\n",
      "episode:  190 score-623.86  avg_scr -831.25\n",
      "episode:  191 score-1774.68  avg_scr -840.38\n",
      "episode:  192 score-627.48  avg_scr -835.85\n",
      "episode:  193 score-750.11  avg_scr -832.91\n",
      "episode:  194 score-628.75  avg_scr -830.45\n",
      "episode:  195 score-579.65  avg_scr -826.83\n",
      "episode:  196 score-654.70  avg_scr -821.97\n",
      "episode:  197 score-708.54  avg_scr -819.38\n",
      "episode:  198 score-641.82  avg_scr -816.05\n",
      "episode:  199 score-840.18  avg_scr -813.02\n",
      "episode:  200 score-630.53  avg_scr -808.09\n",
      "episode:  201 score-618.65  avg_scr -805.66\n",
      "episode:  202 score-625.84  avg_scr -803.28\n",
      "episode:  203 score-506.64  avg_scr -799.62\n",
      "episode:  204 score-868.14  avg_scr -798.76\n",
      "episode:  205 score-525.42  avg_scr -796.20\n",
      "episode:  206 score-626.17  avg_scr -794.89\n",
      "episode:  207 score-741.66  avg_scr -792.40\n",
      "episode:  208 score-745.64  avg_scr -792.31\n",
      "episode:  209 score-629.78  avg_scr -788.29\n",
      "episode:  210 score-621.52  avg_scr -786.85\n",
      "episode:  211 score-512.94  avg_scr -784.51\n",
      "episode:  212 score-738.49  avg_scr -782.06\n",
      "episode:  213 score-738.44  avg_scr -779.55\n",
      "episode:  214 score-506.38  avg_scr -775.83\n",
      "episode:  215 score-626.49  avg_scr -772.81\n",
      "episode:  216 score-593.35  avg_scr -771.14\n",
      "episode:  217 score-550.51  avg_scr -769.01\n",
      "episode:  218 score-1731.08  avg_scr -773.74\n",
      "episode:  219 score-867.36  avg_scr -773.82\n",
      "episode:  220 score-632.40  avg_scr -770.18\n",
      "episode:  221 score-795.82  avg_scr -767.94\n",
      "episode:  222 score-623.99  avg_scr -765.65\n",
      "episode:  223 score-626.84  avg_scr -760.24\n",
      "episode:  224 score-509.39  avg_scr -755.65\n",
      "episode:  225 score-750.87  avg_scr -754.27\n",
      "episode:  226 score-625.94  avg_scr -752.67\n",
      "episode:  227 score-612.17  avg_scr -749.29\n",
      "episode:  228 score-625.16  avg_scr -748.52\n",
      "episode:  229 score-553.21  avg_scr -745.46\n",
      "episode:  230 score-880.10  avg_scr -746.39\n",
      "episode:  231 score-622.06  avg_scr -743.87\n",
      "episode:  232 score-622.20  avg_scr -741.18\n",
      "episode:  233 score-775.08  avg_scr -739.20\n",
      "episode:  234 score-622.85  avg_scr -736.85\n",
      "episode:  235 score-832.71  avg_scr -736.55\n",
      "episode:  236 score-1690.76  avg_scr -742.76\n",
      "episode:  237 score-504.74  avg_scr -738.08\n",
      "episode:  238 score-625.80  avg_scr -738.84\n",
      "episode:  239 score-621.44  avg_scr -737.63\n",
      "episode:  240 score-621.70  avg_scr -735.19\n",
      "episode:  241 score-503.18  avg_scr -730.54\n",
      "episode:  242 score-504.16  avg_scr -728.09\n",
      "episode:  243 score-505.79  avg_scr -724.64\n",
      "episode:  244 score-509.65  avg_scr -722.56\n",
      "episode:  245 score-572.80  avg_scr -720.63\n",
      "episode:  246 score-678.97  avg_scr -718.58\n",
      "episode:  247 score-625.45  avg_scr -719.66\n",
      "episode:  248 score-505.73  avg_scr -718.42\n",
      "episode:  249 score-620.48  avg_scr -717.18\n",
      "episode:  250 score-622.10  avg_scr -717.13\n",
      "episode:  251 score-627.47  avg_scr -714.57\n",
      "episode:  252 score-1751.79  avg_scr -725.77\n",
      "episode:  253 score-613.83  avg_scr -723.45\n",
      "episode:  254 score-591.70  avg_scr -723.00\n",
      "episode:  255 score-553.86  avg_scr -720.96\n",
      "episode:  256 score-505.42  avg_scr -718.59\n",
      "episode:  257 score-525.38  avg_scr -716.83\n",
      "episode:  258 score-740.87  avg_scr -716.98\n",
      "episode:  259 score-650.75  avg_scr -717.22\n",
      "episode:  260 score-618.54  avg_scr -716.67\n",
      "episode:  261 score-1679.22  avg_scr -725.98\n",
      "episode:  262 score-505.34  avg_scr -724.74\n",
      "episode:  263 score-727.41  avg_scr -725.10\n",
      "episode:  264 score-501.25  avg_scr -721.66\n",
      "episode:  265 score-722.69  avg_scr -720.73\n",
      "episode:  266 score-620.47  avg_scr -719.58\n",
      "episode:  267 score-503.56  avg_scr -718.27\n",
      "episode:  268 score-508.32  avg_scr -715.89\n",
      "episode:  269 score-622.79  avg_scr -715.62\n",
      "episode:  270 score-625.00  avg_scr -714.77\n",
      "episode:  271 score-723.83  avg_scr -715.63\n",
      "episode:  272 score-622.18  avg_scr -715.52\n",
      "episode:  273 score-695.99  avg_scr -714.78\n",
      "episode:  274 score-505.03  avg_scr -711.28\n",
      "episode:  275 score-503.05  avg_scr -709.97\n",
      "episode:  276 score-621.59  avg_scr -707.67\n",
      "episode:  277 score-503.73  avg_scr -705.23\n",
      "episode:  278 score-624.51  avg_scr -703.13\n",
      "episode:  279 score-506.07  avg_scr -696.35\n",
      "episode:  280 score-505.72  avg_scr -694.02\n",
      "episode:  281 score-965.40  avg_scr -695.02\n",
      "episode:  282 score-1177.55  avg_scr -699.35\n",
      "episode:  283 score-502.06  avg_scr -693.49\n",
      "episode:  284 score-508.80  avg_scr -691.06\n",
      "episode:  285 score-506.49  avg_scr -688.84\n",
      "episode:  286 score-721.51  avg_scr -688.66\n",
      "episode:  287 score-851.97  avg_scr -688.97\n",
      "episode:  288 score-622.64  avg_scr -689.00\n",
      "episode:  289 score-620.49  avg_scr -687.43\n",
      "episode:  290 score-500.59  avg_scr -686.19\n",
      "episode:  291 score-544.31  avg_scr -673.89\n",
      "episode:  292 score-1048.71  avg_scr -678.10\n",
      "episode:  293 score-732.02  avg_scr -677.92\n",
      "episode:  294 score-504.75  avg_scr -676.68\n",
      "episode:  295 score-627.89  avg_scr -677.16\n",
      "episode:  296 score-1088.96  avg_scr -681.51\n",
      "episode:  297 score-726.06  avg_scr -681.68\n",
      "episode:  298 score-504.76  avg_scr -680.31\n",
      "episode:  299 score-504.08  avg_scr -676.95\n",
      "episode:  300 score-502.55  avg_scr -675.67\n",
      "episode:  301 score-529.60  avg_scr -674.78\n",
      "episode:  302 score-1030.19  avg_scr -678.82\n",
      "episode:  303 score-1742.10  avg_scr -691.18\n",
      "episode:  304 score-631.90  avg_scr -688.82\n",
      "episode:  305 score-503.65  avg_scr -688.60\n",
      "episode:  306 score-615.91  avg_scr -688.50\n",
      "episode:  307 score-630.82  avg_scr -687.39\n",
      "episode:  308 score-505.50  avg_scr -684.99\n",
      "episode:  309 score-645.07  avg_scr -685.14\n",
      "episode:  310 score-972.63  avg_scr -688.65\n",
      "episode:  311 score-630.94  avg_scr -689.83\n",
      "episode:  312 score-504.11  avg_scr -687.49\n",
      "episode:  313 score-745.28  avg_scr -687.55\n",
      "episode:  314 score-504.07  avg_scr -687.53\n",
      "episode:  315 score-744.02  avg_scr -688.71\n",
      "episode:  316 score-624.46  avg_scr -689.02\n",
      "episode:  317 score-738.39  avg_scr -690.90\n",
      "episode:  318 score-820.61  avg_scr -681.79\n",
      "episode:  319 score-882.45  avg_scr -681.94\n",
      "episode:  320 score-504.80  avg_scr -680.67\n",
      "episode:  321 score-630.48  avg_scr -679.01\n",
      "episode:  322 score-574.02  avg_scr -678.51\n",
      "episode:  323 score-622.59  avg_scr -678.47\n",
      "episode:  324 score-1010.94  avg_scr -683.49\n",
      "episode:  325 score-505.13  avg_scr -681.03\n",
      "episode:  326 score-503.17  avg_scr -679.80\n",
      "episode:  327 score-505.98  avg_scr -678.74\n",
      "episode:  328 score-500.30  avg_scr -677.49\n",
      "episode:  329 score-615.07  avg_scr -678.11\n",
      "episode:  330 score-656.70  avg_scr -675.88\n",
      "episode:  331 score-522.45  avg_scr -674.88\n",
      "episode:  332 score-504.19  avg_scr -673.70\n",
      "episode:  333 score-504.63  avg_scr -670.99\n",
      "episode:  334 score-621.73  avg_scr -670.98\n",
      "episode:  335 score-621.56  avg_scr -668.87\n",
      "episode:  336 score-622.34  avg_scr -658.19\n",
      "episode:  337 score-624.72  avg_scr -659.39\n",
      "episode:  338 score-508.49  avg_scr -658.21\n",
      "episode:  339 score-757.76  avg_scr -659.58\n",
      "episode:  340 score-744.54  avg_scr -660.81\n",
      "episode:  341 score-864.35  avg_scr -664.42\n",
      "episode:  342 score-607.88  avg_scr -665.45\n",
      "episode:  343 score-620.70  avg_scr -666.60\n",
      "episode:  344 score-834.72  avg_scr -669.85\n",
      "episode:  345 score-612.08  avg_scr -670.25\n",
      "episode:  346 score-505.12  avg_scr -668.51\n",
      "episode:  347 score-505.83  avg_scr -667.31\n",
      "episode:  348 score-626.30  avg_scr -668.52\n",
      "episode:  349 score-619.63  avg_scr -668.51\n",
      "episode:  350 score-630.43  avg_scr -668.59\n",
      "episode:  351 score-761.19  avg_scr -669.93\n",
      "episode:  352 score-619.35  avg_scr -658.61\n",
      "episode:  353 score-733.08  avg_scr -659.80\n",
      "episode:  354 score-504.40  avg_scr -658.93\n",
      "episode:  355 score-503.90  avg_scr -658.43\n",
      "episode:  356 score-621.12  avg_scr -659.58\n",
      "episode:  357 score-613.42  avg_scr -660.46\n",
      "episode:  358 score-501.61  avg_scr -658.07\n",
      "episode:  359 score-504.85  avg_scr -656.61\n",
      "episode:  360 score-500.54  avg_scr -655.43\n",
      "episode:  361 score-503.19  avg_scr -643.67\n",
      "episode:  362 score-620.75  avg_scr -644.83\n",
      "episode:  363 score-506.33  avg_scr -642.61\n",
      "episode:  364 score-621.26  avg_scr -643.81\n",
      "episode:  365 score-505.84  avg_scr -641.65\n",
      "episode:  366 score-501.85  avg_scr -640.46\n",
      "episode:  367 score-504.60  avg_scr -640.47\n",
      "episode:  368 score-505.36  avg_scr -640.44\n",
      "episode:  369 score-623.32  avg_scr -640.45\n",
      "episode:  370 score-501.81  avg_scr -639.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:  371 score-502.83  avg_scr -637.00\n",
      "episode:  372 score-724.16  avg_scr -638.02\n",
      "episode:  373 score-501.94  avg_scr -636.08\n",
      "episode:  374 score-934.61  avg_scr -640.38\n",
      "episode:  375 score-504.71  avg_scr -640.40\n",
      "episode:  376 score-610.73  avg_scr -640.29\n",
      "episode:  377 score-502.09  avg_scr -640.27\n",
      "episode:  378 score-616.20  avg_scr -640.19\n",
      "episode:  379 score-619.99  avg_scr -641.33\n",
      "episode:  380 score-624.05  avg_scr -642.51\n",
      "episode:  381 score-510.64  avg_scr -637.96\n",
      "episode:  382 score-506.14  avg_scr -631.25\n",
      "episode:  383 score-583.93  avg_scr -632.07\n",
      "episode:  384 score-503.64  avg_scr -632.02\n",
      "episode:  385 score-575.34  avg_scr -632.70\n",
      "episode:  386 score-624.18  avg_scr -631.73\n",
      "episode:  387 score-626.14  avg_scr -629.47\n",
      "episode:  388 score-505.24  avg_scr -628.30\n",
      "episode:  389 score-505.32  avg_scr -627.15\n",
      "episode:  390 score-504.68  avg_scr -627.19\n",
      "episode:  391 score-504.95  avg_scr -626.79\n",
      "episode:  392 score-514.56  avg_scr -621.45\n",
      "episode:  393 score-970.96  avg_scr -623.84\n",
      "episode:  394 score-726.25  avg_scr -626.06\n",
      "episode:  395 score-504.65  avg_scr -624.82\n",
      "episode:  396 score-623.43  avg_scr -620.17\n",
      "episode:  397 score-503.13  avg_scr -617.94\n",
      "episode:  398 score-759.70  avg_scr -620.49\n",
      "episode:  399 score-502.79  avg_scr -620.48\n",
      "episode:  400 score-728.49  avg_scr -622.74\n",
      "episode:  401 score-668.02  avg_scr -624.12\n",
      "episode:  402 score-770.05  avg_scr -621.52\n",
      "episode:  403 score-645.37  avg_scr -610.55\n",
      "episode:  404 score-810.49  avg_scr -612.34\n",
      "episode:  405 score-500.85  avg_scr -612.31\n",
      "episode:  406 score-499.36  avg_scr -611.14\n",
      "episode:  407 score-739.50  avg_scr -612.23\n",
      "episode:  408 score-603.57  avg_scr -613.21\n",
      "episode:  409 score-503.91  avg_scr -611.80\n",
      "episode:  410 score-639.58  avg_scr -608.47\n",
      "episode:  411 score-612.37  avg_scr -608.28\n",
      "episode:  412 score-1104.24  avg_scr -614.29\n",
      "episode:  413 score-504.30  avg_scr -611.88\n",
      "episode:  414 score-505.09  avg_scr -611.89\n",
      "episode:  415 score-620.13  avg_scr -610.65\n",
      "episode:  416 score-503.62  avg_scr -609.44\n",
      "episode:  417 score-809.61  avg_scr -610.15\n",
      "episode:  418 score-619.68  avg_scr -608.14\n",
      "episode:  419 score-609.95  avg_scr -605.42\n",
      "episode:  420 score-599.55  avg_scr -606.36\n",
      "episode:  421 score-622.50  avg_scr -606.28\n",
      "episode:  422 score-747.25  avg_scr -608.02\n",
      "episode:  423 score-580.10  avg_scr -607.59\n",
      "episode:  424 score-505.42  avg_scr -602.54\n",
      "episode:  425 score-500.99  avg_scr -602.49\n",
      "episode:  426 score-523.57  avg_scr -602.70\n",
      "episode:  427 score-618.76  avg_scr -603.83\n",
      "episode:  428 score-640.88  avg_scr -605.23\n",
      "episode:  429 score-500.26  avg_scr -604.08\n",
      "episode:  430 score-516.04  avg_scr -602.68\n",
      "episode:  431 score-623.52  avg_scr -603.69\n",
      "episode:  432 score-507.77  avg_scr -603.72\n",
      "episode:  433 score-510.05  avg_scr -603.78\n",
      "episode:  434 score-633.99  avg_scr -603.90\n",
      "episode:  435 score-499.51  avg_scr -602.68\n",
      "episode:  436 score-505.90  avg_scr -601.52\n",
      "episode:  437 score-615.95  avg_scr -601.43\n",
      "episode:  438 score-502.19  avg_scr -601.37\n",
      "episode:  439 score-500.49  avg_scr -598.79\n",
      "episode:  440 score-505.35  avg_scr -596.40\n",
      "episode:  441 score-487.63  avg_scr -592.63\n",
      "episode:  442 score-503.41  avg_scr -591.59\n",
      "episode:  443 score-500.41  avg_scr -590.39\n",
      "episode:  444 score-501.11  avg_scr -587.05\n",
      "episode:  445 score-504.07  avg_scr -585.97\n",
      "episode:  446 score-625.29  avg_scr -587.17\n",
      "episode:  447 score-504.60  avg_scr -587.16\n",
      "episode:  448 score-505.95  avg_scr -585.96\n",
      "episode:  449 score-774.52  avg_scr -587.50\n",
      "episode:  450 score-503.64  avg_scr -586.24\n",
      "episode:  451 score-501.73  avg_scr -583.64\n",
      "episode:  452 score-503.99  avg_scr -582.49\n",
      "episode:  453 score-502.73  avg_scr -580.18\n",
      "episode:  454 score-632.85  avg_scr -581.47\n",
      "episode:  455 score-502.03  avg_scr -581.45\n",
      "episode:  456 score-504.77  avg_scr -580.29\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-e9612b642727>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mreward\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mscore\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-f49f03cccc88>\u001b[0m in \u001b[0;36mget_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0ms1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_screen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0ms2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_processed_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_screen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0ms2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-f49f03cccc88>\u001b[0m in \u001b[0;36mget_processed_screen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mrgb_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.2989\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5870\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1140\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mgrayscale_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrgb_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrayscale_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# PyTorch expects CHW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m#print(type(screen)) # numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "em = EnvManager(device,'Pendulum-v0')\n",
    "#(self,alpha,beta,noise,input_dims,output_dims,tau,max_replay_mem,fc_1,fc_2,update_actor_counter = 5,pre_train_duration = 10000,batch_size = 250,gamma = 0.99)\n",
    "agent = Agent(noise = 0.1,alpha=0.001,beta = 0.001,input_dims = em.get_screen_width()*em.get_screen_height(),tau = 0.005,output_dims = 1,max_replay_mem = 100000,fc_1=1000,fc_2=1000)\n",
    "print(em.get_screen_width(),em.get_screen_height())\n",
    "n_games = 1000\n",
    "scores = []\n",
    "for episode in range(n_games):\n",
    "    em.reset()\n",
    "    state = em.get_state()\n",
    "    score = 0\n",
    "    for timestep in count():\n",
    "        action_idx = agent.select_action(state)\n",
    "        reward     = em.take_action(action_idx)\n",
    "        score+=reward\n",
    "        next_state = em.get_state()\n",
    "        agent.store_memory(state,action_idx,reward, next_state)\n",
    "        agent.learn()\n",
    "        state = next_state\n",
    "        if em.done:\n",
    "            scores.append(score)\n",
    "            avg_score = np.mean(scores[-100:])\n",
    "            print('episode: ', episode, 'score%.2f '% score, 'avg_scr %.2f'%avg_score)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cab60ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "em.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e35de2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
