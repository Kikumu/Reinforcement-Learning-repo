{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we only explore explore/exploit strats, not much care is given to rewards(information gathering and utilization)\n",
    "import gym\n",
    "import numpy as np\n",
    "import tqdm as tqdm\n",
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q = np.zeros((env.observation_space.n, env.action_space.n)) \n",
    "#Q[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pure_exploration_strat_fl(env, n_ep = 1):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n)) \n",
    "    V = np.zeros((env.observation_space.n)) \n",
    "    N = np.zeros((env.observation_space.n, env.action_space.n)) #used to prevent overflow\n",
    "    theta = 1e-10\n",
    "    \n",
    "    gamma = 0.99\n",
    "    name = 'Pure exploration'\n",
    "    for e in range(n_ep):\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = np.random.randint((env.action_space.n))\n",
    "            #print(type(action))\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            N[state][action] += 1\n",
    "            Q[state][action] += (reward + gamma*V[new_state])/ N[state][action]\n",
    "            \n",
    "            #reason for subtraction is because in this env, rewards are 0 or 1. having a constant +ve reward may cause hangups \n",
    "            if done == True:\n",
    "                break\n",
    "            #print(np.max(np.abs(V - np.max(Q, axis = 1))))\n",
    "            #if np.max(np.abs(V - np.max(Q, axis = 1))) < theta:\n",
    "            #    break\n",
    "            state = new_state\n",
    "        V = np.max(Q, axis = 1)\n",
    "            #Q_s[e] = Q\n",
    "            #returns[e] = reward\n",
    "            #actions[e] = action\n",
    "    return V,Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pure_exploration_strat_fl(env, n_ep = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pure_exploitation_strat_fl(env, n_ep = 1):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n)) \n",
    "    V = np.zeros((env.observation_space.n)) \n",
    "    N = np.zeros((env.observation_space.n, env.action_space.n)) #used to prevent overflow\n",
    "    theta = 1e-10\n",
    "    \n",
    "    gamma = 0.99\n",
    "    name = 'Pure exploitation'\n",
    "    for e in range(n_ep):\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = np.argmax(Q[state])\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            N[state][action] += 1\n",
    "            Q[state][action] += (reward + gamma*V[new_state])/ N[state][action]\n",
    "            \n",
    "            #reason for subtraction is because in this env, rewards are 0 or 1. having a constant +ve reward may cause hangups \n",
    "            if done == True:\n",
    "                break\n",
    "            #print(np.max(np.abs(V - np.max(Q, axis = 1))))\n",
    "            #if np.max(np.abs(V - np.max(Q, axis = 1))) < theta:\n",
    "            #    break\n",
    "            state = new_state\n",
    "        V = np.max(Q, axis = 1)\n",
    "            #Q_s[e] = Q\n",
    "            #returns[e] = reward\n",
    "            #actions[e] = action\n",
    "    return V,Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pure_exploitation_strat_fl(env, n_ep = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pure_exploitation_strat(env, n_ep = 300):\n",
    "    Q = np.zeros((env.action_space.n)) #remember, mab has only one state so Q table will be 1 by no. of actions\n",
    "    N = np.zeros((env.action_space.n)) #counts the number of times an action was selected(basically the discount factor)\n",
    "    \n",
    "    Q_s = np.empty((n_ep, env.action_space.n))\n",
    "    returns = np.empty(n_ep)\n",
    "    actions = np.empty(n_ep)\n",
    "    \n",
    "    name = 'Pure exploitation'\n",
    "    for e in tqdm(range(n_ep)):\n",
    "        action = np.argmax(Q)\n",
    "        N[action]+= 1\n",
    "        _, reward, _, _ = env.step(action)\n",
    "        Q[action] += (reward - Q[action])/N[action] \n",
    "        #reason for subtraction is because in this env, rewards are 0 or 1. having a constant +ve reward may cause hangups \n",
    "        \n",
    "        Q_s[e] = Q\n",
    "        returns[e] = reward\n",
    "        actions[e] = action\n",
    "    return name, returns, Qe, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pure_random_strat(env, n_ep = 300):\n",
    "    Q = np.zeros((env.action_space.n)) #remember, mab has only one state so Q table will be 1 by no. of actions\n",
    "    N = np.zeros((env.action_space.n)) #counts the number of times an action was selected(basically the discount factor)\n",
    "    \n",
    "    Q_s = np.empty((n_ep, env.action_space.n))\n",
    "    returns = np.empty(n_ep)\n",
    "    actions = np.empty(n_ep)\n",
    "    \n",
    "    name = 'Pure exploitation'\n",
    "    for e in tqdm(range(n_ep)):\n",
    "        #action = np.argmax(Q)\n",
    "        action = np.random.randint(len(env.action_space.n))\n",
    "        N[action]+= 1\n",
    "        _, reward, _, _ = env.step(action)\n",
    "        Q[action] += (reward - Q[action])/N[action] \n",
    "        #reason for subtraction is because in this env, rewards are 0 or 1. having a constant +ve reward may cause hangups \n",
    "        \n",
    "        Q_s[e] = Q\n",
    "        returns[e] = reward\n",
    "        actions[e] = action\n",
    "    return name, returns, Qe, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_strat(env, n_ep = 300, epsilon = 0.01):\n",
    "    Q = np.zeros((env.action_space.n)) #remember, mab has only one state so Q table will be 1 by no. of actions\n",
    "    N = np.zeros((env.action_space.n)) #counts the number of times an action was selected(basically the discount factor)\n",
    "    \n",
    "    Q_s = np.empty((n_ep, env.action_space.n))\n",
    "    returns = np.empty(n_ep)\n",
    "    actions = np.empty(n_ep)\n",
    "    \n",
    "    name = 'Pure exploitation'\n",
    "    for e in tqdm(range(n_ep)):\n",
    "        #action = np.argmax(Q)\n",
    "        if np.random.random() > epsilon:\n",
    "            action = np.random.randint(len(env.action_space.n))\n",
    "        else:\n",
    "            action = np.argmax(Q)\n",
    "            \n",
    "        N[action]+= 1\n",
    "        _, reward, _, _ = env.step(action)\n",
    "        Q[action] += (reward - Q[action])/N[action] \n",
    "        #reason for subtraction is because in this env, rewards are 0 or 1. having a constant +ve reward may cause hangups \n",
    "        \n",
    "        Q_s[e] = Q\n",
    "        returns[e] = reward\n",
    "        actions[e] = action\n",
    "    return name, returns, Qe, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_strat_fl(env, n_ep = 300, epsilon = 0.001):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n)) \n",
    "    V = np.zeros((env.observation_space.n)) \n",
    "    N = np.zeros((env.observation_space.n, env.action_space.n)) #used to prevent overflow\n",
    "    gamma = 0.99\n",
    "   \n",
    "    for e in (range(n_ep)):\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            if np.random.random() > epsilon:\n",
    "                action = np.random.randint((env.action_space.n))\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "            \n",
    "            N[state][action]+= 1\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            Q[state][action] += (reward - gamma*V[new_state])/N[state][action] \n",
    "            #reason for subtraction is because in this env, rewards are 0 or 1. having a constant +ve reward may cause hangups \n",
    "            state = new_state\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        V = np.max(Q, axis=1)\n",
    "    return Q, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_dec_epsilon_greedy_fl(env,\n",
    "                          init_epsilon = 1.0,\n",
    "                          min_epsilon=0.01,\n",
    "                          decay_ratio = 0.05,\n",
    "                          n_ep = 4000):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n)) \n",
    "    V = np.zeros((env.observation_space.n)) \n",
    "    N = np.zeros((env.observation_space.n, env.action_space.n)) #used to prevent overflow\n",
    "    gamma = 0.99\n",
    "    ep_arr = []\n",
    "    \n",
    "    for episode in range(n_ep):\n",
    "        decay_episodes = n_ep * decay_ratio\n",
    "        epsilon = 1 - episode/decay_episodes\n",
    "        epsilon *= init_epsilon - min_epsilon\n",
    "        epsilon+=min_epsilon\n",
    "        epsilon = np.clip(epsilon, min_epsilon, init_epsilon)\n",
    "        total_rewards = 0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            if np.random.random() > epsilon:\n",
    "                action =  np.argmax(Q[state])\n",
    "            else:\n",
    "                action = np.random.randint(env.action_space.n)\n",
    "            \n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            N[state][action]+=1\n",
    "            Q[state][action]+= (reward + gamma*V[new_state])/N[state][action]\n",
    "            state = new_state\n",
    "            total_rewards+=reward\n",
    "            if done == True:\n",
    "                break\n",
    "        V = np.max(Q, axis=1)\n",
    "        ep_arr.append(total_rewards)\n",
    "    return V,Q,ep_arr      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimistic_initialization(env,\n",
    "                             optimistic_estimate = 1.0,\n",
    "                             initial_count = 100,\n",
    "                             n_ep = 5000):\n",
    "    Q = np.full((env.observation_space.n,env.action_space.n ),optimistic_estimate)\n",
    "    N = np.full((env.observation_space.n,env.action_space.n),initial_count)\n",
    "    V = np.zeros((env.observation_space.n))\n",
    "    gamma = 0.99\n",
    "    for e in range(n_ep):\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = np.argmax(Q[state])\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            N[state][action]+=1\n",
    "            Q[state][action]+= (reward + gamma*V[new_state])/N[state][action]\n",
    "            state = new_state\n",
    "            \n",
    "            if done == True:\n",
    "                break\n",
    "        V = np.max(Q, axis=1)\n",
    "    return V, Q, N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def softmax(env,\n",
    "           init_temp = 1000.0,\n",
    "           min_temp = 0.01,\n",
    "           decay_ratio = 0.04,\n",
    "           n_episodes = 5000):\n",
    "    \n",
    "    theta = 1e-10\n",
    "    Q = np.full((env.observation_space.n, env.action_space.n), theta)\n",
    "    N = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    V = np.zeros((env.observation_space.n))\n",
    "    gamma = 0.99\n",
    "    \n",
    "    for e in range(n_episodes):\n",
    "        decay_episodes = n_episodes*decay_ratio\n",
    "        temp = 1 - e/decay_episodes\n",
    "        temp*=init_temp - min_temp\n",
    "        temp+=min_temp\n",
    "        temp = np.clip(temp, min_temp, init_temp) #makes sure temp isnt 0\n",
    "        \n",
    "        scaled_Q = Q/temp #add temp\n",
    "        \n",
    "        norm_Q = scaled_Q - np.max(scaled_Q, axis = 1).reshape(-1, 1)#norm for stability\n",
    "        exp_Q = np.exp(norm_Q)\n",
    "        probs = exp_Q/np.sum(exp_Q, axis=1).reshape(-1,1)\n",
    "        \n",
    "        if math.isnan(probs[0].sum()) == True:\n",
    "            print(probs)\n",
    "        \n",
    "        assert np.isclose(probs[0].sum(), 1.0)\n",
    "        \n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = np.random.choice(np.arange(env.action_space.n),\n",
    "                                     size = 1,\n",
    "                                     p = probs[state])[0]\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            N[state][action]+=1\n",
    "            Q[state][action]+= (reward + gamma*V[new_state])/N[state][action]\n",
    "            state = new_state\n",
    "            \n",
    "            if done == True:\n",
    "                break\n",
    "        V = np.max(Q, axis = 1)\n",
    "    return V, Q\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "#V,Q = softmax(env,\n",
    "#           init_temp = 1000.0,\n",
    "#           min_temp = 0.01,\n",
    "#           decay_ratio = 0.05,\n",
    "#           n_episodes = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "#V,Q,N = optimistic_initialization(env,\n",
    "#                             optimistic_estimate = 1.0,\n",
    "#                             initial_count = 100,\n",
    "#                             n_ep = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "#V,Q,R = lin_dec_epsilon_greedy_fl(env,\n",
    "#                          init_epsilon = 0.8,\n",
    "#                          min_epsilon=0.1,\n",
    "#                          decay_ratio = 0.05,\n",
    "#                          n_ep = 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epsilon_greedy_strat_fl(env, n_ep = 150, epsilon = 0.01)\n",
    "#q_table = Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maxStepsPerEpisode = 100\n",
    "#import time\n",
    "#from IPython.display import clear_output\n",
    "#for episode in range(10):\n",
    "#    state = env.reset()\n",
    "#    print(\"EPISODE: \", episode+1)\n",
    "#    time.sleep(2)\n",
    "\n",
    "#    for step in range(maxStepsPerEpisode):\n",
    "#        clear_output(wait=True)\n",
    "#        env.render()\n",
    "#        time.sleep(0.5)\n",
    "\n",
    "#        action = np.argmax(q_table[state, :])\n",
    "#        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "#        if done:\n",
    "#            clear_output(wait=True)\n",
    "#            env.render()\n",
    "#            if reward == 1:\n",
    "#                print(\"successfull goal\")\n",
    "#                time.sleep(2)\n",
    "#            elif reward == -1:\n",
    "#                print(\"hole\")\n",
    "#                time.sleep(2)\n",
    "#            clear_output(wait=True)\n",
    "#            break\n",
    "\n",
    "#    state = new_state\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
