{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tqdm as tqdm\n",
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.zeros((env.observation_space.n, env.action_space.n)) \n",
    "Q[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pure_exploration_strat_fl(env, n_ep = 1):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n)) \n",
    "    V = np.zeros((env.observation_space.n)) \n",
    "    N = np.zeros((env.observation_space.n, env.action_space.n)) #used to prevent overflow\n",
    "    theta = 1e-10\n",
    "    \n",
    "    gamma = 0.99\n",
    "    name = 'Pure exploration'\n",
    "    for e in range(n_ep):\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = np.random.randint((env.action_space.n))\n",
    "            #print(type(action))\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            N[state][action] += 1\n",
    "            Q[state][action] += (reward + gamma*V[new_state])/ N[state][action]\n",
    "            \n",
    "            #reason for subtraction is because in this env, rewards are 0 or 1. having a constant +ve reward may cause hangups \n",
    "            if done == True:\n",
    "                break\n",
    "            #print(np.max(np.abs(V - np.max(Q, axis = 1))))\n",
    "            #if np.max(np.abs(V - np.max(Q, axis = 1))) < theta:\n",
    "            #    break\n",
    "            state = new_state\n",
    "        V = np.max(Q, axis = 1)\n",
    "            #Q_s[e] = Q\n",
    "            #returns[e] = reward\n",
    "            #actions[e] = action\n",
    "    return V,Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 4.50160849,  5.90853058,  9.56409104,  7.9835978 ,  7.00255699,\n",
       "         0.        , 13.24277601,  0.        , 15.67750829, 29.28610957,\n",
       "        28.85874111,  0.        ,  0.        , 40.21278833, 36.78320708,\n",
       "         0.        ]),\n",
       " array([[ 3.90779255,  4.43054689,  4.50160849,  3.82934439],\n",
       "        [ 2.74879436,  3.93725579,  4.67477088,  5.90853058],\n",
       "        [ 8.5113944 ,  8.43997479,  9.56409104,  7.33285155],\n",
       "        [ 6.09617931,  5.69949467,  4.52647396,  7.9835978 ],\n",
       "        [ 7.00255699,  5.82980469,  5.56367295,  2.8223371 ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [13.24277601,  9.10803626, 12.22948594,  2.38966444],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 6.01523838, 12.07954029, 11.39858317, 15.67750829],\n",
       "        [19.31864364, 29.28610957, 25.94808345, 12.70668079],\n",
       "        [28.85874111, 21.83318681, 20.63407579, 11.51142516],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [22.46743721, 25.94803784, 40.21278833, 28.99497976],\n",
       "        [36.78320708, 28.97197429, 23.87765435, 26.16473957],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pure_exploration_strat_fl(env, n_ep = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pure_exploitation_strat_fl(env, n_ep = 1):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n)) \n",
    "    V = np.zeros((env.observation_space.n)) \n",
    "    N = np.zeros((env.observation_space.n, env.action_space.n)) #used to prevent overflow\n",
    "    theta = 1e-10\n",
    "    \n",
    "    gamma = 0.99\n",
    "    name = 'Pure exploitation'\n",
    "    for e in range(n_ep):\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = np.argmax(Q[state])\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            N[state][action] += 1\n",
    "            Q[state][action] += (reward + gamma*V[new_state])/ N[state][action]\n",
    "            \n",
    "            #reason for subtraction is because in this env, rewards are 0 or 1. having a constant +ve reward may cause hangups \n",
    "            if done == True:\n",
    "                break\n",
    "            #print(np.max(np.abs(V - np.max(Q, axis = 1))))\n",
    "            #if np.max(np.abs(V - np.max(Q, axis = 1))) < theta:\n",
    "            #    break\n",
    "            state = new_state\n",
    "        V = np.max(Q, axis = 1)\n",
    "            #Q_s[e] = Q\n",
    "            #returns[e] = reward\n",
    "            #actions[e] = action\n",
    "    return V,Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pure_exploitation_strat_fl(env, n_ep = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pure_exploitation_strat(env, n_ep = 300):\n",
    "    Q = np.zeros((env.action_space.n)) #remember, mab has only one state so Q table will be 1 by no. of actions\n",
    "    N = np.zeros((env.action_space.n)) #counts the number of times an action was selected(basically the discount factor)\n",
    "    \n",
    "    Q_s = np.empty((n_ep, env.action_space.n))\n",
    "    returns = np.empty(n_ep)\n",
    "    actions = np.empty(n_ep)\n",
    "    \n",
    "    name = 'Pure exploitation'\n",
    "    for e in tqdm(range(n_ep)):\n",
    "        action = np.argmax(Q)\n",
    "        N[action]+= 1\n",
    "        _, reward, _, _ = env.step(action)\n",
    "        Q[action] += (reward - Q[action])/N[action] \n",
    "        #reason for subtraction is because in this env, rewards are 0 or 1. having a constant +ve reward may cause hangups \n",
    "        \n",
    "        Q_s[e] = Q\n",
    "        returns[e] = reward\n",
    "        actions[e] = action\n",
    "    return name, returns, Qe, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pure_random_strat(env, n_ep = 300):\n",
    "    Q = np.zeros((env.action_space.n)) #remember, mab has only one state so Q table will be 1 by no. of actions\n",
    "    N = np.zeros((env.action_space.n)) #counts the number of times an action was selected(basically the discount factor)\n",
    "    \n",
    "    Q_s = np.empty((n_ep, env.action_space.n))\n",
    "    returns = np.empty(n_ep)\n",
    "    actions = np.empty(n_ep)\n",
    "    \n",
    "    name = 'Pure exploitation'\n",
    "    for e in tqdm(range(n_ep)):\n",
    "        #action = np.argmax(Q)\n",
    "        action = np.random.randint(len(env.action_space.n))\n",
    "        N[action]+= 1\n",
    "        _, reward, _, _ = env.step(action)\n",
    "        Q[action] += (reward - Q[action])/N[action] \n",
    "        #reason for subtraction is because in this env, rewards are 0 or 1. having a constant +ve reward may cause hangups \n",
    "        \n",
    "        Q_s[e] = Q\n",
    "        returns[e] = reward\n",
    "        actions[e] = action\n",
    "    return name, returns, Qe, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_strat(env, n_ep = 300, epsilon = 0.01):\n",
    "    Q = np.zeros((env.action_space.n)) #remember, mab has only one state so Q table will be 1 by no. of actions\n",
    "    N = np.zeros((env.action_space.n)) #counts the number of times an action was selected(basically the discount factor)\n",
    "    \n",
    "    Q_s = np.empty((n_ep, env.action_space.n))\n",
    "    returns = np.empty(n_ep)\n",
    "    actions = np.empty(n_ep)\n",
    "    \n",
    "    name = 'Pure exploitation'\n",
    "    for e in tqdm(range(n_ep)):\n",
    "        #action = np.argmax(Q)\n",
    "        if np.random.random() > epsilon:\n",
    "            action = np.random.randint(len(env.action_space.n))\n",
    "        else:\n",
    "            action = np.argmax(Q)\n",
    "            \n",
    "        N[action]+= 1\n",
    "        _, reward, _, _ = env.step(action)\n",
    "        Q[action] += (reward - Q[action])/N[action] \n",
    "        #reason for subtraction is because in this env, rewards are 0 or 1. having a constant +ve reward may cause hangups \n",
    "        \n",
    "        Q_s[e] = Q\n",
    "        returns[e] = reward\n",
    "        actions[e] = action\n",
    "    return name, returns, Qe, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_strat_fl(env, n_ep = 300, epsilon = 0.001):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n)) \n",
    "    V = np.zeros((env.observation_space.n)) \n",
    "    N = np.zeros((env.observation_space.n, env.action_space.n)) #used to prevent overflow\n",
    "    gamma = 0.99\n",
    "   \n",
    "    for e in (range(n_ep)):\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            if np.random.random() > epsilon:\n",
    "                action = np.random.randint((env.action_space.n))\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "            \n",
    "            N[state][action]+= 1\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            Q[state][action] += (reward - gamma*V[new_state])/N[state][action] \n",
    "            #reason for subtraction is because in this env, rewards are 0 or 1. having a constant +ve reward may cause hangups \n",
    "            state = new_state\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        V = np.max(Q, axis=1)\n",
    "    return Q, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_dec_epsilon_greedy_fl(env,\n",
    "                          init_epsilon = 1.0,\n",
    "                          min_epsilon=0.01,\n",
    "                          decay_ratio = 0.05,\n",
    "                          n_ep = 4000):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n)) \n",
    "    V = np.zeros((env.observation_space.n)) \n",
    "    N = np.zeros((env.observation_space.n, env.action_space.n)) #used to prevent overflow\n",
    "    gamma = 0.99\n",
    "    ep_arr = []\n",
    "    \n",
    "    for episode in range(n_ep):\n",
    "        decay_episodes = n_ep * decay_ratio\n",
    "        epsilon = 1 - episode/decay_episodes\n",
    "        epsilon *= init_epsilon - min_epsilon\n",
    "        epsilon+=min_epsilon\n",
    "        epsilon = np.clip(epsilon, min_epsilon, init_epsilon)\n",
    "        total_rewards = 0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            if np.random.random() > epsilon:\n",
    "                action =  np.argmax(Q[state])\n",
    "            else:\n",
    "                action = np.random.randint(env.action_space.n)\n",
    "            \n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            N[state][action]+=1\n",
    "            Q[state][action]+= (reward + gamma*V[new_state])/N[state][action]\n",
    "            state = new_state\n",
    "            total_rewards+=reward\n",
    "            if done == True:\n",
    "                break\n",
    "        V = np.max(Q, axis=1)\n",
    "        ep_arr.append(total_rewards)\n",
    "    return V,Q,ep_arr\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimistic_initialization(env,\n",
    "                             optimistic_estimate = 1.0,\n",
    "                             initial_count = 100,\n",
    "                             n_ep = 5000):\n",
    "    Q = np.full((env.observation_space.n,env.action_space.n ),optimistic_estimate)\n",
    "    N = np.full((env.observation_space.n,env.action_space.n),initial_count)\n",
    "    V = np.zeros((env.observation_space.n))\n",
    "    gamma = 0.99\n",
    "    for e in range(n_ep):\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = np.argmax(Q[state])\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            N[state][action]+=1\n",
    "            Q[state][action]+= (reward + gamma*V[new_state])/N[state][action]\n",
    "            state = new_state\n",
    "            \n",
    "            if done == True:\n",
    "                break\n",
    "        V = np.max(Q, axis=1)\n",
    "    return V, Q, N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(env,\n",
    "           init_temp = 1000.0,\n",
    "           min_temp = 0.01,\n",
    "           decay_ratio = 0.04,\n",
    "           n_episodes = 5000):\n",
    "    theta = 1e-10\n",
    "    Q = np.full((env.observation_space.n, env.action_space.n), theta)\n",
    "    N = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    V = np.zeros((env.observation_space.n))\n",
    "    gamma = 0.99\n",
    "    for e in range(n_episodes):\n",
    "        decay_episodes = n_episodes*decay_ratio\n",
    "        temp = 1 - e/decay_episodes\n",
    "        temp*=init_temp - min_temp\n",
    "        temp+=min_temp\n",
    "        temp = np.clip(temp, min_temp, init_temp) #makes sure temp isnt 0\n",
    "        \n",
    "        scaled_Q = Q/temp\n",
    "        norm_Q = scaled_Q - np.sum(scaled_Q, axis = 1).reshape(-1, 1)\n",
    "        exp_Q = np.exp(norm_Q)\n",
    "        probs = exp_Q/np.sum(exp_Q, axis=1).reshape(-1,1)\n",
    "        assert np.isclose(probs[0].sum(), 1.0)\n",
    "        \n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = np.random.choice(np.arange(env.action_space.n),\n",
    "                                     size = 1,\n",
    "                                     p = probs[state])[0]\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            N[state][action]+=1\n",
    "            Q[state][action]+= (reward + gamma*V[new_state])/N[state][action]\n",
    "            state = new_state\n",
    "            \n",
    "            if done == True:\n",
    "                break\n",
    "        V = np.max(Q, axis = 1)\n",
    "    return V, Q\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "V,Q = softmax(env,\n",
    "           init_temp = 1000.0,\n",
    "           min_temp = 0.01,\n",
    "           decay_ratio = 0.04,\n",
    "           n_episodes = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "V,Q,N = optimistic_initialization(env,\n",
    "                             optimistic_estimate = 1.0,\n",
    "                             initial_count = 100,\n",
    "                             n_ep = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "V,Q,R = lin_dec_epsilon_greedy_fl(env,\n",
    "                          init_epsilon = 0.8,\n",
    "                          min_epsilon=0.1,\n",
    "                          decay_ratio = 0.05,\n",
    "                          n_ep = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epsilon_greedy_strat_fl(env, n_ep = 150, epsilon = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
