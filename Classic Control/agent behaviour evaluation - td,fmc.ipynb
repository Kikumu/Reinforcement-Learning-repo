{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_schedule(init_value,\n",
    "                   min_value,\n",
    "                   decay_ratio,\n",
    "                   max_steps,\n",
    "                   log_start = -2,\n",
    "                   log_base=10\n",
    "):\n",
    "    decay_steps = int(max_steps * decay_ratio)\n",
    "    rem_steps = max_steps - decay_steps\n",
    "    \n",
    "    values = np.logspace(log_start,\n",
    "                        0,\n",
    "                        decay_steps,\n",
    "                        base = log_base,\n",
    "                        endpoint = True)[::-1]\n",
    "    #print(value)\n",
    "    values = (values - values.min())/(values.max() - values.min())\n",
    "    values = np.pad(values, (0, rem_steps), 'edge')\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory(pi, env, max_steps=20): #generate single trajectory from start to terminal state\n",
    "    done, trajectory = False, []\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        for t in range(max_steps):\n",
    "            action = pi(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            experience = (state, action, reward, next_state)\n",
    "            trajectory.append(experience)\n",
    "            if done == True:\n",
    "                break\n",
    "            state = next_state\n",
    "    return np.array(trajectory, np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_prediction(pi,\n",
    "                  env,\n",
    "                  gamma = 0.99,\n",
    "                  init_alpha=0.5,\n",
    "                  min_alpha=0.01,\n",
    "                  alpha_decay_ratio=0.3,\n",
    "                  n_episodes=500,\n",
    "                  max_steps=100,\n",
    "                  first_visit=False\n",
    "):\n",
    "    nS = env.observation_space.n\n",
    "    \n",
    "    discounts = np.logspace(\n",
    "    0,\n",
    "    max_steps,\n",
    "    num=max_steps,\n",
    "    base=gamma,\n",
    "    endpoint=False)\n",
    "    \n",
    "    alphas = decay_schedule(\n",
    "    init_alpha,\n",
    "    min_alpha,\n",
    "    alpha_decay_ratio,\n",
    "    n_episodes)\n",
    "    \n",
    "    V = np.zeros(nS)\n",
    "    V_track = np.zeros((n_episodes,nS))\n",
    "    \n",
    "    for e in range(n_episodes):\n",
    "        trajectory = generate_trajectory(pi, env, max_steps)\n",
    "        \n",
    "        visited = np.zeros(nS, dtype=np.bool)\n",
    "        for i, (state, action, reward, next_state) in enumerate(trajectory):\n",
    "            if visited[state] and first_visit:\n",
    "                continue\n",
    "            visited[state] = True\n",
    "            \n",
    "            n_steps = len(trajectory[i:])#how far am i from terminal state\n",
    "            G = np.sum(discounts[:n_steps]* trajectory[i:, 2])#get discount from start to n steps\n",
    "            V[state]+=alphas[e] * (G - V[state])\n",
    "        V_track[e] = V\n",
    "    return V.copy(), V_track\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td(pi,\n",
    "       env,\n",
    "       gamma = 0.99,\n",
    "       init_alpha=0.5,\n",
    "       min_alpha=0.01,\n",
    "       alpha_decay_ratio=0.3,\n",
    "       n_episodes=500\n",
    "):\n",
    "    nS=env.observation_space.n\n",
    "    V =np.zeros(nS)\n",
    "    V_track =np.zeros((n_episodes, nS))\n",
    "    \n",
    "    alphas =decay_schedule(\n",
    "    init_alpha,\n",
    "    min_alpha,\n",
    "    alpha_decay_ratio,\n",
    "    n_episodes)\n",
    "    \n",
    "    for e in range(n_episodes):\n",
    "        state, done = env.reset(), False\n",
    "        while not done:\n",
    "            action = pi(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            td_target = reward + gamma*V[next_state]*(not done)\n",
    "            #print(alphas[e], td_target)\n",
    "            td_error = alphas[e]*(td_target - V[state])\n",
    "            V[state]+=td_error\n",
    "            state = next_state\n",
    "        V_track[e] = V\n",
    "    return V, V_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ntd(pi,\n",
    "        env,\n",
    "        gamma=0.99,\n",
    "        init_alpha=0.5,\n",
    "        min_alpha=0.01,\n",
    "        alpha_decay_ratio=0.5,\n",
    "        n_step=3,\n",
    "        n_episodes=500\n",
    "):\n",
    "    nS = env.observation_space.n\n",
    "    V = np.zeros(nS)\n",
    "    V_track = np.zeros((n_episodes, nS))\n",
    "    \n",
    "    alphas = decay_schedule(\n",
    "    init_alpha,\n",
    "    min_alpha,\n",
    "    alpha_decay_ratio,\n",
    "    n_episodes)\n",
    "    \n",
    "    discounts = np.logspace(\n",
    "    0, \n",
    "    n_step+1,\n",
    "    num = n_step + 1,\n",
    "    base=gamma,\n",
    "    endpoint=False)\n",
    "    \n",
    "    for e in range(n_episodes):\n",
    "        state, path, done = env.reset(), [], False\n",
    "        while not done or path is not None:\n",
    "            path = path[1:] # select path+1 trajectory onwards and not curr path\n",
    "            while not done and len(path) < n_step:#if env not done and max steps(n) not achieved\n",
    "                action = pi(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                experience = (state,action,next_state,reward,done)\n",
    "                path.append(experience)\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    break\n",
    "            n = len(path)\n",
    "            est_state = path[0][0]#remember, ntd - we are looking back a couple of steps so we take first step in path\n",
    "            #which is what we are evaluating\n",
    "            rewards = np.array(path)[:,3]\n",
    "            partial_return = discounts[:n]*rewards\n",
    "            bs_val = discounts[-1]*V[next_state]*(not done)\n",
    "            ntd_target = np.sum(np.append(partial_return,bs_val))\n",
    "            ntd_error = alphas[e]*(ntd_target - V[est_state])\n",
    "            V[est_state]+=ntd_error\n",
    "            \n",
    "            #print(path[0][3])\n",
    "            if len(path) == 1 and path[0][-1]:#if only one exp left and path is done\n",
    "                #print(path[0][3])\n",
    "                path = None\n",
    "        \n",
    "        V_track[e] = V\n",
    "    return V, V_track      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_lambda(pi,\n",
    "             env,\n",
    "             gamma = 0.99,\n",
    "             init_alpha = 0.5,\n",
    "             min_alpha = 0.01,\n",
    "             alpha_decay_ratio = 0.3,\n",
    "             lambda_ = 0.3,\n",
    "             n_episodes=500):\n",
    "    nS = env.observation_space.n\n",
    "    V = np.zeros(nS)\n",
    "    V_track =np.zeros((n_episodes,nS))\n",
    "    E = np.zeros(nS) #eligibility traces\n",
    "    \n",
    "    alphas = decay_schedule(\n",
    "    init_alpha,\n",
    "    min_alpha,\n",
    "    alpha_decay_ratio,\n",
    "    n_episodes)\n",
    "    \n",
    "    for e in range(n_episodes):\n",
    "        E.fill(0)\n",
    "        state, done = env.reset(), False\n",
    "        \n",
    "        while not done:\n",
    "            action = pi(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            td_target = reward + gamma*V[next_state]\n",
    "            td_error = td_target - V[state]\n",
    "            E[state]+=1#increment eligibility state\n",
    "            V[state]+=alphas[e]*td_error*E[state]\n",
    "            E*=gamma*lambda_\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "        V_track[e] = V\n",
    "    return V, V_track "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(env,\n",
    "           init_temp = 1000.0,\n",
    "           min_temp = 0.01,\n",
    "           decay_ratio = 0.04,\n",
    "           n_episodes = 5000):\n",
    "    \n",
    "    theta = 1e-10\n",
    "    Q = np.full((env.observation_space.n, env.action_space.n), theta)\n",
    "    N = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    V = np.zeros((env.observation_space.n))\n",
    "    gamma = 0.99\n",
    "    \n",
    "    for e in range(n_episodes):\n",
    "        decay_episodes = n_episodes*decay_ratio\n",
    "        temp = 1 - e/decay_episodes\n",
    "        temp*=init_temp - min_temp\n",
    "        temp+=min_temp\n",
    "        temp = np.clip(temp, min_temp, init_temp) #makes sure temp isnt 0\n",
    "        \n",
    "        scaled_Q = Q/temp #add temp\n",
    "        \n",
    "        norm_Q = scaled_Q - np.max(scaled_Q, axis = 1).reshape(-1, 1)#norm for stability\n",
    "        exp_Q = np.exp(norm_Q)\n",
    "        probs = exp_Q/np.sum(exp_Q, axis=1).reshape(-1,1)\n",
    "        \n",
    "        if math.isnan(probs[0].sum()) == True:\n",
    "            print(probs)\n",
    "        \n",
    "        assert np.isclose(probs[0].sum(), 1.0)\n",
    "        \n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = np.random.choice(np.arange(env.action_space.n),\n",
    "                                     size = 1,\n",
    "                                     p = probs[state])[0]\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            N[state][action]+=1\n",
    "            Q[state][action]+= (reward + gamma*V[new_state])/N[state][action]\n",
    "            state = new_state\n",
    "            \n",
    "            if done == True:\n",
    "                break\n",
    "        V = np.max(Q, axis = 1)\n",
    "    return V, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "V,Q = softmax(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_ = lambda s : {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "V1,V2 = mc_prediction(policy_,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "V3,V4 = td(policy_,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "V5,V6 = ntd(policy_,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "V7,V8 = td_lambda(policy_,env,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0288839 , 0.02763312, 0.03516455, 0.        , 0.03472166,\n",
       "       0.        , 0.05570777, 0.        , 0.0565585 , 0.15588257,\n",
       "       0.15026924, 0.        , 0.        , 0.33464083, 0.54206321,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.99      , 0.9801    , 0.970299  , 0.96059601,\n",
       "       0.95099005, 0.94148015, 0.93206535, 0.92274469, 0.91351725,\n",
       "       0.90438208, 0.89533825, 0.88638487, 0.87752102, 0.86874581,\n",
       "       0.86005835, 0.85145777, 0.84294319, 0.83451376, 0.82616862,\n",
       "       0.81790694, 0.80972787, 0.80163059, 0.79361428, 0.78567814,\n",
       "       0.77782136, 0.77004315, 0.76234271, 0.75471929, 0.74717209,\n",
       "       0.73970037, 0.73230337, 0.72498034, 0.71773053, 0.71055323,\n",
       "       0.70344769, 0.69641322, 0.68944909, 0.6825546 , 0.67572905,\n",
       "       0.66897176, 0.66228204, 0.65565922, 0.64910263, 0.6426116 ,\n",
       "       0.63618549, 0.62982363, 0.62352539, 0.61729014, 0.61111724,\n",
       "       0.60500607, 0.59895601, 0.59296645, 0.58703678, 0.58116641,\n",
       "       0.57535475, 0.5696012 , 0.56390519, 0.55826614, 0.55268348,\n",
       "       0.54715664, 0.54168508, 0.53626823, 0.53090554, 0.52559649,\n",
       "       0.52034052, 0.51513712, 0.50998575, 0.50488589, 0.49983703,\n",
       "       0.49483866, 0.48989027, 0.48499137, 0.48014146, 0.47534004,\n",
       "       0.47058664, 0.46588078, 0.46122197, 0.45660975, 0.45204365,\n",
       "       0.44752321, 0.44304798, 0.4386175 , 0.43423133, 0.42988901,\n",
       "       0.42559012, 0.42133422, 0.41712088, 0.41294967, 0.40882017,\n",
       "       0.40473197, 0.40068465, 0.39667781, 0.39271103, 0.38878392,\n",
       "       0.38489608, 0.38104712, 0.37723665, 0.37346428, 0.36972964])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_steps = 100\n",
    "gamma = 0.99\n",
    "discounts = np.logspace(\n",
    "    0,\n",
    "    max_steps,\n",
    "    num=max_steps,\n",
    "    base=gamma,\n",
    "    endpoint=False)\n",
    "discounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.zeros(6, dtype=np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37346428, 0.36972964])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discounts[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = []\n",
    "path[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.40881460e-02, 1.56625568e+01, 5.52866649e-02, 3.88696495e-03],\n",
       "       [1.11244988e-03, 1.93400759e-02, 1.52391534e-02, 1.54011797e+01],\n",
       "       [1.95746907e+01, 2.12661578e-02, 1.29995203e-02, 1.18683859e-03],\n",
       "       [2.06757563e-03, 4.75383522e-03, 2.98887809e-04, 1.01315428e-03],\n",
       "       [2.45899094e+01, 1.54772202e-02, 8.90080753e-03, 7.57695628e-04],\n",
       "       [1.00000000e-10, 1.00000000e-10, 1.00000000e-10, 1.00000000e-10],\n",
       "       [3.06051315e-09, 2.36764305e+01, 4.07750650e-02, 7.02178648e-04],\n",
       "       [1.00000000e-10, 1.00000000e-10, 1.00000000e-10, 1.00000000e-10],\n",
       "       [1.90816898e-04, 4.23887693e+01, 1.02413344e-02, 5.62264533e-02],\n",
       "       [8.84812526e-02, 7.79759209e+01, 7.94001983e-10, 1.58066883e-01],\n",
       "       [2.20149022e-01, 5.99386944e+01, 2.03028993e-01, 1.07811007e-02],\n",
       "       [1.00000000e-10, 1.00000000e-10, 1.00000000e-10, 1.00000000e-10],\n",
       "       [1.00000000e-10, 1.00000000e-10, 1.00000000e-10, 1.00000000e-10],\n",
       "       [7.55799158e-10, 1.99000000e-10, 1.07015597e+02, 9.68180118e-10],\n",
       "       [2.58637501e-01, 8.05100320e+01, 5.83333334e-01, 4.26740835e-01],\n",
       "       [1.00000000e-10, 1.00000000e-10, 1.00000000e-10, 1.00000000e-10]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.52866649e-02, 1.52391534e-02, 1.29995203e-02, 2.98887809e-04,\n",
       "       8.90080753e-03, 1.00000000e-10, 4.07750650e-02, 1.00000000e-10,\n",
       "       1.02413344e-02, 7.94001983e-10, 2.03028993e-01, 1.00000000e-10,\n",
       "       1.00000000e-10, 1.07015597e+02, 5.83333334e-01, 1.00000000e-10])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q[:,-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
