{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_schedule(init_value,\n",
    "                   min_value,\n",
    "                   decay_ratio,\n",
    "                   max_steps,\n",
    "                   log_start = -2,\n",
    "                   log_base=10\n",
    "):\n",
    "    decay_steps = int(max_steps * decay_ratio)\n",
    "    rem_steps = max_steps - decay_steps\n",
    "    \n",
    "    values = np.logspace(log_start,\n",
    "                        0,\n",
    "                        decay_steps,\n",
    "                        base = log_base,\n",
    "                        endpoint = True)[::-1]\n",
    "    #print(value)\n",
    "    values = (values - values.min())/(values.max() - values.min())\n",
    "    values = np.pad(values, (0, rem_steps), 'edge')\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory(env, select_action, Q, epsilon, max_steps=200): #generate single trajectory from start to terminal state\n",
    "    done, trajectory = False, []\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        #print(max_steps)\n",
    "        for t in range(max_steps):\n",
    "            action = select_action(state, Q, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            experience = (state, action, reward, next_state)\n",
    "            trajectory.append(experience)\n",
    "            if done == True:\n",
    "                break\n",
    "            state = next_state\n",
    "    return np.array(trajectory, np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control(env,\n",
    "              gamma=0.99,\n",
    "               init_alpha=0.5,\n",
    "               min_alpha=0.01,\n",
    "               alpha_decay_ratio=0.5,\n",
    "               init_epsilon=1.0,\n",
    "               min_epsilon=0.1,\n",
    "               epsilon_decay_ratio=0.9,\n",
    "               n_episodes=3000,\n",
    "               max_steps=200,\n",
    "               first_visit=True\n",
    "              ):\n",
    "    nS, nA = env.observation_space.n, env.action_space.n\n",
    "    discounts = np.logspace(\n",
    "    0, \n",
    "    max_steps,\n",
    "    num=max_steps,\n",
    "    base=gamma,\n",
    "    endpoint=False)\n",
    "    \n",
    "    alphas = decay_schedule(\n",
    "    init_alpha,\n",
    "    min_alpha,\n",
    "    alpha_decay_ratio,\n",
    "    n_episodes)#for error func decay\n",
    "    \n",
    "    epsilons = decay_schedule(\n",
    "    init_epsilon,\n",
    "    min_epsilon,\n",
    "    epsilon_decay_ratio,\n",
    "    n_episodes)#epsilon decay\n",
    "    \n",
    "    policy_track = []\n",
    "    Q = np.zeros((nS, nA))\n",
    "    Q_track = np.zeros((n_episodes, nS, nA))\n",
    "    \n",
    "    select_action = lambda state, Q, epsilon: \\\n",
    "    np.argmax(Q[state])\\\n",
    "    if np.random.random() > epsilon\\\n",
    "    else np.random.randint(len(Q[state]))\\\n",
    "    \n",
    "    for e in range(n_episodes):\n",
    "        trajectory = generate_trajectory(env, select_action, Q, epsilons[e])\n",
    "        visited = np.zeros((nS,nA))\n",
    "        \n",
    "        for t, (state,action, reward, next_state) in enumerate(trajectory):\n",
    "            if visited[state][action] and first_visit:\n",
    "                continue\n",
    "            visited[state][action] = True\n",
    "            n_steps = len(trajectory[t:])\n",
    "            G = np.sum(discounts[:n_steps]*trajectory[t:,-2])\n",
    "            Q[state][action]+=alphas[e]*(G - Q[state][action])\n",
    "        Q_track[e] = Q\n",
    "        policy_track.append(np.argmax(Q, axis=1))\n",
    "    V = np.max(Q, axis=1)\n",
    "    policy = lambda s: {s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n",
    "    return Q, V, policy, Q_track, policy_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env,\n",
    "         gamma = 0.99,\n",
    "        init_alpha = 0.5,\n",
    "         min_alpha=0.01,\n",
    "         alpha_decay_ratio=0.5,\n",
    "         init_epsilon=1.0,\n",
    "         min_epsilon=0.1,\n",
    "         epsilon_decay_ratio=0.9,\n",
    "         n_ep=3000):\n",
    "    nS = env.observation_space.n\n",
    "    nA = env.action_space.n\n",
    "    Q = np.zeros((nS, nA))\n",
    "    Q_track = np.zeros((n_ep,nS,nA))\n",
    "    policy_track = []\n",
    "    \n",
    "    select_action = lambda state, Q, epsilon: \\\n",
    "    np.argmax(Q[state])\\\n",
    "    if np.random.random() > epsilon\\\n",
    "    else np.random.randint(len(Q[state]))\n",
    "    \n",
    "    alphas = decay_schedule(\n",
    "    init_alpha,\n",
    "    min_alpha,\n",
    "    alpha_decay_ratio,\n",
    "    n_ep)#for error func decay \n",
    "    \n",
    "    epsilons = decay_schedule(\n",
    "    init_epsilon,\n",
    "    min_epsilon,\n",
    "    epsilon_decay_ratio,\n",
    "    n_ep)#epsilon decay\n",
    "    \n",
    "    for e in range(n_ep):\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = select_action(state, Q, epsilons[e])\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            td_target = reward + gamma*Q[next_state][action]\n",
    "            td_error = td_target - Q[state][action]\n",
    "            Q[state][action]+=alphas[e]*td_error\n",
    "            state = next_state\n",
    "            \n",
    "            if done == True:\n",
    "                break\n",
    "        Q_track[e] = Q\n",
    "        policy_track.append(np.argmax(Q, axis=1))#argmax returns index, max returns value\n",
    "    V = np.max(Q, axis=1)\n",
    "    policy_ = lambda s:{s:a in enumerate(np.argmax(Q, axis=1))}[s]\n",
    "    return Q, V, policy_, Q_track, policy_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env,\n",
    "                gamma = 0.99,\n",
    "              init_alpha=0.5,\n",
    "              min_alpha = 0.01,\n",
    "              alpha_decay_ratio=0.5,\n",
    "              init_epsilon=1.0,\n",
    "              min_epsilon = 0.1,\n",
    "              epsilon_decay_ratio=0.9,\n",
    "              n_ep=3000):\n",
    "    \n",
    "    nS, nA = env.observation_space.n, env.action_space.n\n",
    "    Q = np.zeros((nS, nA))\n",
    "    Q_track = np.zeros((n_ep, nS, nA))\n",
    "    policy_track = []\n",
    "    \n",
    "    alphas = decay_schedule(\n",
    "    init_alpha,\n",
    "    min_alpha,\n",
    "    alpha_decay_ratio,\n",
    "    n_ep)#for error func decay \n",
    "    \n",
    "    epsilons = decay_schedule(\n",
    "    init_epsilon,\n",
    "    min_epsilon,\n",
    "    epsilon_decay_ratio,\n",
    "    n_ep)#epsilon decay\n",
    "    \n",
    "    select_action = lambda state, Q, epsilon: \\\n",
    "    np.argmax(Q[state])\\\n",
    "    if np.random.random() > epsilon\\\n",
    "    else np.random.randint(len(Q[state]))\n",
    "    \n",
    "    for e in range(n_ep):\n",
    "        state = env.reset()\n",
    "        \n",
    "        while True:\n",
    "            action = select_action(state, Q, epsilons[e])\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            sarsa_target=reward + gamma*(np.max(Q[next_state]))*(not done)\n",
    "            sarsa_error =sarsa_target - Q[state][action]\n",
    "            Q[state][action]+=alphas[e]*sarsa_error\n",
    "            state = next_state\n",
    "            if done == True:\n",
    "                break\n",
    "        Q_track[e] = Q\n",
    "        policy_track.append(np.argmax(Q, axis=1))\n",
    "    V = np.max(Q, axis=1)\n",
    "    policy_ = lambda s:{s:a for s, a in enumerate(np.argmax(Q, axis=1))}[s]\n",
    "    return Q, V, policy_, Q_track, policy_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_q_learning(env,\n",
    "                     gamma=0.99,\n",
    "                     init_alpha=0.5,\n",
    "                     min_alpha=0.01,\n",
    "                     alpha_decay_ratio=0.5,\n",
    "                     init_epsilon=1.0,\n",
    "                     min_epsilon=0.1,\n",
    "                     epsilon_decay_ratio=0.9,\n",
    "                     n_ep=3000):\n",
    "    nS, nA = env.observation_space.n, env.action_space.n\n",
    "    Q1 = np.zeros((nS,nA))\n",
    "    Q2 = np.zeros((nS,nA))\n",
    "    policy_track = []\n",
    "    Q1_track = np.zeros((n_ep, nS, nA))\n",
    "    Q2_track = np.zeros((n_ep, nS, nA))\n",
    "    \n",
    "    select_action = lambda state, Q, epsilon: \\\n",
    "    np.argmax(Q[state])\\\n",
    "    if np.random.random() > epsilon\\\n",
    "    else np.random.randint(len(Q[state]))\n",
    "    \n",
    "    alphas = decay_schedule(\n",
    "    init_alpha,\n",
    "    min_alpha,\n",
    "    alpha_decay_ratio,\n",
    "    n_ep)#for error func decay \n",
    "    \n",
    "    epsilons = decay_schedule(\n",
    "    init_epsilon,\n",
    "    min_epsilon,\n",
    "    epsilon_decay_ratio,\n",
    "    n_ep)#epsilon decay\n",
    "    \n",
    "    for e in range(n_ep):\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = select_action(state, (Q1 + Q2)/2, epsilons[e])\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            if np.random.randint(2):\n",
    "                Q1_argmax = np.argmax(Q1[next_state])\n",
    "                td_target = reward + gamma*Q2[next_state][Q1_argmax]#pick d b on Q2\n",
    "                td_error = td_target - Q1[state][action]\n",
    "                Q1[state][action]+=alphas[e]*td_error\n",
    "            else:\n",
    "                #print(Q2)\n",
    "                Q2_argmax = np.argmax(Q2[next_state])\n",
    "                td_target = reward + gamma*Q1[next_state][Q2_argmax]\n",
    "                td_error = td_target - Q2[state][action]\n",
    "                Q2[state][action]+=alphas[e]*td_error\n",
    "            \n",
    "            state = next_state\n",
    "            if done == True:\n",
    "                break\n",
    "        Q1_track[e]=Q1\n",
    "        Q2_track[e]=Q2\n",
    "        policy_track.append(np.argmax((Q1+Q2/2), axis=1))\n",
    "    Q = (Q1+Q2)/2\n",
    "    V = np.max(Q, axis = 1)\n",
    "    policy_ = lambda s:{s,a in enumerate(np.argmax(Q, axis =1))}[s]\n",
    "    return Q, V, policy_, Q_track, policy_track    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, V, POLICY, Q_track, policy_track = mc_control(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, V, POLICY, Q_track, policy_track = sarsa(env)#td(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, V, POLICY, Q_track, policy_track = q_learning(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, V, POLICY, Q_track, policy_track = double_q_learning(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-1096e59cd817>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "maxStepsPerEpisode = 100\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "for episode in range(10):\n",
    "    state = env.reset()\n",
    "    print(\"EPISODE: \", episode+1)\n",
    "    time.sleep(2)\n",
    "\n",
    "    for step in range(maxStepsPerEpisode):\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        action = np.argmax(q_table[state, :])\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"successfull goal\")\n",
    "                time.sleep(2)\n",
    "            elif reward == -1:\n",
    "                print(\"hole\")\n",
    "                time.sleep(2)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "\n",
    "    state = new_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
