{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from random import seed\n",
    "from sklearn import preprocessing\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvManager():\n",
    "    def __init__(self, device,environment):\n",
    "        self.device = device\n",
    "        #self.env = gym.make(environment).unwrapped\n",
    "        self.env = gym.make(environment)\n",
    "        self.env.reset()\n",
    "        self.current_screen = None\n",
    "        self.done = False\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        self.current_screen = None\n",
    "        \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render(mode)\n",
    "        \n",
    "    def num_actions_available(self):\n",
    "        return self.env.action_space\n",
    "        \n",
    "    def take_action(self, action):   \n",
    "        _, reward, self.done, _ = self.env.step([action])\n",
    "        return reward\n",
    "    \n",
    "    def just_starting(self):\n",
    "        return self.current_screen is None\n",
    "    \n",
    "    def get_state(self):\n",
    "        if self.just_starting() or self.done:\n",
    "            self.current_screen = self.get_processed_screen()\n",
    "            black_screen = torch.zeros_like(self.current_screen)\n",
    "            return black_screen\n",
    "        else:\n",
    "            s1 = self.current_screen\n",
    "            s2 = self.get_processed_screen()\n",
    "            self.current_screen = s2\n",
    "            return s2 - s1\n",
    "    \n",
    "    def get_screen_height(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[2]\n",
    "    \n",
    "    def get_screen_width(self):\n",
    "        screen = self.get_processed_screen()\n",
    "        return screen.shape[3]\n",
    "       \n",
    "    def get_processed_screen(self):\n",
    "        screen = em.render('rgb_array')\n",
    "        rgb_weights = [0.2989, 0.5870, 0.1140]\n",
    "        grayscale_image = np.dot(screen[...,:3], rgb_weights) \n",
    "        screen = grayscale_image.transpose((0, 1)) # PyTorch expects CHW\n",
    "        #print(type(screen)) # numpy\n",
    "        screen = self.crop_screen(screen)\n",
    "        return self.transform_screen_data(screen)\n",
    "    \n",
    "    def crop_screen(self, screen):\n",
    "        screen_height = screen.shape[0]\n",
    "        screen_width  = screen.shape[1]\n",
    "        #print('screen height(top/bottom): ',screen_height)\n",
    "        #print('screen height(left/right): ',screen_width)\n",
    "        # Strip off top and bottom\n",
    "        top = int(screen_height * 0.2)\n",
    "        #print('top: ',top)\n",
    "        bottom = int(screen_height * 0.8)\n",
    "        #print('bottom: ',bottom)\n",
    "        \n",
    "        \n",
    "        #strip off left/right\n",
    "        left  = int(screen_width * 0.2)\n",
    "        #print('left: ',left)\n",
    "        right = int(screen_width * 0.8)\n",
    "        #print('right: ',right)\n",
    "        \n",
    "        screen = screen[top:bottom, left:right]\n",
    "        return screen\n",
    "    \n",
    "    \n",
    "    def transform_screen_data(self, screen):       \n",
    "        # Convert to float, rescale, convert to tensor\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "        \n",
    "        # Use torchvision package to compose image transforms\n",
    "        resize = T.Compose([\n",
    "            T.ToPILImage()\n",
    "            ,T.Resize((40,90))\n",
    "            ,T.ToTensor()\n",
    "        ])\n",
    "        \n",
    "        return resize(screen).unsqueeze(0).to(self.device) # add a batch dimension (BCHW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "        \n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push_count % self.capacity] = experience\n",
    "        self.push_count += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def can_provide_sample(self, batch_size):\n",
    "        return len(self.memory) >= batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tensors(experiences):\n",
    "    # Convert batch of Experiences to Experience of batches\n",
    "    batch = Experience(*zip(*experiences))\n",
    "\n",
    "    t1 = torch.stack(batch.state)\n",
    "    t2 = torch.stack(batch.action)\n",
    "    t3 = torch.stack(batch.reward)\n",
    "    t4 = torch.stack(batch.next_state)\n",
    "\n",
    "    return (t1,t2,t3,t4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    ('state', 'action', 'reward', 'next_state')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OU_noise():\n",
    "    def __init__(self,mu,sigma=0.15,theta=0.15,dt=1e-2,x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu    = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt    = dt\n",
    "        self.x0    = x0\n",
    "        self.reset()\n",
    "        \n",
    "    def __call__(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev)* \\\n",
    "                self.dt +self.sigma * np.sqrt(self.dt)*np.random.normal(size = self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "    \n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_network(nn.Module):\n",
    "    def __init__(self, lr, input_size, fc1, fc2, linear_out):\n",
    "        super(Actor_network, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.linear_out = linear_out\n",
    "        self.lr = lr\n",
    "        self.n_hidden_fc1 = fc1\n",
    "        self.n_hidden_fc2 = fc2\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.fc1  = nn.Linear(self.input_size,self.n_hidden_fc1)\n",
    "        self.fc2  = nn.Linear(self.n_hidden_fc1,self.n_hidden_fc2)\n",
    "        self.fc3  = nn.Linear(self.n_hidden_fc2,self.linear_out)\n",
    "        \n",
    "        self.ln1 =nn.LayerNorm(self.n_hidden_fc1)\n",
    "        self.ln2 =nn.LayerNorm(self.n_hidden_fc2)\n",
    "        \n",
    "        fc1 = 1.0/np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        self.fc1.weight.data.uniform_(-fc1,fc1)\n",
    "        self.fc1.bias.data.uniform_(-fc1,fc1)\n",
    "        \n",
    "        fc2 = 1.0/np.sqrt(self.fc2.weight.data.size()[0])\n",
    "        self.fc2.weight.data.uniform_(-fc2,fc2)\n",
    "        self.fc2.bias.data.uniform_(-fc2,fc2)\n",
    "        \n",
    "        fc3 = 0.003\n",
    "        self.fc3.weight.data.uniform_(-fc3,fc3)\n",
    "        self.fc3.bias.data.uniform_(-fc3,fc3)\n",
    "        \n",
    "        self.optimizer = optim.Adam(params = self.parameters() ,lr = lr)\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def forward(self, states):\n",
    "        states = self.fc1(states)\n",
    "        states = self.ln1(states)\n",
    "        states = F.relu(states)\n",
    "        \n",
    "        states = self.fc2(states)\n",
    "        states = self.ln2(states)\n",
    "        states = F.relu(states)\n",
    "        \n",
    "        states = self.fc3(states)\n",
    "        out = torch.tanh(states)*2.0 #multiply by max/min env space, tanh because of -1 1 min maxing\n",
    "    \n",
    "        return out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic_network(nn.Module):\n",
    "    def __init__(self, lr, input_size, fc1, fc2, linear_out):\n",
    "        super(Critic_network, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.linear_out = linear_out\n",
    "        self.lr = lr\n",
    "        self.n_hidden_fc1 = fc1\n",
    "        self.n_hidden_fc2 = fc2\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.fc1   = nn.Linear(self.input_size,self.n_hidden_fc1)\n",
    "        self.fc2  = nn.Linear(self.n_hidden_fc1,self.n_hidden_fc2)\n",
    "        \n",
    "        self.action_values = nn.Linear(self.linear_out,self.n_hidden_fc2)\n",
    "        \n",
    "        self.state_q_value = nn.Linear(self.n_hidden_fc2, 1)\n",
    "        \n",
    "        #normalize data cause we are sampling from so many diff environments\n",
    "        self.ln1 = nn.LayerNorm(self.n_hidden_fc1) \n",
    "        self.ln2 = nn.LayerNorm(self.n_hidden_fc2)\n",
    "        \n",
    "        #initialize weights and biases according to number of neurons per laer involved\n",
    "        fc1 = 1.0/np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        self.fc1.weight.data.uniform_(-fc1,fc1)\n",
    "        self.fc1.bias.data.uniform_(-fc1,fc1)\n",
    "        \n",
    "        fc2 = 1.0/np.sqrt(self.fc2.weight.data.size()[0])\n",
    "        self.fc2.weight.data.uniform_(-fc2,fc2)\n",
    "        self.fc2.bias.data.uniform_(-fc2,fc2)\n",
    "        \n",
    "        fc3 = 1.0/np.sqrt(self.action_values.weight.data.size()[0])\n",
    "        self.action_values.weight.data.uniform_(-fc3,fc3)\n",
    "        self.action_values.bias.data.uniform_(-fc3,fc3)\n",
    "        \n",
    "        #final output layer initialize by 0.003 as desired by paper\n",
    "        self.state_q_value.weight.data.uniform_(-0.003,0.003)\n",
    "        self.state_q_value.bias.data.uniform_(-0.003,0.003)\n",
    "        \n",
    "        self.optimizer = optim.Adam(params = self.parameters() ,lr = lr, weight_decay = 0.01)#weight decay?\n",
    "        self.to(self.device)\n",
    "        \n",
    "        \n",
    "    def forward(self, states, actions):\n",
    "        states = self.fc1(states).to(self.device)\n",
    "        states = self.ln1(states)\n",
    "        states = F.relu(states)\n",
    "        \n",
    "        states = self.fc2(states)\n",
    "        states = self.ln2(states)\n",
    "        #states = F.relu(states)\n",
    "        \n",
    "        action_v = self.action_values(actions)#action nn has same output size as states\n",
    "        state_action_add = F.relu(torch.add(action_v,states))\n",
    "        \n",
    "        final_q = self.state_q_value(state_action_add)\n",
    "         \n",
    "        return final_q  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,alpha,beta,input_dim,tau,n_actions,max_replay_mem,fc1_,fc2_,batch_size = 250,gamma = 0.99):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        \n",
    "        self.memory = ReplayMemory(max_replay_mem)\n",
    "        \n",
    "        self.noise = OU_noise(mu = np.zeros(n_actions))\n",
    "        \n",
    "        self.actor = Actor_network(alpha,input_dim,fc1_,fc2_,linear_out=n_actions)\n",
    "        \n",
    "        self.t_actor= Actor_network(alpha,input_dim,fc1_,fc2_,linear_out=n_actions)\n",
    "        \n",
    "        self.critic= Critic_network(beta,input_dim,fc1_,fc2_,linear_out=n_actions)\n",
    "        \n",
    "        self.t_critic=Critic_network(beta,input_dim,fc1_,fc2_,linear_out=n_actions)\n",
    "        \n",
    "        self.update_network_parameters(tau=1)\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        self.actor.eval()\n",
    "        state = state.flatten()\n",
    "        mu = self.actor(state).to(self.actor.device)\n",
    "        mu_noise = mu + torch.tensor(self.noise(), dtype=torch.float).to(self.actor.device)\n",
    "        self.actor.train()\n",
    "        return mu_noise.cpu().detach().numpy()[0]\n",
    "    \n",
    "    def store_memory(self, state, action, reward, next_state):\n",
    "        state = state.flatten()\n",
    "        next_state = next_state.flatten()\n",
    "        self.memory.push(Experience(state,action,reward,next_state))\n",
    "        \n",
    "    def learn(self):\n",
    "        if self.memory.can_provide_sample(self.batch_size):\n",
    "            states,actions,rewards,next_states   = extract_tensors(self.memory.sample(self.batch_size))\n",
    "            states = torch.tensor(states, dtype  = torch.float).to(self.actor.device)\n",
    "            actions = torch.tensor(actions, dtype= torch.float).to(self.actor.device)\n",
    "            rewards = torch.tensor(rewards, dtype= torch.float).to(self.actor.device)\n",
    "            next_states = torch.tensor(next_states, dtype=torch.float).to(self.actor.device)\n",
    "            \n",
    "            target_actions_network = self.t_actor(next_states)\n",
    "            target_critic_network = self.t_critic(next_states,target_actions_network)\n",
    "            #print('states crit: ', states.shape,'actions: ', actions.shape)\n",
    "            critic_network = self.critic(states,actions)\n",
    "            #print('critic: ',critic_network.shape)\n",
    "            #print('critic_t: ',target_critic_network.shape)\n",
    "            target_critic_network = target_critic_network.view(-1)\n",
    "            rewards = rewards.view(-1)\n",
    "            #print('critic_t_: ',target_critic_network.shape)\n",
    "            #print('rewards: ',rewards.shape)\n",
    "            target = rewards+self.gamma*target_critic_network\n",
    "            \n",
    "            #print('target: ', target.shape)\n",
    "            target = target.view(self.batch_size,1)\n",
    "            #print('target prime: ', target.shape)\n",
    "            self.critic.optimizer.zero_grad()\n",
    "            #target and critic must be same size\n",
    "            #print('critic: ',critic_network.shape)\n",
    "            critic_loss = F.mse_loss(target,critic_network)\n",
    "            critic_loss.backward()\n",
    "            self.critic.optimizer.step()\n",
    "            \n",
    "            #actor loss\n",
    "            self.actor.optimizer.zero_grad()\n",
    "            actor_loss = -self.critic(states,self.actor(states))\n",
    "            actor_loss = torch.mean(actor_loss)\n",
    "            actor_loss.backward()\n",
    "            self.actor.optimizer.step()\n",
    "            \n",
    "            self.update_network_parameters()\n",
    "            \n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "        \n",
    "        actor_params = self.actor.named_parameters()\n",
    "        critic_params = self.critic.named_parameters()\n",
    "        target_actor_params = self.t_actor.named_parameters()\n",
    "        target_critic_params = self.t_critic.named_parameters()\n",
    "        \n",
    "        critic_dict = dict(critic_params)\n",
    "        actor_dict = dict(actor_params)\n",
    "        target_critic_dict = dict(target_critic_params)\n",
    "        target_actor_dict = dict(target_actor_params)\n",
    "        \n",
    "        for weight in critic_dict:\n",
    "            critic_dict[weight] = tau*critic_dict[weight].clone() + (1-tau)*\\\n",
    "                target_critic_dict[weight].clone()\n",
    "        \n",
    "        for weight in actor_dict:\n",
    "            actor_dict[weight] = tau*actor_dict[weight].clone() + (1-tau)*\\\n",
    "                target_actor_dict[weight].clone()\n",
    "        \n",
    "        self.t_critic.load_state_dict(critic_dict)\n",
    "        self.t_actor.load_state_dict(actor_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 250\n",
    "gamma = 0.7\n",
    "eps_start = 1\n",
    "eps_end = 0.001\n",
    "eps_decay = 0.0001\n",
    "target_update = 10\n",
    "memory_size = 10000\n",
    "lr = 0.001\n",
    "num_episodes = 10000#for more episodes for better results\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 40\n",
      "episode:  0 score-1261.04  avg_scr -1261.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\scowt\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "c:\\users\\scowt\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "c:\\users\\scowt\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:41: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "c:\\users\\scowt\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-9dbfd123fc9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstore_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction_idx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-7b433e5aede2>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mtarget_critic_network\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_critic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_actions_network\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[1;31m#print('states crit: ', states.shape,'actions: ', actions.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[0mcritic_network\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m             \u001b[1;31m#print('critic: ',critic_network.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;31m#print('critic_t: ',target_critic_network.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\scowt\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-c0001391f737>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, states, actions)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mln1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\scowt\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\scowt\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\scowt\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1751\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1752\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1753\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "em = EnvManager(device,'Pendulum-v0')\n",
    "#(self,alpha,beta,input_dim,tau,n_actions,max_replay_mem,fc1_,fc2_,batch_size = 250,gamma = 0.99)\n",
    "agent = Agent(alpha=0.0001,beta = 0.001,input_dim = em.get_screen_width()*em.get_screen_height(),tau = 0.001,n_actions = 1,max_replay_mem = 100000,fc1_=1000,fc2_=1000)\n",
    "print(em.get_screen_width(),em.get_screen_height())\n",
    "n_games = 1000\n",
    "scores = []\n",
    "for episode in range(n_games):\n",
    "    em.reset()\n",
    "    state = em.get_state()\n",
    "    agent.noise.reset()\n",
    "    score = 0\n",
    "    for timestep in count():\n",
    "        action_idx = agent.choose_action(state)\n",
    "        reward     = em.take_action(action_idx)\n",
    "        score+=reward\n",
    "        next_state = em.get_state()\n",
    "        action_idx = torch.tensor([action_idx], device=device)\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        agent.store_memory(state,action_idx,reward, next_state)\n",
    "        agent.learn()\n",
    "        state = next_state\n",
    "        if em.done:\n",
    "            scores.append(score)\n",
    "            avg_score = np.mean(scores[-100:])\n",
    "            print('episode: ', episode, 'score%.2f '% score, 'avg_scr %.2f'%avg_score)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
